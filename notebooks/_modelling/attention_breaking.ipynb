{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7d57b98-3cdf-446a-aad4-069a402386d7",
   "metadata": {},
   "source": [
    "* Let's decompose attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2d09508-0a63-42ae-8a25-ae0d486dd4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hydra\n",
    "from src.commons.constants import PROJECT_PATH\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "import pandas as pd\n",
    "import os\n",
    "from copy import deepcopy\n",
    "import torch.nn.functional as F\n",
    "from torchmetrics import Metric\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils import data\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "from src.commons.utils_io import load_sam\n",
    "from src.commons.utils import to_numpy, SegAnyChangeVersion, show_img, show_pair_img, show_prediction_sample, resize\n",
    "from src.models.commons.mask_process import extract_object_from_batch, binarize_mask\n",
    "from src.commons.constants import IMG_SIZE\n",
    "from src.data.process import generate_grid_prompt\n",
    "from src.commons.utils import create_sample_grid_with_prompt, get_mask_with_prompt, fig2arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "736dfa96-7598-4e39-8bf2-7002df3d1837",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_config():\n",
    "    # Initialize the Hydra configuration\n",
    "    hydra.initialize(config_path=\"../../configs\", version_base=None)\n",
    "    \n",
    "    # Compose the configuration with the desired environment override\n",
    "    cfg = hydra.compose(config_name=\"train\", overrides=[\"experiment=probing_diff\", \"sam_type=small\", \"data=levir-cd\"])\n",
    "    \n",
    "    return cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4d6312e-85cc-4f4e-b36b-936cd9f3d626",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data:\n",
      "  name: levir-cd\n",
      "  _target_: src.data.datamodule.CDDataModule\n",
      "  params:\n",
      "    prompt_type: sample\n",
      "    n_prompt: 1\n",
      "    loc: center\n",
      "    batch_size: 2\n",
      "    num_worker: 4\n",
      "    pin_memory: false\n",
      "    n_shape: 3\n",
      "model:\n",
      "  network:\n",
      "    image_encoder:\n",
      "      _target_: src.models.segment_anything.modeling.image_encoder_dev.ImageEncoderViT\n",
      "      depth: 12\n",
      "      embed_dim: 768\n",
      "      img_size: 1024\n",
      "      mlp_ratio: 4\n",
      "      norm_layer: null\n",
      "      num_heads: 12\n",
      "      patch_size: 16\n",
      "      qkv_bias: true\n",
      "      use_rel_pos: true\n",
      "      global_attn_indexes:\n",
      "      - 2\n",
      "      - 5\n",
      "      - 8\n",
      "      - 11\n",
      "      window_size: 14\n",
      "      out_chans: 256\n",
      "    prompt_encoder:\n",
      "      _target_: src.models.segment_anything.modeling.prompt_encoder_dev.PromptEncoder\n",
      "      embed_dim: 256\n",
      "      image_embedding_size:\n",
      "      - 64\n",
      "      - 64\n",
      "      input_image_size:\n",
      "      - 1024\n",
      "      - 1024\n",
      "      mask_in_chans: 16\n",
      "    mask_decoder:\n",
      "      transformer:\n",
      "        _target_: src.models.segment_anything.modeling.transformer_dev.TwoWayTransformer\n",
      "        depth: 2\n",
      "        embedding_dim: 256\n",
      "        mlp_dim: 2048\n",
      "        num_heads: 8\n",
      "      _target_: src.models.segment_anything.modeling.mask_decoder_dev.MaskDecoder\n",
      "      num_multimask_outputs: 3\n",
      "      transformer_dim: 256\n",
      "      iou_head_depth: 3\n",
      "      iou_head_hidden_dim: 256\n",
      "    _target_: src.models.magic_pen.bisam_diff.BiSamDiff\n",
      "  instance:\n",
      "    _target_: src.models.magic_pen.task.MagicPenModule\n",
      "    network: ${model.network}\n",
      "    params:\n",
      "      sam_ckpt_path: ${sam_ckpt_path}\n",
      "      use_weights:\n",
      "      - image_encoder\n",
      "      ft_mode: probing\n",
      "sam_name: small\n",
      "sam_ckpt_path: ${paths.sam_ckpt_dir}/sam_vit_b_01ec64.pth\n",
      "sam_enc_arch: vit-b\n",
      "tags:\n",
      "- dev\n",
      "task_name: train_probing_diff\n",
      "callbacks:\n",
      "  model_checkpoint:\n",
      "    _target_: lightning.pytorch.callbacks.ModelCheckpoint\n",
      "    dirpath: ${paths.output_dir}/checkpoints\n",
      "    filename: epoch_{epoch:03d}\n",
      "    monitor: val/loss\n",
      "    verbose: false\n",
      "    save_last: true\n",
      "    save_top_k: 1\n",
      "    mode: min\n",
      "    auto_insert_metric_name: false\n",
      "    save_weights_only: false\n",
      "    every_n_train_steps: null\n",
      "    train_time_interval: null\n",
      "    every_n_epochs: null\n",
      "    save_on_train_epoch_end: null\n",
      "  early_stopping:\n",
      "    _target_: lightning.pytorch.callbacks.EarlyStopping\n",
      "    monitor: val/valBinary_JaccardIndex\n",
      "    min_delta: 0.0\n",
      "    patience: 10\n",
      "    verbose: false\n",
      "    mode: min\n",
      "    strict: true\n",
      "    check_finite: true\n",
      "    stopping_threshold: null\n",
      "    divergence_threshold: null\n",
      "    check_on_train_epoch_end: null\n",
      "  model_summary:\n",
      "    _target_: lightning.pytorch.callbacks.RichModelSummary\n",
      "    max_depth: 1\n",
      "  rich_progress_bar:\n",
      "    _target_: lightning.pytorch.callbacks.RichProgressBar\n",
      "logger:\n",
      "  tensorboard:\n",
      "    _target_: lightning.pytorch.loggers.tensorboard.TensorBoardLogger\n",
      "    save_dir: ${paths.output_dir}/tensorboard/\n",
      "    name: null\n",
      "    default_hp_metric: false\n",
      "trainer:\n",
      "  _target_: lightning.pytorch.trainer.Trainer\n",
      "  default_root_dir: ${paths.output_dir}\n",
      "  min_epochs: 1\n",
      "  max_epochs: 100\n",
      "  accelerator: cpu\n",
      "  devices: 1\n",
      "  check_val_every_n_epoch: 1\n",
      "  deterministic: false\n",
      "  num_sanity_val_steps: 0\n",
      "paths:\n",
      "  root_dir: ${oc.env:PROJECT_PATH}\n",
      "  data_dir: ${oc.env:DATA_PATH}\n",
      "  log_dir: ${oc.env:LOGS_PATH}\n",
      "  output_dir: ${hydra:runtime.output_dir}\n",
      "  work_dir: ${hydra:runtime.cwd}\n",
      "  sam_ckpt_dir: ${oc.env:CHECKPOINTS_PATH}/sam\n",
      "extras:\n",
      "  ignore_warnings: false\n",
      "  enforce_tags: true\n",
      "  print_config: true\n",
      "train: true\n",
      "test: true\n",
      "ckpt_path: null\n",
      "seed: 66\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from hydra.core.global_hydra import GlobalHydra\n",
    "GlobalHydra.instance().clear()\n",
    "cfg = load_config()\n",
    "print(OmegaConf.to_yaml(cfg))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0085469-92bf-443c-890e-fe59d190cab1",
   "metadata": {},
   "source": [
    "### Load dloader manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20423931-96f3-4dd1-8417-94314d777911",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data.loader import BiTemporalDataset\n",
    "from src.data.process import DefaultTransform\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "params = {\n",
    "    \"prompt_type\": \"sample\",\n",
    "    \"n_prompt\": 1,\n",
    "    \"n_shape\":3,\n",
    "    \"loc\": \"center\",\n",
    "    \"batch_size\": 2,\n",
    "}\n",
    "ds = BiTemporalDataset(\n",
    "            name=\"levir-cd\",\n",
    "            dtype=\"test\",\n",
    "            transform=DefaultTransform(),\n",
    "            params=OmegaConf.create(params),\n",
    "        )\n",
    "\n",
    "dloader = data.DataLoader(\n",
    "            ds,\n",
    "            batch_size=params.get('batch_size'),\n",
    "            shuffle=False,\n",
    "            num_workers=0,\n",
    "        )\n",
    "\n",
    "batch = next(iter(dloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9ab573e-b6da-493c-8809-d21cab0de8ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INIT VIT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-05 16:11:18,889 - INFO ::  Weights loaded for : ['image_encoder']\n"
     ]
    }
   ],
   "source": [
    "module = hydra.utils.instantiate(cfg.model.instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a40adc78-2447-40b0-b917-d4aaf2ce5536",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"/var/data/usr/mdizier/stylo_magique/checkpoints/sam/sam_vit_b_01ec64.pth\"\n",
    "module.model.load_state_dict(torch.load(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27018744-78ab-46c9-bd4c-0838a70ecf99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-05 16:11:19,831 - INFO ::  build vit_b BiSam\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INIT VIT\n"
     ]
    }
   ],
   "source": [
    "from src.models.segment_any_change.model import BiSam\n",
    "\n",
    "bisam = load_sam(\n",
    "    model_type=\"vit_b\", model_cls=BiSam, version= \"dev\", device=\"cpu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef8d4a5e-cfd2-4f5c-8669-e90daaa9c542",
   "metadata": {},
   "outputs": [],
   "source": [
    "patcher = bisam.image_encoder.patch_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1635eaf5-da0e-4990-8cc6-80591d797e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_patches = patcher(batch[\"img_A\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d0a760e8-e8f4-4309-8970-dfb8f80ba532",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([768, 3, 16, 16])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(patcher.proj.named_parameters()))[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4670fa60-fb20-4585-aa90-f411a7652327",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 1024, 1024])\n",
      "torch.Size([2, 64, 64, 768])\n"
     ]
    }
   ],
   "source": [
    "print(batch[\"img_A\"].shape)\n",
    "print(img_patches.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8ad93796-237a-4164-8dbf-7d4d86476c39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3145728"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3*1024*1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "683e79b5-2573-4898-9298-8b01b811ae8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3145728"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "64*64*768"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97802503-16dd-4b94-9280-b51175ad30d9",
   "metadata": {},
   "source": [
    "### Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "72719db8-7b43-4243-bad9-e95978c1a229",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6cbd0c7e-46b5-4ef2-8f84-8e0e9474e390",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_heads = 8\n",
    "dim = 768\n",
    "qkv_bias=True\n",
    "head_dim = dim // num_heads\n",
    "scale = head_dim**-0.5\n",
    "\n",
    "qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "proj = nn.Linear(dim, dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d0085551-71a5-46e4-8e3d-c7448cfc59f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patches torch.Size([2, 64, 64, 768])\n"
     ]
    }
   ],
   "source": [
    "x = img_patches.clone()\n",
    "print(\"patches\", x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80489cf0-fa93-47fd-8079-a89754ec3214",
   "metadata": {},
   "source": [
    "* expand last dim (channel dimension) with Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "852107b5-5e3a-4973-bb79-51d4bbbe0dcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qkv_out torch.Size([2, 64, 64, 2304])\n",
      "qkv_out resh torch.Size([3, 2, 8, 4096, 96])\n",
      "q : torch.Size([16, 4096, 96])\n",
      "k : torch.Size([16, 4096, 96])\n",
      "v : torch.Size([16, 4096, 96])\n",
      "attn : torch.Size([16, 4096, 96])\n"
     ]
    }
   ],
   "source": [
    "B, H, W, _ = x.shape\n",
    "# qkv with shape (3, B, nHead, H * W, C)\n",
    "qkv_out = (\n",
    "    qkv(x)\n",
    ")\n",
    "print(\"qkv_out\", qkv_out.shape)\n",
    "qkv_out = qkv_out.reshape(B, H * W, 3, num_heads, -1).permute(2, 0, 3, 1, 4)\n",
    "print(\"qkv_out resh\", qkv_out.shape)\n",
    "# q, k, v with shape (B * nHead, H * W, C)\n",
    "q, k, v = qkv_out.reshape(3, B * num_heads, H * W, -1).unbind(0)\n",
    "print(\"q :\", q.shape)\n",
    "print(\"k :\", k.shape)\n",
    "print(\"v :\", v.shape)\n",
    "attn = (q * scale) @ k.transpose(-2, -1)\n",
    "print(\"attn :\", v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1c776310-4e39-4fa5-9e89-5b69c88fca9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 96, 4096])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k.transpose(-2, -1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c9263c15-4f4c-4a72-9f21-fc442826a04a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x : torch.Size([2, 64, 64, 768])\n",
      "out : torch.Size([2, 64, 64, 768])\n"
     ]
    }
   ],
   "source": [
    "attn = attn.softmax(dim=-1)\n",
    "x = (\n",
    "    (attn @ v)\n",
    "    .view(B, num_heads, H, W, -1)\n",
    "    .permute(0, 2, 3, 1, 4)\n",
    "    .reshape(B, H, W, -1)\n",
    ")\n",
    "print(\"x :\", x.shape)\n",
    "x = proj(x)\n",
    "print(\"out :\", x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ca8c5b-8445-49ef-991c-cd5cc2c3fd70",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
