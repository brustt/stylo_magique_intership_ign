{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de65c63-bc24-4208-8c6c-8d1974046e72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55ef0c01-51f2-409b-a0f2-eeb54ef6868a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "import pandas as pd\n",
    "import os\n",
    "from copy import deepcopy\n",
    "import torch.nn.functional as F\n",
    "from torchmetrics import Metric\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils import data\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "from src.commons.utils import to_numpy, SegAnyChangeVersion, show_img, show_pair_img, show_prediction_sample, resize\n",
    "from src.models.commons.mask_process import extract_object_from_batch, binarize_mask\n",
    "from src.commons.constants import IMG_SIZE\n",
    "from src.data.process import generate_grid_prompt\n",
    "from src.commons.utils import create_sample_grid_with_prompt, get_mask_with_prompt, fig2arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8bf87522-bd0a-46ca-9bf1-ac116d9603c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.commons.utils_io import load_sam\n",
    "from src.models.commons.bisam import BiSam2, SamModeInference\n",
    "from src.models.segment_any_change.model import BiSam\n",
    "\n",
    "from src.commons.utils import batch_to_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7f76d0d-eb3f-44d1-9cdd-ccfd0883880f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_prompts_on_mask(mask: torch.Tensor, batch, batch_idx: int):\n",
    "    if mask.shape[-1] != IMG_SIZE[0]:\n",
    "        mask = resize(mask, IMG_SIZE)\n",
    "    coord_points = batch[\"point_coords\"][batch_idx]\n",
    "    mask_pt = get_mask_with_prompt(binarize_mask(mask[batch_idx], th=0) , coord_points)\n",
    "    show_img(mask_pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf00ef9-0e8a-4854-8b9a-6f01d594668d",
   "metadata": {},
   "source": [
    "### Load dloader manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "83cb733b-d239-40b6-acbe-da6f31e5a613",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data.loader import BiTemporalDataset\n",
    "from src.data.process import DefaultTransform\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "params = {\n",
    "    \"prompt_type\": \"sample\",\n",
    "    \"n_prompt\": 1,\n",
    "    \"n_shape\":3,\n",
    "    \"loc\": \"center\",\n",
    "    \"batch_size\": 2,\n",
    "}\n",
    "ds = BiTemporalDataset(\n",
    "            name=\"levir-cd\",\n",
    "            dtype=\"test\",\n",
    "            transform=DefaultTransform(),\n",
    "            params=OmegaConf.create(params),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b422e0e5-3b21-4ea8-b091-6f4c5210ba82",
   "metadata": {},
   "outputs": [],
   "source": [
    "dloader = data.DataLoader(\n",
    "            ds,\n",
    "            batch_size=params.get('batch_size'),\n",
    "            shuffle=False,\n",
    "            num_workers=0,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c7ff7a1e-3953-44bf-8eab-28209256b0b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/MDizier/data/dl/levir-cd/test/label/test_1.png\n",
      "/home/MDizier/data/dl/levir-cd/test/label/test_2.png\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(dloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c03f3d-f94e-46a4-aae8-441b1d800684",
   "metadata": {},
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c729451c-a21a-4548-a425-912c301ad175",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-25 09:50:04,288 - INFO ::  build vit_b BiSam2\n"
     ]
    }
   ],
   "source": [
    "bisam2 = load_sam(\n",
    "    model_type=\"vit_b\", model_cls=BiSam2, version= \"dev2\", device=\"cpu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49bb8ac4-f2d9-45a3-b4e0-0bfd6265c4d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-25 09:50:05,274 - INFO ::  build vit_b BiSam\n"
     ]
    }
   ],
   "source": [
    "bisam = load_sam(\n",
    "    model_type=\"vit_b\", model_cls=BiSam, version= \"dev\", device=\"cpu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1b4dfc-ad6b-422b-9675-cbb85e31a440",
   "metadata": {},
   "source": [
    "### change model : many prompt to one mask :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52c759bd-eb63-41d4-bca1-688118808c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.commons.utils_io import load_config\n",
    "import hydra\n",
    "from hydra.core.global_hydra import GlobalHydra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18d285b6-6b7b-4a54-b0d3-3fa86282436a",
   "metadata": {},
   "outputs": [],
   "source": [
    "GlobalHydra.instance().clear()\n",
    "list_args=[\"experiment=mp_naive\", \"sam_type=small\", \"data=levir-cd\", \"data.params.n_shape=3\", \"data.params.num_worker=0\"]\n",
    "cfg = load_config(list_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0dc0fbf-5749-45bc-919a-a0b4e52f5103",
   "metadata": {},
   "outputs": [],
   "source": [
    "module = hydra.utils.instantiate(cfg.model.instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5ff5bb1-ffef-473f-b1db-3f1f1cd389b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg.data.params.num_worker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a7a4339e-c90a-4163-879f-3727059fbd41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.segment_anything.modeling.prompt_encoder import PromptEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "236c2758-6cd5-4a37-88e2-f46f08384911",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_encoder = module.model.prompt_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "67b21b40-3955-4415-8161-88b16e3ce80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_embed_dim = 512\n",
    "image_size = 1024\n",
    "vit_patch_size = 16\n",
    "image_embedding_size = image_size // vit_patch_size\n",
    "\n",
    "\n",
    "prompt_encoder_extent = PromptEncoder(\n",
    "    embed_dim=prompt_embed_dim,\n",
    "    image_embedding_size=(image_embedding_size, image_embedding_size),\n",
    "    input_image_size=(image_size, image_size),\n",
    "    mask_in_chans=16,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f65376-f191-465d-84b0-10ddfd77a9c0",
   "metadata": {},
   "source": [
    "#### Get& set new weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ecbd7d17-12cd-4345-9c23-a4e4186ce41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bf33417b-6db8-4c20-bbac-4e1803e9dc9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point_embeddings.0.weight torch.Size([1, 256])\n",
      "point_embeddings.1.weight torch.Size([1, 256])\n",
      "point_embeddings.2.weight torch.Size([1, 256])\n",
      "point_embeddings.3.weight torch.Size([1, 256])\n",
      "not_a_point_embed.weight torch.Size([1, 256])\n",
      "mask_downscaling.0.weight torch.Size([4, 1, 2, 2])\n",
      "mask_downscaling.0.bias torch.Size([4])\n",
      "mask_downscaling.1.weight torch.Size([4])\n",
      "mask_downscaling.1.bias torch.Size([4])\n",
      "mask_downscaling.3.weight torch.Size([16, 4, 2, 2])\n",
      "mask_downscaling.3.bias torch.Size([16])\n",
      "mask_downscaling.4.weight torch.Size([16])\n",
      "mask_downscaling.4.bias torch.Size([16])\n",
      "mask_downscaling.6.weight torch.Size([256, 16, 1, 1])\n",
      "mask_downscaling.6.bias torch.Size([256])\n",
      "no_mask_embed.weight torch.Size([1, 256])\n"
     ]
    }
   ],
   "source": [
    "for name, p in prompt_encoder.named_parameters():\n",
    "    print(name, p.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0c0b8fed-1d3d-491c-b7bf-9df83ace8f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = nn.Conv2d(1, 16 // 4, kernel_size=2, stride=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d7b8daed-48ab-406e-a687-d27f8bd102a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 1, 2, 2])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer.weight.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "792f5cb6-04b9-408b-9ac0-6244cc4a213e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point_embeddings.0.weight torch.Size([1, 256])\n",
      "point_embeddings.1.weight torch.Size([1, 256])\n",
      "point_embeddings.2.weight torch.Size([1, 256])\n",
      "point_embeddings.3.weight torch.Size([1, 256])\n",
      "not_a_point_embed.weight torch.Size([1, 256])\n",
      "mask_downscaling.0.weight torch.Size([4, 1, 2, 2])\n",
      "mask_downscaling.0.bias torch.Size([4])\n",
      "mask_downscaling.1.weight torch.Size([4])\n",
      "mask_downscaling.1.bias torch.Size([4])\n",
      "mask_downscaling.3.weight torch.Size([16, 4, 2, 2])\n",
      "mask_downscaling.3.bias torch.Size([16])\n",
      "mask_downscaling.4.weight torch.Size([16])\n",
      "mask_downscaling.4.bias torch.Size([16])\n",
      "mask_downscaling.6.weight torch.Size([256, 16, 1, 1])\n",
      "mask_downscaling.6.bias torch.Size([256])\n",
      "no_mask_embed.weight torch.Size([1, 256])\n"
     ]
    }
   ],
   "source": [
    "# copy\n",
    "new_weights = {}\n",
    "for name, p in prompt_encoder.named_parameters():\n",
    "    if \"weight\" in name:\n",
    "        if not \"mask_downscaling\" in name:\n",
    "            new_weights[name] = torch.cat([p, p], dim=1)\n",
    "        else:\n",
    "            new_weights[name] = p\n",
    "\n",
    "    print(name, p.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "90fffd9d-4ff4-4a6f-931f-fd065b377555",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['pe_layer.positional_encoding_gaussian_matrix', 'point_embeddings.0.weight', 'point_embeddings.1.weight', 'point_embeddings.2.weight', 'point_embeddings.3.weight', 'not_a_point_embed.weight', 'mask_downscaling.0.weight', 'mask_downscaling.0.bias', 'mask_downscaling.1.weight', 'mask_downscaling.1.bias', 'mask_downscaling.3.weight', 'mask_downscaling.3.bias', 'mask_downscaling.4.weight', 'mask_downscaling.4.bias', 'mask_downscaling.6.weight', 'mask_downscaling.6.bias', 'no_mask_embed.weight'])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict = prompt_encoder_extent.state_dict()\n",
    "state_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "85fa34d4-c342-412a-bee5-37c01faa3021",
   "metadata": {},
   "outputs": [],
   "source": [
    "for p, name in new_weights.items():\n",
    "    if name in state_dict:\n",
    "        print(f\"Found parameter '{name}' with shape {state_dict[name].shape}\")\n",
    "        state_dict[name] = p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f3f9a9-0881-4509-9b26-6e2ce64db52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if now we can"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ec3504f7-87e8-47ea-81c1-cecaf8c12aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_encoder = module.model.prompt_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "30c24050-7c1a-4236-9c2c-9418f543c305",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_encoder_extent.load_state_dict(state_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa80846-7253-4fbf-a714-164daaa22d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72155f5f-e769-46a0-ae21-97a6d08c98a2",
   "metadata": {},
   "source": [
    "### Mask decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "756a5d0c-8bc2-4121-a3a4-224dda688762",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.segment_anything.modeling.mask_decoder_dev import MaskDecoder\n",
    "from src.models.segment_anything.modeling.transformer_dev import TwoWayTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8beacd24-8051-4122-9047-193382700e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_decoder = module.model.mask_decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c9d0e4e1-fec5-41f7-b06a-76048b12f4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dim=512\n",
    "mask_decoder_extent=MaskDecoder(\n",
    "    num_multimask_outputs=3,\n",
    "    transformer=TwoWayTransformer(\n",
    "        depth=2,\n",
    "        embedding_dim=new_dim,\n",
    "        mlp_dim=2048,\n",
    "        num_heads=8,\n",
    "        ),\n",
    "    transformer_dim=new_dim,\n",
    "    iou_head_depth=3,\n",
    "    iou_head_hidden_dim=256,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "fd338101-bbec-404b-ae5f-d90e3af834e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.segment_anything.modeling.common import LayerNorm2d\n",
    "\n",
    "transformer_dim= 512\n",
    "activation=nn.GELU\n",
    "layer_up_extent = nn.Sequential(\n",
    "            nn.ConvTranspose2d(\n",
    "                transformer_dim, transformer_dim // 4, kernel_size=2, stride=2\n",
    "            ),\n",
    "            LayerNorm2d(transformer_dim // 4),\n",
    "            activation(),\n",
    "            nn.ConvTranspose2d(\n",
    "                transformer_dim // 4, transformer_dim // 8, kernel_size=2, stride=2\n",
    "            ),\n",
    "            activation(),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "08474c58-0980-46c7-94dc-4b3e2aa652cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extended : \n",
      "0.weight torch.Size([512, 128, 2, 2])\n",
      "0.bias torch.Size([128])\n",
      "1.weight torch.Size([128])\n",
      "1.bias torch.Size([128])\n",
      "3.weight torch.Size([128, 64, 2, 2])\n",
      "3.bias torch.Size([64])\n",
      "======\n",
      "old : \n",
      "0.weight torch.Size([256, 64, 2, 2])\n",
      "0.bias torch.Size([64])\n",
      "1.weight torch.Size([64])\n",
      "1.bias torch.Size([64])\n",
      "3.weight torch.Size([64, 32, 2, 2])\n",
      "3.bias torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "print(\"extended : \")\n",
    "for n, l2 in layer_up_extent.named_parameters():\n",
    "    print(n, l2.shape)\n",
    "print(\"======\")\n",
    "print(\"old : \")\n",
    "for n, l2 in mask_decoder.output_upscaling.named_parameters():\n",
    "    print(n, l2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "39051251-0652-43b0-bfe1-a966ff51f611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvTranspose2d(512, 128, kernel_size=(2, 2), stride=(2, 2))\n",
      "LayerNorm2d()\n",
      "GELU(approximate='none')\n",
      "ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))\n",
      "GELU(approximate='none')\n"
     ]
    }
   ],
   "source": [
    "for layer in layer_up_extent.children():\n",
    "    print(layer)\n",
    "    if isinstance(layer, nn.Conv2d):\n",
    "        print(layer.weight.data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "fb3f4712-6106-4e5f-98a9-78a79e870290",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512, 128, 2, 2])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_up_extent[0].weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "189a103d-b53f-4bc0-bf8a-94c10a49c97f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([256, 64, 2, 2])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_decoder.output_upscaling[0].weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "d2735fa1-ed45-4836-9f29-0de08fd94ef8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.nn.modules.container.Sequential"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(layer_up_extent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "4cbd3c2a-1fed-40bc-bb30-0c515164a789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 64, 2, 2])\n",
      "torch.Size([64, 32, 2, 2])\n",
      "Found parameter '0.weight' with shape torch.Size([512, 128, 2, 2])\n",
      "Found parameter '0.bias' with shape torch.Size([128])\n",
      "Found parameter '1.weight' with shape torch.Size([128])\n",
      "Found parameter '1.bias' with shape torch.Size([128])\n",
      "Found parameter '3.weight' with shape torch.Size([128, 64, 2, 2])\n",
      "Found parameter '3.bias' with shape torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "state_dict = mask_decoder.output_upscaling.state_dict()\n",
    "\n",
    "new_weights = {}\n",
    "\n",
    "for name, p in  mask_decoder.output_upscaling.named_parameters():\n",
    "    if \"weight\" in name and p.ndim > 1:\n",
    "        print(p.shape)\n",
    "        #new_weights[name] = torch.cat([p, p], dim)\n",
    "        new_weights[name] = p.repeat(2, 1, 1, 1).repeat(1, 2, 1, 1)\n",
    "    else:\n",
    "        new_weights[name] = torch.cat([p, p])\n",
    "for name, p in new_weights.items():\n",
    "    if name in state_dict:\n",
    "        print(f\"Found parameter '{name}' with shape {p.shape}\")\n",
    "        state_dict[name] = p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "c3d43f78-cb0a-4824-9586-ceabb5c3a066",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_up_extent.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c0dba4-1d3f-44a9-bb37-85cef48fa014",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "d6cdc225-fa95-4673-8e96-b04a6393aaec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "im shape : torch.Size([1, 512, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "img_A = torch.rand(1, 256, 64, 64)\n",
    "img_B = torch.rand(1, 256, 64, 64)\n",
    "img = torch.cat([img_A, img_B], dim=1)\n",
    "print(f\"im shape : {img.shape}\")\n",
    "out = layer_up_extent(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e66ad2-bafb-4d97-9f87-08056d7d4a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy\n",
    "new_weights = {}\n",
    "for name, p in mask_decoder.named_parameters():\n",
    "    if \"weight\" in name:\n",
    "        if not \"mask_downscaling\" in name:\n",
    "            new_weights[name] = torch.cat([p, p], dim=1)\n",
    "        else:\n",
    "            new_weights[name] = p\n",
    "\n",
    "    print(name, p.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "b1906d96-9504-420d-ac09-b40434e1411c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformer.layers.0.self_attn.q_proj.weight torch.Size([512, 512])\n",
      "transformer.layers.0.self_attn.q_proj.bias torch.Size([512])\n",
      "transformer.layers.0.self_attn.k_proj.weight torch.Size([512, 512])\n",
      "transformer.layers.0.self_attn.k_proj.bias torch.Size([512])\n",
      "transformer.layers.0.self_attn.v_proj.weight torch.Size([512, 512])\n",
      "transformer.layers.0.self_attn.v_proj.bias torch.Size([512])\n",
      "transformer.layers.0.self_attn.out_proj.weight torch.Size([512, 512])\n",
      "transformer.layers.0.self_attn.out_proj.bias torch.Size([512])\n",
      "transformer.layers.0.norm1.weight torch.Size([512])\n",
      "transformer.layers.0.norm1.bias torch.Size([512])\n",
      "transformer.layers.0.cross_attn_token_to_image.q_proj.weight torch.Size([256, 512])\n",
      "transformer.layers.0.cross_attn_token_to_image.q_proj.bias torch.Size([256])\n",
      "transformer.layers.0.cross_attn_token_to_image.k_proj.weight torch.Size([256, 512])\n",
      "transformer.layers.0.cross_attn_token_to_image.k_proj.bias torch.Size([256])\n",
      "transformer.layers.0.cross_attn_token_to_image.v_proj.weight torch.Size([256, 512])\n",
      "transformer.layers.0.cross_attn_token_to_image.v_proj.bias torch.Size([256])\n",
      "transformer.layers.0.cross_attn_token_to_image.out_proj.weight torch.Size([512, 256])\n",
      "transformer.layers.0.cross_attn_token_to_image.out_proj.bias torch.Size([512])\n",
      "transformer.layers.0.norm2.weight torch.Size([512])\n",
      "transformer.layers.0.norm2.bias torch.Size([512])\n",
      "transformer.layers.0.mlp.lin1.weight torch.Size([2048, 512])\n",
      "transformer.layers.0.mlp.lin1.bias torch.Size([2048])\n",
      "transformer.layers.0.mlp.lin2.weight torch.Size([512, 2048])\n",
      "transformer.layers.0.mlp.lin2.bias torch.Size([512])\n",
      "transformer.layers.0.norm3.weight torch.Size([512])\n",
      "transformer.layers.0.norm3.bias torch.Size([512])\n",
      "transformer.layers.0.norm4.weight torch.Size([512])\n",
      "transformer.layers.0.norm4.bias torch.Size([512])\n",
      "transformer.layers.0.cross_attn_image_to_token.q_proj.weight torch.Size([256, 512])\n",
      "transformer.layers.0.cross_attn_image_to_token.q_proj.bias torch.Size([256])\n",
      "transformer.layers.0.cross_attn_image_to_token.k_proj.weight torch.Size([256, 512])\n",
      "transformer.layers.0.cross_attn_image_to_token.k_proj.bias torch.Size([256])\n",
      "transformer.layers.0.cross_attn_image_to_token.v_proj.weight torch.Size([256, 512])\n",
      "transformer.layers.0.cross_attn_image_to_token.v_proj.bias torch.Size([256])\n",
      "transformer.layers.0.cross_attn_image_to_token.out_proj.weight torch.Size([512, 256])\n",
      "transformer.layers.0.cross_attn_image_to_token.out_proj.bias torch.Size([512])\n",
      "transformer.layers.1.self_attn.q_proj.weight torch.Size([512, 512])\n",
      "transformer.layers.1.self_attn.q_proj.bias torch.Size([512])\n",
      "transformer.layers.1.self_attn.k_proj.weight torch.Size([512, 512])\n",
      "transformer.layers.1.self_attn.k_proj.bias torch.Size([512])\n",
      "transformer.layers.1.self_attn.v_proj.weight torch.Size([512, 512])\n",
      "transformer.layers.1.self_attn.v_proj.bias torch.Size([512])\n",
      "transformer.layers.1.self_attn.out_proj.weight torch.Size([512, 512])\n",
      "transformer.layers.1.self_attn.out_proj.bias torch.Size([512])\n",
      "transformer.layers.1.norm1.weight torch.Size([512])\n",
      "transformer.layers.1.norm1.bias torch.Size([512])\n",
      "transformer.layers.1.cross_attn_token_to_image.q_proj.weight torch.Size([256, 512])\n",
      "transformer.layers.1.cross_attn_token_to_image.q_proj.bias torch.Size([256])\n",
      "transformer.layers.1.cross_attn_token_to_image.k_proj.weight torch.Size([256, 512])\n",
      "transformer.layers.1.cross_attn_token_to_image.k_proj.bias torch.Size([256])\n",
      "transformer.layers.1.cross_attn_token_to_image.v_proj.weight torch.Size([256, 512])\n",
      "transformer.layers.1.cross_attn_token_to_image.v_proj.bias torch.Size([256])\n",
      "transformer.layers.1.cross_attn_token_to_image.out_proj.weight torch.Size([512, 256])\n",
      "transformer.layers.1.cross_attn_token_to_image.out_proj.bias torch.Size([512])\n",
      "transformer.layers.1.norm2.weight torch.Size([512])\n",
      "transformer.layers.1.norm2.bias torch.Size([512])\n",
      "transformer.layers.1.mlp.lin1.weight torch.Size([2048, 512])\n",
      "transformer.layers.1.mlp.lin1.bias torch.Size([2048])\n",
      "transformer.layers.1.mlp.lin2.weight torch.Size([512, 2048])\n",
      "transformer.layers.1.mlp.lin2.bias torch.Size([512])\n",
      "transformer.layers.1.norm3.weight torch.Size([512])\n",
      "transformer.layers.1.norm3.bias torch.Size([512])\n",
      "transformer.layers.1.norm4.weight torch.Size([512])\n",
      "transformer.layers.1.norm4.bias torch.Size([512])\n",
      "transformer.layers.1.cross_attn_image_to_token.q_proj.weight torch.Size([256, 512])\n",
      "transformer.layers.1.cross_attn_image_to_token.q_proj.bias torch.Size([256])\n",
      "transformer.layers.1.cross_attn_image_to_token.k_proj.weight torch.Size([256, 512])\n",
      "transformer.layers.1.cross_attn_image_to_token.k_proj.bias torch.Size([256])\n",
      "transformer.layers.1.cross_attn_image_to_token.v_proj.weight torch.Size([256, 512])\n",
      "transformer.layers.1.cross_attn_image_to_token.v_proj.bias torch.Size([256])\n",
      "transformer.layers.1.cross_attn_image_to_token.out_proj.weight torch.Size([512, 256])\n",
      "transformer.layers.1.cross_attn_image_to_token.out_proj.bias torch.Size([512])\n",
      "transformer.final_attn_token_to_image.q_proj.weight torch.Size([256, 512])\n",
      "transformer.final_attn_token_to_image.q_proj.bias torch.Size([256])\n",
      "transformer.final_attn_token_to_image.k_proj.weight torch.Size([256, 512])\n",
      "transformer.final_attn_token_to_image.k_proj.bias torch.Size([256])\n",
      "transformer.final_attn_token_to_image.v_proj.weight torch.Size([256, 512])\n",
      "transformer.final_attn_token_to_image.v_proj.bias torch.Size([256])\n",
      "transformer.final_attn_token_to_image.out_proj.weight torch.Size([512, 256])\n",
      "transformer.final_attn_token_to_image.out_proj.bias torch.Size([512])\n",
      "transformer.norm_final_attn.weight torch.Size([512])\n",
      "transformer.norm_final_attn.bias torch.Size([512])\n",
      "iou_token.weight torch.Size([1, 512])\n",
      "mask_tokens.weight torch.Size([4, 512])\n",
      "output_upscaling.0.weight torch.Size([512, 128, 2, 2])\n",
      "output_upscaling.0.bias torch.Size([128])\n",
      "output_upscaling.1.weight torch.Size([128])\n",
      "output_upscaling.1.bias torch.Size([128])\n",
      "output_upscaling.3.weight torch.Size([128, 64, 2, 2])\n",
      "output_upscaling.3.bias torch.Size([64])\n",
      "output_hypernetworks_mlps.0.layers.0.weight torch.Size([512, 512])\n",
      "output_hypernetworks_mlps.0.layers.0.bias torch.Size([512])\n",
      "output_hypernetworks_mlps.0.layers.1.weight torch.Size([512, 512])\n",
      "output_hypernetworks_mlps.0.layers.1.bias torch.Size([512])\n",
      "output_hypernetworks_mlps.0.layers.2.weight torch.Size([64, 512])\n",
      "output_hypernetworks_mlps.0.layers.2.bias torch.Size([64])\n",
      "output_hypernetworks_mlps.1.layers.0.weight torch.Size([512, 512])\n",
      "output_hypernetworks_mlps.1.layers.0.bias torch.Size([512])\n",
      "output_hypernetworks_mlps.1.layers.1.weight torch.Size([512, 512])\n",
      "output_hypernetworks_mlps.1.layers.1.bias torch.Size([512])\n",
      "output_hypernetworks_mlps.1.layers.2.weight torch.Size([64, 512])\n",
      "output_hypernetworks_mlps.1.layers.2.bias torch.Size([64])\n",
      "output_hypernetworks_mlps.2.layers.0.weight torch.Size([512, 512])\n",
      "output_hypernetworks_mlps.2.layers.0.bias torch.Size([512])\n",
      "output_hypernetworks_mlps.2.layers.1.weight torch.Size([512, 512])\n",
      "output_hypernetworks_mlps.2.layers.1.bias torch.Size([512])\n",
      "output_hypernetworks_mlps.2.layers.2.weight torch.Size([64, 512])\n",
      "output_hypernetworks_mlps.2.layers.2.bias torch.Size([64])\n",
      "output_hypernetworks_mlps.3.layers.0.weight torch.Size([512, 512])\n",
      "output_hypernetworks_mlps.3.layers.0.bias torch.Size([512])\n",
      "output_hypernetworks_mlps.3.layers.1.weight torch.Size([512, 512])\n",
      "output_hypernetworks_mlps.3.layers.1.bias torch.Size([512])\n",
      "output_hypernetworks_mlps.3.layers.2.weight torch.Size([64, 512])\n",
      "output_hypernetworks_mlps.3.layers.2.bias torch.Size([64])\n",
      "iou_prediction_head.layers.0.weight torch.Size([256, 512])\n",
      "iou_prediction_head.layers.0.bias torch.Size([256])\n",
      "iou_prediction_head.layers.1.weight torch.Size([256, 256])\n",
      "iou_prediction_head.layers.1.bias torch.Size([256])\n",
      "iou_prediction_head.layers.2.weight torch.Size([4, 256])\n",
      "iou_prediction_head.layers.2.bias torch.Size([4])\n"
     ]
    }
   ],
   "source": [
    "for name, p in mask_decoder_extent.named_parameters():\n",
    "    print(name, p.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e67c43-3886-402e-897a-523d095d6e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = torch.tensor([10, 10])\n",
    "t2 = torch.tensor([20, 20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a47f2cdb-f8f5-464d-ae03-1011d5bee3fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformer.layers.0.self_attn.q_proj.weight torch.Size([256, 256])\n",
      "transformer.layers.0.self_attn.q_proj.bias torch.Size([256])\n",
      "transformer.layers.0.self_attn.k_proj.weight torch.Size([256, 256])\n",
      "transformer.layers.0.self_attn.k_proj.bias torch.Size([256])\n",
      "transformer.layers.0.self_attn.v_proj.weight torch.Size([256, 256])\n",
      "transformer.layers.0.self_attn.v_proj.bias torch.Size([256])\n",
      "transformer.layers.0.self_attn.out_proj.weight torch.Size([256, 256])\n",
      "transformer.layers.0.self_attn.out_proj.bias torch.Size([256])\n",
      "transformer.layers.0.norm1.weight torch.Size([256])\n",
      "transformer.layers.0.norm1.bias torch.Size([256])\n",
      "transformer.layers.0.cross_attn_token_to_image.q_proj.weight torch.Size([128, 256])\n",
      "transformer.layers.0.cross_attn_token_to_image.q_proj.bias torch.Size([128])\n",
      "transformer.layers.0.cross_attn_token_to_image.k_proj.weight torch.Size([128, 256])\n",
      "transformer.layers.0.cross_attn_token_to_image.k_proj.bias torch.Size([128])\n",
      "transformer.layers.0.cross_attn_token_to_image.v_proj.weight torch.Size([128, 256])\n",
      "transformer.layers.0.cross_attn_token_to_image.v_proj.bias torch.Size([128])\n",
      "transformer.layers.0.cross_attn_token_to_image.out_proj.weight torch.Size([256, 128])\n",
      "transformer.layers.0.cross_attn_token_to_image.out_proj.bias torch.Size([256])\n",
      "transformer.layers.0.norm2.weight torch.Size([256])\n",
      "transformer.layers.0.norm2.bias torch.Size([256])\n",
      "transformer.layers.0.mlp.lin1.weight torch.Size([2048, 256])\n",
      "transformer.layers.0.mlp.lin1.bias torch.Size([2048])\n",
      "transformer.layers.0.mlp.lin2.weight torch.Size([256, 2048])\n",
      "transformer.layers.0.mlp.lin2.bias torch.Size([256])\n",
      "transformer.layers.0.norm3.weight torch.Size([256])\n",
      "transformer.layers.0.norm3.bias torch.Size([256])\n",
      "transformer.layers.0.norm4.weight torch.Size([256])\n",
      "transformer.layers.0.norm4.bias torch.Size([256])\n",
      "transformer.layers.0.cross_attn_image_to_token.q_proj.weight torch.Size([128, 256])\n",
      "transformer.layers.0.cross_attn_image_to_token.q_proj.bias torch.Size([128])\n",
      "transformer.layers.0.cross_attn_image_to_token.k_proj.weight torch.Size([128, 256])\n",
      "transformer.layers.0.cross_attn_image_to_token.k_proj.bias torch.Size([128])\n",
      "transformer.layers.0.cross_attn_image_to_token.v_proj.weight torch.Size([128, 256])\n",
      "transformer.layers.0.cross_attn_image_to_token.v_proj.bias torch.Size([128])\n",
      "transformer.layers.0.cross_attn_image_to_token.out_proj.weight torch.Size([256, 128])\n",
      "transformer.layers.0.cross_attn_image_to_token.out_proj.bias torch.Size([256])\n",
      "transformer.layers.1.self_attn.q_proj.weight torch.Size([256, 256])\n",
      "transformer.layers.1.self_attn.q_proj.bias torch.Size([256])\n",
      "transformer.layers.1.self_attn.k_proj.weight torch.Size([256, 256])\n",
      "transformer.layers.1.self_attn.k_proj.bias torch.Size([256])\n",
      "transformer.layers.1.self_attn.v_proj.weight torch.Size([256, 256])\n",
      "transformer.layers.1.self_attn.v_proj.bias torch.Size([256])\n",
      "transformer.layers.1.self_attn.out_proj.weight torch.Size([256, 256])\n",
      "transformer.layers.1.self_attn.out_proj.bias torch.Size([256])\n",
      "transformer.layers.1.norm1.weight torch.Size([256])\n",
      "transformer.layers.1.norm1.bias torch.Size([256])\n",
      "transformer.layers.1.cross_attn_token_to_image.q_proj.weight torch.Size([128, 256])\n",
      "transformer.layers.1.cross_attn_token_to_image.q_proj.bias torch.Size([128])\n",
      "transformer.layers.1.cross_attn_token_to_image.k_proj.weight torch.Size([128, 256])\n",
      "transformer.layers.1.cross_attn_token_to_image.k_proj.bias torch.Size([128])\n",
      "transformer.layers.1.cross_attn_token_to_image.v_proj.weight torch.Size([128, 256])\n",
      "transformer.layers.1.cross_attn_token_to_image.v_proj.bias torch.Size([128])\n",
      "transformer.layers.1.cross_attn_token_to_image.out_proj.weight torch.Size([256, 128])\n",
      "transformer.layers.1.cross_attn_token_to_image.out_proj.bias torch.Size([256])\n",
      "transformer.layers.1.norm2.weight torch.Size([256])\n",
      "transformer.layers.1.norm2.bias torch.Size([256])\n",
      "transformer.layers.1.mlp.lin1.weight torch.Size([2048, 256])\n",
      "transformer.layers.1.mlp.lin1.bias torch.Size([2048])\n",
      "transformer.layers.1.mlp.lin2.weight torch.Size([256, 2048])\n",
      "transformer.layers.1.mlp.lin2.bias torch.Size([256])\n",
      "transformer.layers.1.norm3.weight torch.Size([256])\n",
      "transformer.layers.1.norm3.bias torch.Size([256])\n",
      "transformer.layers.1.norm4.weight torch.Size([256])\n",
      "transformer.layers.1.norm4.bias torch.Size([256])\n",
      "transformer.layers.1.cross_attn_image_to_token.q_proj.weight torch.Size([128, 256])\n",
      "transformer.layers.1.cross_attn_image_to_token.q_proj.bias torch.Size([128])\n",
      "transformer.layers.1.cross_attn_image_to_token.k_proj.weight torch.Size([128, 256])\n",
      "transformer.layers.1.cross_attn_image_to_token.k_proj.bias torch.Size([128])\n",
      "transformer.layers.1.cross_attn_image_to_token.v_proj.weight torch.Size([128, 256])\n",
      "transformer.layers.1.cross_attn_image_to_token.v_proj.bias torch.Size([128])\n",
      "transformer.layers.1.cross_attn_image_to_token.out_proj.weight torch.Size([256, 128])\n",
      "transformer.layers.1.cross_attn_image_to_token.out_proj.bias torch.Size([256])\n",
      "transformer.final_attn_token_to_image.q_proj.weight torch.Size([128, 256])\n",
      "transformer.final_attn_token_to_image.q_proj.bias torch.Size([128])\n",
      "transformer.final_attn_token_to_image.k_proj.weight torch.Size([128, 256])\n",
      "transformer.final_attn_token_to_image.k_proj.bias torch.Size([128])\n",
      "transformer.final_attn_token_to_image.v_proj.weight torch.Size([128, 256])\n",
      "transformer.final_attn_token_to_image.v_proj.bias torch.Size([128])\n",
      "transformer.final_attn_token_to_image.out_proj.weight torch.Size([256, 128])\n",
      "transformer.final_attn_token_to_image.out_proj.bias torch.Size([256])\n",
      "transformer.norm_final_attn.weight torch.Size([256])\n",
      "transformer.norm_final_attn.bias torch.Size([256])\n",
      "iou_token.weight torch.Size([1, 256])\n",
      "mask_tokens.weight torch.Size([4, 256])\n",
      "output_upscaling.0.weight torch.Size([256, 64, 2, 2])\n",
      "output_upscaling.0.bias torch.Size([64])\n",
      "output_upscaling.1.weight torch.Size([64])\n",
      "output_upscaling.1.bias torch.Size([64])\n",
      "output_upscaling.3.weight torch.Size([64, 32, 2, 2])\n",
      "output_upscaling.3.bias torch.Size([32])\n",
      "output_hypernetworks_mlps.0.layers.0.weight torch.Size([256, 256])\n",
      "output_hypernetworks_mlps.0.layers.0.bias torch.Size([256])\n",
      "output_hypernetworks_mlps.0.layers.1.weight torch.Size([256, 256])\n",
      "output_hypernetworks_mlps.0.layers.1.bias torch.Size([256])\n",
      "output_hypernetworks_mlps.0.layers.2.weight torch.Size([32, 256])\n",
      "output_hypernetworks_mlps.0.layers.2.bias torch.Size([32])\n",
      "output_hypernetworks_mlps.1.layers.0.weight torch.Size([256, 256])\n",
      "output_hypernetworks_mlps.1.layers.0.bias torch.Size([256])\n",
      "output_hypernetworks_mlps.1.layers.1.weight torch.Size([256, 256])\n",
      "output_hypernetworks_mlps.1.layers.1.bias torch.Size([256])\n",
      "output_hypernetworks_mlps.1.layers.2.weight torch.Size([32, 256])\n",
      "output_hypernetworks_mlps.1.layers.2.bias torch.Size([32])\n",
      "output_hypernetworks_mlps.2.layers.0.weight torch.Size([256, 256])\n",
      "output_hypernetworks_mlps.2.layers.0.bias torch.Size([256])\n",
      "output_hypernetworks_mlps.2.layers.1.weight torch.Size([256, 256])\n",
      "output_hypernetworks_mlps.2.layers.1.bias torch.Size([256])\n",
      "output_hypernetworks_mlps.2.layers.2.weight torch.Size([32, 256])\n",
      "output_hypernetworks_mlps.2.layers.2.bias torch.Size([32])\n",
      "output_hypernetworks_mlps.3.layers.0.weight torch.Size([256, 256])\n",
      "output_hypernetworks_mlps.3.layers.0.bias torch.Size([256])\n",
      "output_hypernetworks_mlps.3.layers.1.weight torch.Size([256, 256])\n",
      "output_hypernetworks_mlps.3.layers.1.bias torch.Size([256])\n",
      "output_hypernetworks_mlps.3.layers.2.weight torch.Size([32, 256])\n",
      "output_hypernetworks_mlps.3.layers.2.bias torch.Size([32])\n",
      "iou_prediction_head.layers.0.weight torch.Size([256, 256])\n",
      "iou_prediction_head.layers.0.bias torch.Size([256])\n",
      "iou_prediction_head.layers.1.weight torch.Size([256, 256])\n",
      "iou_prediction_head.layers.1.bias torch.Size([256])\n",
      "iou_prediction_head.layers.2.weight torch.Size([4, 256])\n",
      "iou_prediction_head.layers.2.bias torch.Size([4])\n"
     ]
    }
   ],
   "source": [
    "for name, p in mask_decoder.named_parameters():\n",
    "    print(name, p.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378704d7-bd17-46b3-b21e-dc86163f0f53",
   "metadata": {},
   "source": [
    "Compliqué d'aligner les dimenions, i.e de copier les poids. Faisons simple pour l'instant : MLP qui projette la concatentaion dans la dimension d'entrée du transformer (256). On initialisera le reseau pour renvoyer une des images ($f(i1, i2) = i1$)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07fa7011-7673-4436-a39d-df193c16e9bf",
   "metadata": {},
   "source": [
    "## Concat + proj layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3090a223-0186-432f-bdbc-77d06f34e6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.magic_pen.bisam_concat import BiSamConcat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8b92494-17e9-43ba-9391-d3ad84e37dcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-25 17:35:18,550 - INFO ::  build vit_b BiSamConcat\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at alloc_cpu.cpp:75] err == 0. DefaultCPUAllocator: can't allocate memory: you tried to allocate 8796093022208 bytes. Error code 12 (Cannot allocate memory)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m bisam_concat \u001b[38;5;241m=\u001b[39m \u001b[43mload_sam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvit_b\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBiSamConcat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mversion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdev\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_strict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/stage_stylo_magique_2024/src/commons/utils_io.py:51\u001b[0m, in \u001b[0;36mload_sam\u001b[0;34m(model_type, model_cls, version, device, is_strict)\u001b[0m\n\u001b[1;32m     47\u001b[0m     sam \u001b[38;5;241m=\u001b[39m sam_model_registry_v2[model_type](\n\u001b[1;32m     48\u001b[0m         checkpoint\u001b[38;5;241m=\u001b[39mSAM_DICT_CHECKPOINT[model_type], model\u001b[38;5;241m=\u001b[39mmodel_cls, is_strict\u001b[38;5;241m=\u001b[39mis_strict\n\u001b[1;32m     49\u001b[0m     )\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mcase\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdev\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 51\u001b[0m     sam \u001b[38;5;241m=\u001b[39m \u001b[43msam_model_registry\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSAM_DICT_CHECKPOINT\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_cls\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_strict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_strict\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mcase\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     55\u001b[0m     sam \u001b[38;5;241m=\u001b[39m sam_model_registry_v0[model_type](\n\u001b[1;32m     56\u001b[0m         checkpoint\u001b[38;5;241m=\u001b[39mSAM_DICT_CHECKPOINT[model_type], is_strict\u001b[38;5;241m=\u001b[39mis_strict\n\u001b[1;32m     57\u001b[0m     )\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "File \u001b[0;32m~/projects/stage_stylo_magique_2024/src/models/segment_anything/build_sam_dev.py:62\u001b[0m, in \u001b[0;36mbuild_sam_vit_b\u001b[0;34m(checkpoint, model, is_strict)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbuild_sam_vit_b\u001b[39m(checkpoint\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, model\u001b[38;5;241m=\u001b[39mSam, is_strict:\u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m     60\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbuild vit_b \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 62\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_build_sam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_embed_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m768\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_depth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m12\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_num_heads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m12\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_global_attn_indexes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m11\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheckpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_strict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_strict\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/stage_stylo_magique_2024/src/models/segment_anything/build_sam_dev.py:98\u001b[0m, in \u001b[0;36m_build_sam\u001b[0;34m(encoder_embed_dim, encoder_depth, encoder_num_heads, encoder_global_attn_indexes, model, checkpoint, is_strict)\u001b[0m\n\u001b[1;32m     96\u001b[0m vit_patch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m16\u001b[39m\n\u001b[1;32m     97\u001b[0m image_embedding_size \u001b[38;5;241m=\u001b[39m image_size \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m vit_patch_size\n\u001b[0;32m---> 98\u001b[0m sam \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimage_encoder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mImageEncoderViT\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdepth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_depth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[43m        \u001b[49m\u001b[43membed_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_embed_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimg_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmlp_ratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnorm_layer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLayerNorm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-6\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_num_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvit_patch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[43m        \u001b[49m\u001b[43mqkv_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_rel_pos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[43m        \u001b[49m\u001b[43mglobal_attn_indexes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_global_attn_indexes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwindow_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m14\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[43m        \u001b[49m\u001b[43mout_chans\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt_embed_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt_encoder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPromptEncoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[43m        \u001b[49m\u001b[43membed_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt_embed_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimage_embedding_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mimage_embedding_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_embedding_size\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_image_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mimage_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_size\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmask_in_chans\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmask_decoder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMaskDecoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_multimask_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtransformer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTwoWayTransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdepth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[43m            \u001b[49m\u001b[43membedding_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt_embed_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmlp_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2048\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtransformer_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt_embed_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m        \u001b[49m\u001b[43miou_head_depth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m        \u001b[49m\u001b[43miou_head_hidden_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# pixel_mean=[123.675, 116.28, 103.53],\u001b[39;49;00m\n\u001b[1;32m    132\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# pixel_std=[58.395, 57.12, 57.375],\u001b[39;49;00m\n\u001b[1;32m    133\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    134\u001b[0m sam\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m checkpoint \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/projects/stage_stylo_magique_2024/src/models/magic_pen/bisam_concat.py:57\u001b[0m, in \u001b[0;36mBiSamConcat.__init__\u001b[0;34m(self, image_encoder, prompt_encoder, mask_decoder, embedding_dim)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprompt_encoder \u001b[38;5;241m=\u001b[39m prompt_encoder\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmask_decoder \u001b[38;5;241m=\u001b[39m mask_decoder\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproj_layer \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mSequential(\n\u001b[0;32m---> 57\u001b[0m     \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLinear\u001b[49m\u001b[43m(\u001b[49m\u001b[43membedding_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_dim\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m,\n\u001b[1;32m     58\u001b[0m     nn\u001b[38;5;241m.\u001b[39mGELU(),\n\u001b[1;32m     59\u001b[0m     nn\u001b[38;5;241m.\u001b[39mLinear(embedding_dim \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m, embedding_dim \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m),\n\u001b[1;32m     60\u001b[0m )\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mregister_buffer(\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpixel_mean\u001b[39m\u001b[38;5;124m\"\u001b[39m, torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;241m123.675\u001b[39m, \u001b[38;5;241m116.28\u001b[39m, \u001b[38;5;241m103.53\u001b[39m])\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m), \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     65\u001b[0m )\n",
      "File \u001b[0;32m~/miniforge3/envs/mp/lib/python3.10/site-packages/torch/nn/modules/linear.py:96\u001b[0m, in \u001b[0;36mLinear.__init__\u001b[0;34m(self, in_features, out_features, bias, device, dtype)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_features \u001b[38;5;241m=\u001b[39m in_features\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_features \u001b[38;5;241m=\u001b[39m out_features\n\u001b[0;32m---> 96\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight \u001b[38;5;241m=\u001b[39m Parameter(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_features\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfactory_kwargs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m bias:\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;241m=\u001b[39m Parameter(torch\u001b[38;5;241m.\u001b[39mempty(out_features, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfactory_kwargs))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: [enforce fail at alloc_cpu.cpp:75] err == 0. DefaultCPUAllocator: can't allocate memory: you tried to allocate 8796093022208 bytes. Error code 12 (Cannot allocate memory)"
     ]
    }
   ],
   "source": [
    "bisam_concat = load_sam(model_type=\"vit_b\", model_cls=BiSamConcat, version=\"dev\", is_strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d2679f5a-88b1-47fd-a200-fc8f508e7ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = torch.rand(1, 256, 64, 64)\n",
    "t2 = torch.rand(1, 256, 64, 64)\n",
    "\n",
    "t = torch.cat([t1, t2], dim=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "10449125-2807-4695-8cb5-1649b0d1715a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "embedding_dim = 512\n",
    "proj_layer = nn.Sequential(\n",
    "    nn.Linear(embedding_dim, embedding_dim // 2),\n",
    "    nn.GELU(),\n",
    "    nn.Linear(embedding_dim // 2, embedding_dim // 2),\n",
    ")\n",
    "nt = t.permute(0, 2, 3, 1)\n",
    "out = proj_layer(nt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f78248f9-8890-4826-b7f8-9c32ecb3c0df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([256, 256])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proj_layer[2].weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7a9209-1d66-42fe-b5f2-591dcf65ef8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bad48d83-f089-4e13-88df-132f1f42e3e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64, 64, 256])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "29354c10-3b12-4a9e-b76a-637a3ddf60fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=512, out_features=256, bias=True)\n",
       "  (1): GELU(approximate='none')\n",
       "  (2): Linear(in_features=256, out_features=256, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bisam_concat.proj_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d9eafe66-5889-4eb0-8804-2fd4e8f2fd0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([256, 256])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bisam_concat.proj_layer[2].weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "11ec7666-7856-43bd-be0e-c17642f8feeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.1032, 0.7729, 0.2045,  ..., 0.2749, 0.0041, 0.5167],\n",
       "         [0.0074, 0.6442, 0.1886,  ..., 0.9020, 0.1105, 0.5572],\n",
       "         [0.0708, 0.1315, 0.0636,  ..., 0.1769, 0.3098, 0.7716],\n",
       "         ...,\n",
       "         [0.8921, 0.3043, 0.5191,  ..., 0.4278, 0.4532, 0.6163],\n",
       "         [0.8727, 0.4626, 0.5150,  ..., 0.9024, 0.8074, 0.7236],\n",
       "         [0.1782, 0.8031, 0.4868,  ..., 0.1242, 0.3426, 0.9536]],\n",
       "\n",
       "        [[0.5906, 0.2554, 0.1775,  ..., 0.3373, 0.0346, 0.0101],\n",
       "         [0.9056, 0.8721, 0.1368,  ..., 0.9305, 0.0161, 0.0986],\n",
       "         [0.2562, 0.3361, 0.3596,  ..., 0.0277, 0.9163, 0.9335],\n",
       "         ...,\n",
       "         [0.5878, 0.8925, 0.1020,  ..., 0.4737, 0.1308, 0.6430],\n",
       "         [0.3662, 0.3958, 0.0709,  ..., 0.5917, 0.2893, 0.7657],\n",
       "         [0.5449, 0.9591, 0.5408,  ..., 0.8525, 0.2673, 0.5322]],\n",
       "\n",
       "        [[0.3269, 0.6760, 0.1862,  ..., 0.9873, 0.6220, 0.9696],\n",
       "         [0.6660, 0.1807, 0.7212,  ..., 0.2625, 0.5854, 0.7509],\n",
       "         [0.7402, 0.2325, 0.5359,  ..., 0.9713, 0.7115, 0.5191],\n",
       "         ...,\n",
       "         [0.7702, 0.4810, 0.8080,  ..., 0.9274, 0.4365, 0.7743],\n",
       "         [0.0938, 0.1341, 0.2471,  ..., 0.0190, 0.1569, 0.6983],\n",
       "         [0.1079, 0.0330, 0.1041,  ..., 0.6843, 0.5583, 0.2335]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.6505, 0.8829, 0.1943,  ..., 0.2673, 0.9881, 0.1322],\n",
       "         [0.1638, 0.4865, 0.4499,  ..., 0.3016, 0.9518, 0.6057],\n",
       "         [0.3669, 0.7985, 0.3239,  ..., 0.4586, 0.2518, 0.8319],\n",
       "         ...,\n",
       "         [0.4766, 0.6445, 0.6203,  ..., 0.3684, 0.9730, 0.4585],\n",
       "         [0.9968, 0.1758, 0.8823,  ..., 0.7936, 0.0960, 0.1181],\n",
       "         [0.4842, 0.7816, 0.9713,  ..., 0.4800, 0.0725, 0.2516]],\n",
       "\n",
       "        [[0.5199, 0.5882, 0.5486,  ..., 0.8498, 0.4385, 0.3146],\n",
       "         [0.2759, 0.2317, 0.8278,  ..., 0.4590, 0.5516, 0.8956],\n",
       "         [0.5445, 0.6810, 0.2264,  ..., 0.0270, 0.3182, 0.4211],\n",
       "         ...,\n",
       "         [0.9600, 0.2900, 0.2262,  ..., 0.4358, 0.0416, 0.1185],\n",
       "         [0.2362, 0.9728, 0.6448,  ..., 0.1065, 0.3480, 0.5893],\n",
       "         [0.7051, 0.2212, 0.9437,  ..., 0.7266, 0.6655, 0.7161]],\n",
       "\n",
       "        [[0.4960, 0.9738, 0.6643,  ..., 0.7555, 0.5389, 0.8326],\n",
       "         [0.8715, 0.7024, 0.3506,  ..., 0.7272, 0.4637, 0.7589],\n",
       "         [0.5993, 0.8643, 0.4010,  ..., 0.7598, 0.1576, 0.2664],\n",
       "         ...,\n",
       "         [0.1061, 0.7475, 0.3954,  ..., 0.4342, 0.6797, 0.4485],\n",
       "         [0.8608, 0.3731, 0.4537,  ..., 0.8444, 0.0467, 0.9208],\n",
       "         [0.1634, 0.3797, 0.5363,  ..., 0.4688, 0.2854, 0.6697]]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0fb088d4-b648-4c27-bc37-17fe3d201119",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512, 64, 64])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8cbdd4b9-5cc0-4ea4-8eda-962cd37807b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512, 4096])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nt = t.view(1, 512, -1)\n",
    "nt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ebb5edeb-d402-4895-a6fb-6048da27cc7d",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (32768x64 and 512x256)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mbisam_concat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproj_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/mp/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniforge3/envs/mp/lib/python3.10/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/miniforge3/envs/mp/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniforge3/envs/mp/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (32768x64 and 512x256)"
     ]
    }
   ],
   "source": [
    "bisam_concat.proj_layer(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86b5d8b-ecd2-47d6-85a8-eeb79d77f755",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
