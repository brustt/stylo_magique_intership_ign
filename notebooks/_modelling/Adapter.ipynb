{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b4f34bf-3b59-4ed3-ade1-6852fa94a7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "class Adapter_ViT(nn.Module):\n",
    "    \"\"\"Applies mlp adapter to a vision transformer.\n",
    "\n",
    "    Args:\n",
    "        vit_model: a vision transformer model, see base_vit.py\n",
    "        num_layers: number of hidden layers\n",
    "        num_classes: how many classes the model output, default to the vit model\n",
    "\n",
    "    Examples::\n",
    "        >>> model = timm.create_model(\"vit_base_patch16_224.orig_in21k_ft_in1k\", pretrained=True)\n",
    "        >>> adapter_model = Adapter_ViT(model, r=4)\n",
    "        >>> preds = adapter_model(img)\n",
    "        >>> print(preds.shape)\n",
    "        torch.Size([1, 1000])\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                vit_model,\n",
    "                num_classes: int = 0):\n",
    "        super(Adapter_ViT, self).__init__()\n",
    "        \n",
    "        assert num_classes > 0\n",
    "        \n",
    "        for param in vit_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        self.dim = vit_model.blocks[0].attn.qkv.in_features\n",
    "        self.adapter = nn.Sequential()\n",
    "        for t_layer_i in range(len(vit_model.blocks)//2):\n",
    "            self.adapter.add_module(\"layer_\" + str(t_layer_i), nn.Linear(self.dim, self.dim))\n",
    "            self.adapter.add_module(\"relu_\" + str(t_layer_i), nn.ReLU())\n",
    "        self.adapter.add_module(\"fc\", nn.Linear(self.dim, num_classes))\n",
    "        \n",
    "        self.backbone = vit_model\n",
    "        self.backbone.head = self.adapter\n",
    "        \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return self.backbone(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b1e316-a5c9-4333-ae92-53e3273a713b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.commons.utils_io import load_config\n",
    "import hydra\n",
    "from hydra.core.global_hydra import GlobalHydra\n",
    "\n",
    "GlobalHydra.instance().clear()\n",
    "list_args=[\"experiment=mp_naive\", \"sam_type=small\", \"data=levir-cd\", \"data.params.n_shape=3\", \"data.params.num_worker=0\"]\n",
    "cfg = load_config(list_args)\n",
    "\n",
    "module = hydra.utils.instantiate(cfg.model.instance)\n",
    "# model = timm.create_model(\"vit_base_patch16_224.orig_in21k_ft_in1k\", pretrained=True)\n",
    "adapter_model = Adapter_ViT(model,num_classes=14)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
