{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f3d02d-0fd0-4ff1-8cdc-62b67be48a59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f300772-a188-4544-89ca-54ec89a8fe77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hydra\n",
    "from src.commons.constants import PROJECT_PATH\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "import pandas as pd\n",
    "import os\n",
    "from copy import deepcopy\n",
    "import torch.nn.functional as F\n",
    "from torchmetrics import Metric\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils import data\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "from src.commons.utils_io import load_sam\n",
    "from src.commons.utils import to_numpy, SegAnyChangeVersion, show_img, show_pair_img, show_prediction_sample, resize\n",
    "from src.models.commons.mask_process import extract_object_from_batch, binarize_mask\n",
    "from src.commons.constants import IMG_SIZE\n",
    "from src.data.process import generate_grid_prompt\n",
    "from src.commons.utils import create_sample_grid_with_prompt, get_mask_with_prompt, fig2arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02f57dad-7cbf-4fbc-8804-e39ae76613cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_config():\n",
    "    # Initialize the Hydra configuration\n",
    "    hydra.initialize(config_path=\"../../configs\", version_base=None)\n",
    "    \n",
    "    # Compose the configuration with the desired environment override\n",
    "    cfg = hydra.compose(config_name=\"train\", overrides=[\"experiment=probing_diff\", \"sam_type=small\", \"data=levir-cd\"])\n",
    "    \n",
    "    return cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51f10476-a0ce-47fc-aa6b-53a6c447fdfe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data:\n",
      "  name: levir-cd\n",
      "  _target_: src.data.datamodule.CDDataModule\n",
      "  params:\n",
      "    prompt_type: sample\n",
      "    n_prompt: 1\n",
      "    loc: center\n",
      "    batch_size: 2\n",
      "    num_worker: 4\n",
      "    pin_memory: false\n",
      "    n_shape: 3\n",
      "model:\n",
      "  network:\n",
      "    image_encoder:\n",
      "      _target_: src.models.segment_anything.modeling.image_encoder_dev.ImageEncoderViT\n",
      "      depth: 12\n",
      "      embed_dim: 768\n",
      "      img_size: 1024\n",
      "      mlp_ratio: 4\n",
      "      norm_layer: null\n",
      "      num_heads: 12\n",
      "      patch_size: 16\n",
      "      qkv_bias: true\n",
      "      use_rel_pos: true\n",
      "      global_attn_indexes:\n",
      "      - 2\n",
      "      - 5\n",
      "      - 8\n",
      "      - 11\n",
      "      window_size: 14\n",
      "      out_chans: 256\n",
      "    prompt_encoder:\n",
      "      _target_: src.models.segment_anything.modeling.prompt_encoder_dev.PromptEncoder\n",
      "      embed_dim: 256\n",
      "      image_embedding_size:\n",
      "      - 64\n",
      "      - 64\n",
      "      input_image_size:\n",
      "      - 1024\n",
      "      - 1024\n",
      "      mask_in_chans: 16\n",
      "    mask_decoder:\n",
      "      transformer:\n",
      "        _target_: src.models.segment_anything.modeling.transformer_dev.TwoWayTransformer\n",
      "        depth: 2\n",
      "        embedding_dim: 256\n",
      "        mlp_dim: 2048\n",
      "        num_heads: 8\n",
      "      _target_: src.models.segment_anything.modeling.mask_decoder_dev.MaskDecoder\n",
      "      num_multimask_outputs: 3\n",
      "      transformer_dim: 256\n",
      "      iou_head_depth: 3\n",
      "      iou_head_hidden_dim: 256\n",
      "    _target_: src.models.magic_pen.bisam_diff.BiSamDiff\n",
      "  instance:\n",
      "    _target_: src.models.magic_pen.task.MagicPenModule\n",
      "    network: ${model.network}\n",
      "    params:\n",
      "      sam_ckpt_path: ${sam_ckpt_path}\n",
      "      use_weights:\n",
      "      - image_encoder\n",
      "      ft_mode: probing\n",
      "sam_name: small\n",
      "sam_ckpt_path: ${paths.sam_ckpt_dir}/sam_vit_b_01ec64.pth\n",
      "sam_enc_arch: vit-b\n",
      "tags:\n",
      "- dev\n",
      "task_name: train_probing_diff\n",
      "callbacks:\n",
      "  model_checkpoint:\n",
      "    _target_: lightning.pytorch.callbacks.ModelCheckpoint\n",
      "    dirpath: ${paths.output_dir}/checkpoints\n",
      "    filename: epoch_{epoch:03d}\n",
      "    monitor: val/loss\n",
      "    verbose: false\n",
      "    save_last: true\n",
      "    save_top_k: 1\n",
      "    mode: min\n",
      "    auto_insert_metric_name: false\n",
      "    save_weights_only: false\n",
      "    every_n_train_steps: null\n",
      "    train_time_interval: null\n",
      "    every_n_epochs: null\n",
      "    save_on_train_epoch_end: null\n",
      "  rich_progress_bar:\n",
      "    _target_: lightning.pytorch.callbacks.RichProgressBar\n",
      "logger:\n",
      "  tensorboard:\n",
      "    _target_: lightning.pytorch.loggers.tensorboard.TensorBoardLogger\n",
      "    save_dir: ${paths.output_dir}/tensorboard/\n",
      "    name: null\n",
      "    default_hp_metric: false\n",
      "trainer:\n",
      "  _target_: lightning.pytorch.trainer.Trainer\n",
      "  default_root_dir: ${paths.output_dir}\n",
      "  min_epochs: 1\n",
      "  max_epochs: 100\n",
      "  accelerator: cpu\n",
      "  devices: 1\n",
      "  check_val_every_n_epoch: 1\n",
      "  deterministic: false\n",
      "  num_sanity_val_steps: 0\n",
      "paths:\n",
      "  root_dir: ${oc.env:PROJECT_PATH}\n",
      "  data_dir: ${oc.env:DATA_PATH}\n",
      "  log_dir: ${oc.env:LOGS_PATH}\n",
      "  output_dir: ${hydra:runtime.output_dir}\n",
      "  work_dir: ${hydra:runtime.cwd}\n",
      "  sam_ckpt_dir: ${oc.env:CHECKPOINTS_PATH}/sam\n",
      "extras:\n",
      "  ignore_warnings: false\n",
      "  enforce_tags: true\n",
      "  print_config: true\n",
      "train: true\n",
      "test: true\n",
      "ckpt_path: null\n",
      "seed: 66\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from hydra.core.global_hydra import GlobalHydra\n",
    "GlobalHydra.instance().clear()\n",
    "cfg = load_config()\n",
    "print(OmegaConf.to_yaml(cfg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6138967f-14f5-4ac6-9120-a0a5d733814d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-02 12:08:03,241 - INFO ::  Weights loaded for : ['image_encoder']\n"
     ]
    }
   ],
   "source": [
    "module = hydra.utils.instantiate(cfg.model.instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3ceb423e-720a-46e8-9cec-575a7a6edd6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(1, 256)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "module.model.prompt_encoder.point_embeddings[2].requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea0ccfb-deda-4bbf-96a2-ac4af46fff21",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
