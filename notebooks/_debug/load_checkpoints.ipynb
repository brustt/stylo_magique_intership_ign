{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0955a5f-aaa4-48e9-b622-9372974a4b80",
   "metadata": {},
   "source": [
    "On cherche à réecrire le checkpoint pth de SAM pour être \"conforme\" à lightning, i.e avec les clés attendues dans le dictionnaire de state_dict.\n",
    "\n",
    "L'erreur initiale est : `KeyError: 'pytorch-lightning_version'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7b134c7-9a4a-4a77-baec-402c4fcb9fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hydra\n",
    "import torch\n",
    "from src.commons.constants import PROJECT_PATH\n",
    "from omegaconf import DictConfig, OmegaConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75ec6ccc-33e1-4f1a-8abd-86d203301d53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34mcallbacks\u001b[0m/  eval.yaml    \u001b[01;34mhparams_search\u001b[0m/  \u001b[01;34mlocal\u001b[0m/    \u001b[01;34mmodel\u001b[0m/     \u001b[01;34mtrainer\u001b[0m/\n",
      "\u001b[01;34mdata\u001b[0m/       \u001b[01;34mexperiment\u001b[0m/  \u001b[01;34mhydra\u001b[0m/           \u001b[01;34mlogger\u001b[0m/   \u001b[01;34mpaths\u001b[0m/     train.yaml\n",
      "\u001b[01;34mdebug\u001b[0m/      \u001b[01;34mextras\u001b[0m/      __init__.py      \u001b[01;34mmetrics\u001b[0m/  \u001b[01;34msam_type\u001b[0m/\n"
     ]
    }
   ],
   "source": [
    "ls ../../configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de33b054-2e05-496d-ad17-07145c417021",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_config():\n",
    "    # Initialize the Hydra configuration\n",
    "    hydra.initialize(config_path=\"../../configs\", version_base=None)\n",
    "    \n",
    "    # Compose the configuration with the desired environment override\n",
    "    cfg = hydra.compose(config_name=\"train\", overrides=[\"experiment=adapter\", \"sam_type=small\", \"data=levir-cd\"])\n",
    "    \n",
    "    return cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f1b5420-83c4-4055-96a7-757e1f761660",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data:\n",
      "  name: levir-cd\n",
      "  _target_: src.data.datamodule.CDDataModule\n",
      "  params:\n",
      "    prompt_type: sample\n",
      "    n_prompt: 1\n",
      "    loc: center\n",
      "    batch_size: 2\n",
      "    n_shape: 3\n",
      "    num_worker: 2\n",
      "    pin_memory: false\n",
      "model:\n",
      "  network:\n",
      "    image_encoder:\n",
      "      _target_: src.models.magic_pen.adapter.ImageEncoderViTAdapter\n",
      "      depth: 12\n",
      "      embed_dim: 768\n",
      "      img_size: 1024\n",
      "      mlp_ratio: 4\n",
      "      norm_layer: null\n",
      "      num_heads: 12\n",
      "      patch_size: 16\n",
      "      qkv_bias: true\n",
      "      use_rel_pos: true\n",
      "      global_attn_indexes:\n",
      "      - 2\n",
      "      - 5\n",
      "      - 8\n",
      "      - 11\n",
      "      window_size: 14\n",
      "      out_chans: 256\n",
      "      adapter_inter_dim: 16\n",
      "    prompt_encoder:\n",
      "      _target_: src.models.segment_anything.modeling.prompt_encoder_dev.PromptEncoder\n",
      "      embed_dim: 512\n",
      "      image_embedding_size:\n",
      "      - 64\n",
      "      - 64\n",
      "      input_image_size:\n",
      "      - 1024\n",
      "      - 1024\n",
      "      mask_in_chans: 16\n",
      "    mask_decoder:\n",
      "      transformer:\n",
      "        _target_: src.models.segment_anything.modeling.transformer_dev.TwoWayTransformer\n",
      "        depth: 2\n",
      "        embedding_dim: 512\n",
      "        mlp_dim: 2048\n",
      "        num_heads: 8\n",
      "      _target_: src.models.segment_anything.modeling.mask_decoder_dev.MaskDecoder\n",
      "      num_multimask_outputs: 3\n",
      "      transformer_dim: 512\n",
      "      iou_head_depth: 3\n",
      "      iou_head_hidden_dim: 256\n",
      "    _target_: src.models.magic_pen.bisam_concat.BiSamConcat\n",
      "  instance:\n",
      "    _target_: src.models.magic_pen.task.MagicPenModule\n",
      "    network: ${model.network}\n",
      "    params:\n",
      "      sam_ckpt_path: ${sam_ckpt_path}\n",
      "      use_weights:\n",
      "      - image_encoder\n",
      "      ft_mode: adapter\n",
      "sam_name: small\n",
      "sam_ckpt_path: ${paths.sam_ckpt_dir}/sam_vit_b_01ec64.pth\n",
      "sam_enc_arch: vit-b\n",
      "tags:\n",
      "- dev\n",
      "task_name: train_adapter_concat\n",
      "callbacks:\n",
      "  model_checkpoint:\n",
      "    _target_: lightning.pytorch.callbacks.ModelCheckpoint\n",
      "    dirpath: ${paths.output_dir}/checkpoints\n",
      "    filename: epoch_{epoch:03d}\n",
      "    monitor: val/loss\n",
      "    verbose: false\n",
      "    save_last: true\n",
      "    save_top_k: 1\n",
      "    mode: min\n",
      "    auto_insert_metric_name: false\n",
      "    save_weights_only: false\n",
      "    every_n_train_steps: null\n",
      "    train_time_interval: null\n",
      "    every_n_epochs: null\n",
      "    save_on_train_epoch_end: null\n",
      "  rich_progress_bar:\n",
      "    _target_: lightning.pytorch.callbacks.RichProgressBar\n",
      "logger:\n",
      "  tensorboard:\n",
      "    _target_: lightning.pytorch.loggers.tensorboard.TensorBoardLogger\n",
      "    save_dir: ${paths.output_dir}/tensorboard/\n",
      "    name: null\n",
      "    default_hp_metric: false\n",
      "trainer:\n",
      "  _target_: lightning.pytorch.trainer.Trainer\n",
      "  default_root_dir: ${paths.output_dir}\n",
      "  min_epochs: 1\n",
      "  max_epochs: 100\n",
      "  accelerator: cpu\n",
      "  devices: 1\n",
      "  check_val_every_n_epoch: 1\n",
      "  deterministic: false\n",
      "paths:\n",
      "  root_dir: ${oc.env:PROJECT_PATH}\n",
      "  data_dir: ${oc.env:DATA_PATH}\n",
      "  log_dir: ${oc.env:LOGS_PATH}\n",
      "  output_dir: ${hydra:runtime.output_dir}\n",
      "  work_dir: ${hydra:runtime.cwd}\n",
      "  sam_ckpt_dir: ${oc.env:CHECKPOINTS_PATH}/sam\n",
      "extras:\n",
      "  ignore_warnings: false\n",
      "  enforce_tags: true\n",
      "  print_config: true\n",
      "train: true\n",
      "test: true\n",
      "ckpt_path: null\n",
      "seed: 66\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from hydra.core.global_hydra import GlobalHydra\n",
    "GlobalHydra.instance().clear()\n",
    "cfg = load_config()\n",
    "print(OmegaConf.to_yaml(cfg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a39bbe16-631b-41d7-986a-3128d9f36cc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-30 15:58:46,285 - INFO ::  Weights loaded for : ['image_encoder']\n"
     ]
    }
   ],
   "source": [
    "module = hydra.utils.instantiate(cfg.model.instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "58a9eb68-c6cf-4dd1-91d9-e06a373c2962",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"/var/data/usr/mdizier/stylo_magique/checkpoints/sam/sam_vit_b_01ec64.pth\"\n",
    "module.model.load_state_dict(torch.load(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36c24ffc-cd4e-4168-91bc-8405bf2a5afb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ImageEncoderViTAdapter(\n",
       "  (patch_embed): PatchEmbed(\n",
       "    (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "  )\n",
       "  (blocks): ModuleList(\n",
       "    (0-11): 12 x AdapterBlock(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): MLPBlock(\n",
       "        (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "      )\n",
       "      (adapter): Adapter(\n",
       "        (act): GELU(approximate='none')\n",
       "        (down_layer): Linear(in_features=768, out_features=16, bias=True)\n",
       "        (up_layer): Linear(in_features=16, out_features=768, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (neck): Sequential(\n",
       "    (0): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (1): LayerNorm2d()\n",
       "    (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (3): LayerNorm2d()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "module.model.image_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d3df5d5b-b370-4714-a8fe-110bd8e0612a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "bool(re.search('adapter', \"blocks.0.adater.up_layer.weight\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "654908e5-e596-433e-bcbb-68f62ab97aed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos_embed torch.Size([1, 64, 64, 768])\n",
      "patch_embed.proj.weight torch.Size([768, 3, 16, 16])\n",
      "patch_embed.proj.bias torch.Size([768])\n",
      "blocks.0.scale torch.Size([768])\n",
      "blocks.0.norm1.weight torch.Size([768])\n",
      "blocks.0.norm1.bias torch.Size([768])\n",
      "blocks.0.attn.rel_pos_h torch.Size([27, 64])\n",
      "blocks.0.attn.rel_pos_w torch.Size([27, 64])\n",
      "blocks.0.attn.qkv.weight torch.Size([2304, 768])\n",
      "blocks.0.attn.qkv.bias torch.Size([2304])\n",
      "blocks.0.attn.proj.weight torch.Size([768, 768])\n",
      "blocks.0.attn.proj.bias torch.Size([768])\n",
      "blocks.0.norm2.weight torch.Size([768])\n",
      "blocks.0.norm2.bias torch.Size([768])\n",
      "blocks.0.mlp.lin1.weight torch.Size([3072, 768])\n",
      "blocks.0.mlp.lin1.bias torch.Size([3072])\n",
      "blocks.0.mlp.lin2.weight torch.Size([768, 3072])\n",
      "blocks.0.mlp.lin2.bias torch.Size([768])\n",
      "blocks.0.adapter.down_layer.weight torch.Size([16, 768])\n",
      "blocks.0.adapter.down_layer.bias torch.Size([16])\n",
      "blocks.0.adapter.up_layer.weight torch.Size([768, 16])\n",
      "blocks.0.adapter.up_layer.bias torch.Size([768])\n",
      "blocks.1.scale torch.Size([768])\n",
      "blocks.1.norm1.weight torch.Size([768])\n",
      "blocks.1.norm1.bias torch.Size([768])\n",
      "blocks.1.attn.rel_pos_h torch.Size([27, 64])\n",
      "blocks.1.attn.rel_pos_w torch.Size([27, 64])\n",
      "blocks.1.attn.qkv.weight torch.Size([2304, 768])\n",
      "blocks.1.attn.qkv.bias torch.Size([2304])\n",
      "blocks.1.attn.proj.weight torch.Size([768, 768])\n",
      "blocks.1.attn.proj.bias torch.Size([768])\n",
      "blocks.1.norm2.weight torch.Size([768])\n",
      "blocks.1.norm2.bias torch.Size([768])\n",
      "blocks.1.mlp.lin1.weight torch.Size([3072, 768])\n",
      "blocks.1.mlp.lin1.bias torch.Size([3072])\n",
      "blocks.1.mlp.lin2.weight torch.Size([768, 3072])\n",
      "blocks.1.mlp.lin2.bias torch.Size([768])\n",
      "blocks.1.adapter.down_layer.weight torch.Size([16, 768])\n",
      "blocks.1.adapter.down_layer.bias torch.Size([16])\n",
      "blocks.1.adapter.up_layer.weight torch.Size([768, 16])\n",
      "blocks.1.adapter.up_layer.bias torch.Size([768])\n",
      "blocks.2.scale torch.Size([768])\n",
      "blocks.2.norm1.weight torch.Size([768])\n",
      "blocks.2.norm1.bias torch.Size([768])\n",
      "blocks.2.attn.rel_pos_h torch.Size([127, 64])\n",
      "blocks.2.attn.rel_pos_w torch.Size([127, 64])\n",
      "blocks.2.attn.qkv.weight torch.Size([2304, 768])\n",
      "blocks.2.attn.qkv.bias torch.Size([2304])\n",
      "blocks.2.attn.proj.weight torch.Size([768, 768])\n",
      "blocks.2.attn.proj.bias torch.Size([768])\n",
      "blocks.2.norm2.weight torch.Size([768])\n",
      "blocks.2.norm2.bias torch.Size([768])\n",
      "blocks.2.mlp.lin1.weight torch.Size([3072, 768])\n",
      "blocks.2.mlp.lin1.bias torch.Size([3072])\n",
      "blocks.2.mlp.lin2.weight torch.Size([768, 3072])\n",
      "blocks.2.mlp.lin2.bias torch.Size([768])\n",
      "blocks.2.adapter.down_layer.weight torch.Size([16, 768])\n",
      "blocks.2.adapter.down_layer.bias torch.Size([16])\n",
      "blocks.2.adapter.up_layer.weight torch.Size([768, 16])\n",
      "blocks.2.adapter.up_layer.bias torch.Size([768])\n",
      "blocks.3.scale torch.Size([768])\n",
      "blocks.3.norm1.weight torch.Size([768])\n",
      "blocks.3.norm1.bias torch.Size([768])\n",
      "blocks.3.attn.rel_pos_h torch.Size([27, 64])\n",
      "blocks.3.attn.rel_pos_w torch.Size([27, 64])\n",
      "blocks.3.attn.qkv.weight torch.Size([2304, 768])\n",
      "blocks.3.attn.qkv.bias torch.Size([2304])\n",
      "blocks.3.attn.proj.weight torch.Size([768, 768])\n",
      "blocks.3.attn.proj.bias torch.Size([768])\n",
      "blocks.3.norm2.weight torch.Size([768])\n",
      "blocks.3.norm2.bias torch.Size([768])\n",
      "blocks.3.mlp.lin1.weight torch.Size([3072, 768])\n",
      "blocks.3.mlp.lin1.bias torch.Size([3072])\n",
      "blocks.3.mlp.lin2.weight torch.Size([768, 3072])\n",
      "blocks.3.mlp.lin2.bias torch.Size([768])\n",
      "blocks.3.adapter.down_layer.weight torch.Size([16, 768])\n",
      "blocks.3.adapter.down_layer.bias torch.Size([16])\n",
      "blocks.3.adapter.up_layer.weight torch.Size([768, 16])\n",
      "blocks.3.adapter.up_layer.bias torch.Size([768])\n",
      "blocks.4.scale torch.Size([768])\n",
      "blocks.4.norm1.weight torch.Size([768])\n",
      "blocks.4.norm1.bias torch.Size([768])\n",
      "blocks.4.attn.rel_pos_h torch.Size([27, 64])\n",
      "blocks.4.attn.rel_pos_w torch.Size([27, 64])\n",
      "blocks.4.attn.qkv.weight torch.Size([2304, 768])\n",
      "blocks.4.attn.qkv.bias torch.Size([2304])\n",
      "blocks.4.attn.proj.weight torch.Size([768, 768])\n",
      "blocks.4.attn.proj.bias torch.Size([768])\n",
      "blocks.4.norm2.weight torch.Size([768])\n",
      "blocks.4.norm2.bias torch.Size([768])\n",
      "blocks.4.mlp.lin1.weight torch.Size([3072, 768])\n",
      "blocks.4.mlp.lin1.bias torch.Size([3072])\n",
      "blocks.4.mlp.lin2.weight torch.Size([768, 3072])\n",
      "blocks.4.mlp.lin2.bias torch.Size([768])\n",
      "blocks.4.adapter.down_layer.weight torch.Size([16, 768])\n",
      "blocks.4.adapter.down_layer.bias torch.Size([16])\n",
      "blocks.4.adapter.up_layer.weight torch.Size([768, 16])\n",
      "blocks.4.adapter.up_layer.bias torch.Size([768])\n",
      "blocks.5.scale torch.Size([768])\n",
      "blocks.5.norm1.weight torch.Size([768])\n",
      "blocks.5.norm1.bias torch.Size([768])\n",
      "blocks.5.attn.rel_pos_h torch.Size([127, 64])\n",
      "blocks.5.attn.rel_pos_w torch.Size([127, 64])\n",
      "blocks.5.attn.qkv.weight torch.Size([2304, 768])\n",
      "blocks.5.attn.qkv.bias torch.Size([2304])\n",
      "blocks.5.attn.proj.weight torch.Size([768, 768])\n",
      "blocks.5.attn.proj.bias torch.Size([768])\n",
      "blocks.5.norm2.weight torch.Size([768])\n",
      "blocks.5.norm2.bias torch.Size([768])\n",
      "blocks.5.mlp.lin1.weight torch.Size([3072, 768])\n",
      "blocks.5.mlp.lin1.bias torch.Size([3072])\n",
      "blocks.5.mlp.lin2.weight torch.Size([768, 3072])\n",
      "blocks.5.mlp.lin2.bias torch.Size([768])\n",
      "blocks.5.adapter.down_layer.weight torch.Size([16, 768])\n",
      "blocks.5.adapter.down_layer.bias torch.Size([16])\n",
      "blocks.5.adapter.up_layer.weight torch.Size([768, 16])\n",
      "blocks.5.adapter.up_layer.bias torch.Size([768])\n",
      "blocks.6.scale torch.Size([768])\n",
      "blocks.6.norm1.weight torch.Size([768])\n",
      "blocks.6.norm1.bias torch.Size([768])\n",
      "blocks.6.attn.rel_pos_h torch.Size([27, 64])\n",
      "blocks.6.attn.rel_pos_w torch.Size([27, 64])\n",
      "blocks.6.attn.qkv.weight torch.Size([2304, 768])\n",
      "blocks.6.attn.qkv.bias torch.Size([2304])\n",
      "blocks.6.attn.proj.weight torch.Size([768, 768])\n",
      "blocks.6.attn.proj.bias torch.Size([768])\n",
      "blocks.6.norm2.weight torch.Size([768])\n",
      "blocks.6.norm2.bias torch.Size([768])\n",
      "blocks.6.mlp.lin1.weight torch.Size([3072, 768])\n",
      "blocks.6.mlp.lin1.bias torch.Size([3072])\n",
      "blocks.6.mlp.lin2.weight torch.Size([768, 3072])\n",
      "blocks.6.mlp.lin2.bias torch.Size([768])\n",
      "blocks.6.adapter.down_layer.weight torch.Size([16, 768])\n",
      "blocks.6.adapter.down_layer.bias torch.Size([16])\n",
      "blocks.6.adapter.up_layer.weight torch.Size([768, 16])\n",
      "blocks.6.adapter.up_layer.bias torch.Size([768])\n",
      "blocks.7.scale torch.Size([768])\n",
      "blocks.7.norm1.weight torch.Size([768])\n",
      "blocks.7.norm1.bias torch.Size([768])\n",
      "blocks.7.attn.rel_pos_h torch.Size([27, 64])\n",
      "blocks.7.attn.rel_pos_w torch.Size([27, 64])\n",
      "blocks.7.attn.qkv.weight torch.Size([2304, 768])\n",
      "blocks.7.attn.qkv.bias torch.Size([2304])\n",
      "blocks.7.attn.proj.weight torch.Size([768, 768])\n",
      "blocks.7.attn.proj.bias torch.Size([768])\n",
      "blocks.7.norm2.weight torch.Size([768])\n",
      "blocks.7.norm2.bias torch.Size([768])\n",
      "blocks.7.mlp.lin1.weight torch.Size([3072, 768])\n",
      "blocks.7.mlp.lin1.bias torch.Size([3072])\n",
      "blocks.7.mlp.lin2.weight torch.Size([768, 3072])\n",
      "blocks.7.mlp.lin2.bias torch.Size([768])\n",
      "blocks.7.adapter.down_layer.weight torch.Size([16, 768])\n",
      "blocks.7.adapter.down_layer.bias torch.Size([16])\n",
      "blocks.7.adapter.up_layer.weight torch.Size([768, 16])\n",
      "blocks.7.adapter.up_layer.bias torch.Size([768])\n",
      "blocks.8.scale torch.Size([768])\n",
      "blocks.8.norm1.weight torch.Size([768])\n",
      "blocks.8.norm1.bias torch.Size([768])\n",
      "blocks.8.attn.rel_pos_h torch.Size([127, 64])\n",
      "blocks.8.attn.rel_pos_w torch.Size([127, 64])\n",
      "blocks.8.attn.qkv.weight torch.Size([2304, 768])\n",
      "blocks.8.attn.qkv.bias torch.Size([2304])\n",
      "blocks.8.attn.proj.weight torch.Size([768, 768])\n",
      "blocks.8.attn.proj.bias torch.Size([768])\n",
      "blocks.8.norm2.weight torch.Size([768])\n",
      "blocks.8.norm2.bias torch.Size([768])\n",
      "blocks.8.mlp.lin1.weight torch.Size([3072, 768])\n",
      "blocks.8.mlp.lin1.bias torch.Size([3072])\n",
      "blocks.8.mlp.lin2.weight torch.Size([768, 3072])\n",
      "blocks.8.mlp.lin2.bias torch.Size([768])\n",
      "blocks.8.adapter.down_layer.weight torch.Size([16, 768])\n",
      "blocks.8.adapter.down_layer.bias torch.Size([16])\n",
      "blocks.8.adapter.up_layer.weight torch.Size([768, 16])\n",
      "blocks.8.adapter.up_layer.bias torch.Size([768])\n",
      "blocks.9.scale torch.Size([768])\n",
      "blocks.9.norm1.weight torch.Size([768])\n",
      "blocks.9.norm1.bias torch.Size([768])\n",
      "blocks.9.attn.rel_pos_h torch.Size([27, 64])\n",
      "blocks.9.attn.rel_pos_w torch.Size([27, 64])\n",
      "blocks.9.attn.qkv.weight torch.Size([2304, 768])\n",
      "blocks.9.attn.qkv.bias torch.Size([2304])\n",
      "blocks.9.attn.proj.weight torch.Size([768, 768])\n",
      "blocks.9.attn.proj.bias torch.Size([768])\n",
      "blocks.9.norm2.weight torch.Size([768])\n",
      "blocks.9.norm2.bias torch.Size([768])\n",
      "blocks.9.mlp.lin1.weight torch.Size([3072, 768])\n",
      "blocks.9.mlp.lin1.bias torch.Size([3072])\n",
      "blocks.9.mlp.lin2.weight torch.Size([768, 3072])\n",
      "blocks.9.mlp.lin2.bias torch.Size([768])\n",
      "blocks.9.adapter.down_layer.weight torch.Size([16, 768])\n",
      "blocks.9.adapter.down_layer.bias torch.Size([16])\n",
      "blocks.9.adapter.up_layer.weight torch.Size([768, 16])\n",
      "blocks.9.adapter.up_layer.bias torch.Size([768])\n",
      "blocks.10.scale torch.Size([768])\n",
      "blocks.10.norm1.weight torch.Size([768])\n",
      "blocks.10.norm1.bias torch.Size([768])\n",
      "blocks.10.attn.rel_pos_h torch.Size([27, 64])\n",
      "blocks.10.attn.rel_pos_w torch.Size([27, 64])\n",
      "blocks.10.attn.qkv.weight torch.Size([2304, 768])\n",
      "blocks.10.attn.qkv.bias torch.Size([2304])\n",
      "blocks.10.attn.proj.weight torch.Size([768, 768])\n",
      "blocks.10.attn.proj.bias torch.Size([768])\n",
      "blocks.10.norm2.weight torch.Size([768])\n",
      "blocks.10.norm2.bias torch.Size([768])\n",
      "blocks.10.mlp.lin1.weight torch.Size([3072, 768])\n",
      "blocks.10.mlp.lin1.bias torch.Size([3072])\n",
      "blocks.10.mlp.lin2.weight torch.Size([768, 3072])\n",
      "blocks.10.mlp.lin2.bias torch.Size([768])\n",
      "blocks.10.adapter.down_layer.weight torch.Size([16, 768])\n",
      "blocks.10.adapter.down_layer.bias torch.Size([16])\n",
      "blocks.10.adapter.up_layer.weight torch.Size([768, 16])\n",
      "blocks.10.adapter.up_layer.bias torch.Size([768])\n",
      "blocks.11.scale torch.Size([768])\n",
      "blocks.11.norm1.weight torch.Size([768])\n",
      "blocks.11.norm1.bias torch.Size([768])\n",
      "blocks.11.attn.rel_pos_h torch.Size([127, 64])\n",
      "blocks.11.attn.rel_pos_w torch.Size([127, 64])\n",
      "blocks.11.attn.qkv.weight torch.Size([2304, 768])\n",
      "blocks.11.attn.qkv.bias torch.Size([2304])\n",
      "blocks.11.attn.proj.weight torch.Size([768, 768])\n",
      "blocks.11.attn.proj.bias torch.Size([768])\n",
      "blocks.11.norm2.weight torch.Size([768])\n",
      "blocks.11.norm2.bias torch.Size([768])\n",
      "blocks.11.mlp.lin1.weight torch.Size([3072, 768])\n",
      "blocks.11.mlp.lin1.bias torch.Size([3072])\n",
      "blocks.11.mlp.lin2.weight torch.Size([768, 3072])\n",
      "blocks.11.mlp.lin2.bias torch.Size([768])\n",
      "blocks.11.adapter.down_layer.weight torch.Size([16, 768])\n",
      "blocks.11.adapter.down_layer.bias torch.Size([16])\n",
      "blocks.11.adapter.up_layer.weight torch.Size([768, 16])\n",
      "blocks.11.adapter.up_layer.bias torch.Size([768])\n",
      "neck.0.weight torch.Size([256, 768, 1, 1])\n",
      "neck.1.weight torch.Size([256])\n",
      "neck.1.bias torch.Size([256])\n",
      "neck.2.weight torch.Size([256, 256, 3, 3])\n",
      "neck.3.weight torch.Size([256])\n",
      "neck.3.bias torch.Size([256])\n"
     ]
    }
   ],
   "source": [
    "for name, m in module.model.image_encoder.named_parameters():\n",
    "    #if not name.startwith\n",
    "    print(name, m.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f78957b-b6f9-4b2a-8bb0-aff1cd9322c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
