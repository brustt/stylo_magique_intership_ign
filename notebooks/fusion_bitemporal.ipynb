{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a077cba-3aa0-4f3c-aa4e-0ff76f7351d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1a7a05f-8615-42e5-ae8c-174df768b619",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome in JZAY\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "import pandas as pd\n",
    "import os\n",
    "from copy import deepcopy\n",
    "import torch.nn.functional as F\n",
    "from torchmetrics import Metric\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils import data\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "from src.commons.utils import to_numpy, SegAnyChangeVersion, show_img, show_pair_img, show_prediction_sample, resize\n",
    "from src.models.commons.mask_process import extract_object_from_batch, binarize_mask\n",
    "from src.commons.constants import IMG_SIZE\n",
    "from src.data.process import generate_grid_prompt\n",
    "from src.commons.utils import create_sample_grid_with_prompt, get_mask_with_prompt, fig2arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7ce822f-1383-4f6d-855d-48a41b79e977",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.commons.utils_io import load_sam\n",
    "from src.models.commons.bisam import BiSam2, SamModeInference\n",
    "from src.models.commons.model import BiSam\n",
    "\n",
    "from src.commons.utils import batch_to_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6befc0-89ad-477f-96d9-fa9b6e3d3def",
   "metadata": {},
   "source": [
    "### Load dloader manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7b39254-3994-4f95-b1fb-4fd354a3146e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data.loader import BiTemporalDataset\n",
    "from src.data.process import DefaultTransform\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "params = {\n",
    "    \"prompt_type\": \"sample\",\n",
    "    \"n_prompt\": 1,\n",
    "    \"n_shape\":3,\n",
    "    \"loc\": \"center\",\n",
    "    \"batch_size\": 2,\n",
    "}\n",
    "ds = BiTemporalDataset(\n",
    "            name=\"levir-cd\",\n",
    "            dtype=\"test\",\n",
    "            transform=DefaultTransform(),\n",
    "            params=OmegaConf.create(params),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d3c3788-fc07-4c37-a430-17a4d0d23c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dloader = data.DataLoader(\n",
    "            ds,\n",
    "            batch_size=params.get('batch_size'),\n",
    "            shuffle=False,\n",
    "            num_workers=0,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8eff82f-7fe4-437d-b129-211754affa32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/rustt/Documents/IGN/data/levir-cd/test/label/test_1.png\n",
      "/home/rustt/Documents/IGN/data/levir-cd/test/label/test_2.png\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(dloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd4bf86-bbf8-4017-956a-b45b6a9049c0",
   "metadata": {},
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7b1c4eb-212a-4aee-8c66-2a7ae1109a7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-23 21:21:21,942 - INFO ::  build vit_b BiSam\n"
     ]
    }
   ],
   "source": [
    "bisam = load_sam(\n",
    "    model_type=\"vit_b\", model_cls=BiSam, version= \"dev\", device=\"cpu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b19bca-6d80-4545-a92c-9b1a1154a856",
   "metadata": {},
   "source": [
    "### Load config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36dff64d-cb8f-44e0-971c-650b914c92f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.commons.utils_io import load_config\n",
    "import hydra\n",
    "from hydra.core.global_hydra import GlobalHydra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "969326b1-9fd4-402e-b274-925336df0d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "GlobalHydra.instance().clear()\n",
    "list_args=[\"experiment=mp_naive\", \"sam_type=small\", \"data=levir-cd\", \"data.params.n_shape=3\", \"data.params.num_worker=0\"]\n",
    "cfg = load_config(list_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "08c98ffa-42c8-4ada-92e1-4d2eee4f2f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "module = hydra.utils.instantiate(cfg.model.instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "784233ff-f9e1-4fe7-bcb0-9ca9f69dcc5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_images = torch.cat(\n",
    "    [batch[\"img_A\"], batch[\"img_B\"]]\n",
    ")\n",
    "input_images = bisam.preprocess(input_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d1ad78e8-6c59-4b53-baf4-08c8aaff23c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 49.7 s, sys: 17.5 s, total: 1min 7s\n",
      "Wall time: 8.49 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "emb_B = bisam.image_encoder(batch[\"img_B\"][0].unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e3ed91f1-2e7f-43ac-80e4-fb2896576618",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_A = torch.rand((256, 64, 64))\n",
    "img_B = torch.rand((256, 64, 64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b2617cf2-c4d5-49d7-98b8-de5601051558",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_embeddings = torch.cat([img_A, img_B])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2001144d-a8f4-4ddc-ae3b-2b23fdeb0472",
   "metadata": {},
   "outputs": [],
   "source": [
    "# one mask for prompt\n",
    "point_coords = batch[\"point_coords\"]\n",
    "point_labels = batch[\"point_labels\"]\n",
    "\n",
    "sparse_embeddings, dense_embeddings = bisam.prompt_encoder(\n",
    "    points=(\n",
    "        point_coords[:,None, :],\n",
    "        point_labels[:, None,...],\n",
    "    ), \n",
    "    boxes=None,\n",
    "    masks=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6ee28dbd-3679-43e4-9565-096557bf3fed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sparse_embeddings: torch.Size([2, 1, 4, 256])\n",
      "dense_embeddings: torch.Size([2, 1, 256, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "print(f\"sparse_embeddings: {sparse_embeddings.shape}\")\n",
    "print(f\"dense_embeddings: {dense_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cc441a95-fd80-4359-9117-e8e653e890d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "layer = nn.Conv2d(image_embeddings.shape[0], image_embeddings.shape[0] // 2, kernel_size=3, padding=1)\n",
    "out = layer(image_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "18c219e8-2abd-4388-96a3-6d6d38068370",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([256, 512, 3, 3])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8abe8c7b-b10e-4504-99e5-d477cb050be1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512, 64, 64])\n",
      "torch.Size([256, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "print(image_embeddings.shape)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01814f9-e045-4f78-adf4-1706a1f0e5d1",
   "metadata": {},
   "source": [
    "### Try to extent SAM modules to bitemporal\n",
    "* prompt embedding\n",
    "* Maks decoder\n",
    "\n",
    "Essayons de concatener les embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d2de4338-701f-4340-b11e-a5ce059160bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512, 64, 64])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f7e4b6d5-4fcf-49f7-bac2-3f223ea17a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_embeddings = image_embeddings.unsqueeze(0) # extent over batch dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c0440112-824c-4270-9c3b-dcb323e51bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.segment_anything.modeling.prompt_encoder_dev import PromptEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b8a6717c-69fd-4b89-be89-a24bebf5e673",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_embed_dim = image_embeddings.shape[1]\n",
    "image_size = 1024\n",
    "vit_patch_size = 16\n",
    "image_embedding_size = image_size // vit_patch_size\n",
    "\n",
    "\n",
    "prompt_encoder_extent = PromptEncoder(\n",
    "    embed_dim=prompt_embed_dim,\n",
    "    image_embedding_size=(image_embedding_size, image_embedding_size),\n",
    "    input_image_size=(image_size, image_size),\n",
    "    mask_in_chans=16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ec3ddb7b-f05d-4b1f-a5e7-414f8e0e5c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_embeddings, dense_embeddings = prompt_encoder_extent(\n",
    "    points=(\n",
    "        point_coords[:,None, :],\n",
    "        point_labels[:, None,...],\n",
    "    ), \n",
    "    boxes=None,\n",
    "    masks=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "874b4ed5-10ab-4f00-8a65-08cc8987b850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sparse_embeddings: torch.Size([2, 1, 4, 512])\n",
      "dense_embeddings: torch.Size([2, 1, 512, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "print(f\"sparse_embeddings: {sparse_embeddings.shape}\")\n",
    "print(f\"dense_embeddings: {dense_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212286b4-24b5-429b-beff-21edbf16c74e",
   "metadata": {},
   "source": [
    "### Cross attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c47fe7c3-9a96-469c-afc8-41788182bc08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class LocalCrossAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, window_size):\n",
    "        super(LocalCrossAttention, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.window_size = window_size\n",
    "        self.multihead_attn = nn.MultiheadAttention(embed_dim=embed_dim, num_heads=num_heads, batch_first=True)\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(embed_dim, embed_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        B, C, H, W = x1.size()\n",
    "        window_size = self.window_size\n",
    "        assert H % window_size == 0 and W % window_size == 0, \"Height and Width must be divisible by the window size.\"\n",
    "\n",
    "        # Function to divide into windows\n",
    "        def window_partition(x):\n",
    "            \"\"\"\n",
    "            return [B * num_windows, C, window_size, window_size].\n",
    "            \"\"\"\n",
    "            B, C, H, W = x.shape\n",
    "            x = x.view(B, C, H // window_size, window_size, W // window_size, window_size)\n",
    "            windows = x.permute(0, 2, 4, 1, 3, 5).contiguous()\n",
    "            windows = windows.view(-1, C, window_size, window_size)\n",
    "            return windows\n",
    "\n",
    "        # Function to merge windows back to feature map\n",
    "        def window_reverse(windows, H, W):\n",
    "            B = int(windows.shape[0] / (H * W / window_size / window_size))\n",
    "            x = windows.view(B, H // window_size, W // window_size, -1, window_size, window_size)\n",
    "            x = x.permute(0, 3, 1, 4, 2, 5).contiguous()\n",
    "            x = x.view(B, -1, H, W)\n",
    "            return x\n",
    "\n",
    "        x1_windows = window_partition(x1)  # shape: (num_windows*B, C, window_size, window_size)\n",
    "        x2_windows = window_partition(x2)  # shape: (num_windows*B, C, window_size, window_size)\n",
    "\n",
    "        # Flatten spatial dimensions\n",
    "        x1_windows = x1_windows.view(-1, C, window_size * window_size).permute(0, 2, 1)  # (num_windows*B, window_size*window_size, C)\n",
    "        x2_windows = x2_windows.view(-1, C, window_size * window_size).permute(0, 2, 1)  # (num_windows*B, window_size*window_size, C)\n",
    "\n",
    "        # Apply cross attention within windows : multihead_attn(query, key, value)\n",
    "        attn_output, _ = self.multihead_attn(x1_windows, x2_windows, x2_windows)  # (num_windows*B, window_size*window_size, C)\n",
    "        x1_windows = x1_windows + attn_output\n",
    "        x1_windows = self.norm1(x1_windows)\n",
    "\n",
    "        # Feed Forward\n",
    "        ff_output = self.ff(x1_windows)  # (num_windows*B, window_size*window_size, C)\n",
    "        x1_windows = x1_windows + ff_output\n",
    "        x1_windows = self.norm2(x1_windows)\n",
    "\n",
    "        # Reshape back to windowed spatial dimensions\n",
    "        x1_windows = x1_windows.permute(0, 2, 1).view(-1, C, window_size, window_size)  # (num_windows*B, C, window_size, window_size)\n",
    "\n",
    "        # Merge windows back to original dimensions\n",
    "        x1 = window_reverse(x1_windows, H, W)  # (B, C, H, W)\n",
    "\n",
    "        return x1\n",
    "\n",
    "\n",
    "sam_decoder = None  # replace with actual decoder\n",
    "embed_dim = 256\n",
    "num_heads = 8\n",
    "window_size = 8  # Example window size, adjust as needed\n",
    "\n",
    "# Instantiate the local cross attention module\n",
    "local_cross_attention_module = LocalCrossAttention(embed_dim, num_heads, window_size)\n",
    "\n",
    "B = 2\n",
    "# Example input tensors, replace with actual image data\n",
    "emb1 = torch.rand(B, 256, 64, 64)\n",
    "emb2 = torch.rand(B, 256, 64, 64)\n",
    "\n",
    "output = local_cross_attention_module(emb1, emb2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07913dbd-907b-4b73-b538-75e79894f652",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 256, 64, 64])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5dfdf9e-7d83-4bd0-8969-3c421178cfa3",
   "metadata": {},
   "source": [
    "=> implement with current Attention Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ed62a1df-2f45-485d-ae41-09e82f1d7143",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to divide into windows\n",
    "def window_partition(x):\n",
    "    \"\"\"\n",
    "    return [B * num_windows, C, window_size, window_size].\n",
    "    \"\"\"\n",
    "    B, C, H, W = x.shape\n",
    "    x = x.view(B, C, H // window_size, window_size, W // window_size, window_size)\n",
    "    windows = x.permute(0, 2, 4, 1, 3, 5).contiguous()\n",
    "    windows = windows.view(-1, C, window_size, window_size)\n",
    "    return windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b1fbf8ee-6cbd-47bb-8a85-8e0c1ce3da0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_tensor = []\n",
    "for _ in range(10):\n",
    "    stack_tensor.append(\n",
    "        torch.zeros(1, 1, 64, 64) + _\n",
    "    )\n",
    "stack_tensor = torch.cat(stack_tensor, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "7a0f502f-43ce-4de4-a951-d8586b18b475",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10, 64, 64])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stack_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e5be09e1-0be5-42ca-9612-3ef62aa90790",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 8\n",
    "win_tensor = window_partition(stack_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "1bd2eccb-f46e-4d67-a0be-a57abfe6cc30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 10, 8, 8])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "win_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "51fd93ee-b8f6-4a72-94ed-989ccec994c3",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (32) must match the size of tensor b (64) at non-singleton dimension 4",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m low_res_masks, iou_predictions \u001b[38;5;241m=\u001b[39m  \u001b[43mbisam\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmask_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_masks_batch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimage_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage_embeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# (B, 256, 64, 64)\u001b[39;49;00m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimage_pe\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbisam\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprompt_encoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_dense_pe\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# (1, 256, 64, 64)\u001b[39;49;00m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43msparse_prompt_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msparse_embeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# (B, N, 2, 256)\u001b[39;49;00m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdense_prompt_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdense_embeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# (B, N, 256, 64, 64)\u001b[39;49;00m\n\u001b[1;32m      6\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m preds, iou_predictions \u001b[38;5;241m=\u001b[39m bisam\u001b[38;5;241m.\u001b[39mselect_masks(\n\u001b[1;32m      9\u001b[0m     low_res_masks, \n\u001b[1;32m     10\u001b[0m     iou_predictions, \n\u001b[1;32m     11\u001b[0m     multimask_output\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     12\u001b[0m )\n\u001b[1;32m     13\u001b[0m preds \u001b[38;5;241m=\u001b[39m bisam\u001b[38;5;241m.\u001b[39mupscale_masks(\n\u001b[1;32m     14\u001b[0m     preds,\n\u001b[1;32m     15\u001b[0m     IMG_SIZE\n\u001b[1;32m     16\u001b[0m )\n",
      "File \u001b[0;32m~/Documents/IGN/stage_stylo_magique_2024/src/models/segment_anything/modeling/mask_decoder_dev.py:160\u001b[0m, in \u001b[0;36mMaskDecoder.predict_masks_batch\u001b[0;34m(self, image_embeddings, image_pe, sparse_prompt_embeddings, dense_prompt_embeddings)\u001b[0m\n\u001b[1;32m    156\u001b[0m src \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrepeat_interleave(image_embeddings[:, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m], tokens\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    158\u001b[0m \u001b[38;5;66;03m# print(f\"img_embedding (src) expanded to tokens dim : {src.shape}\")\u001b[39;00m\n\u001b[0;32m--> 160\u001b[0m src \u001b[38;5;241m=\u001b[39m \u001b[43msrc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdense_prompt_embeddings\u001b[49m\n\u001b[1;32m    161\u001b[0m pos_src \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrepeat_interleave(image_pe, tokens\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    162\u001b[0m pos_src \u001b[38;5;241m=\u001b[39m pos_src\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mexpand(src\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (32) must match the size of tensor b (64) at non-singleton dimension 4"
     ]
    }
   ],
   "source": [
    "low_res_masks, iou_predictions =  bisam.mask_decoder.predict_masks_batch(\n",
    "    image_embeddings=image_embeddings,  # (B, 256, 64, 64)\n",
    "    image_pe=bisam.prompt_encoder.get_dense_pe(),  # (1, 256, 64, 64)\n",
    "    sparse_prompt_embeddings=sparse_embeddings,  # (B, N, 2, 256)\n",
    "    dense_prompt_embeddings=dense_embeddings,  # (B, N, 256, 64, 64)\n",
    ")\n",
    "\n",
    "preds, iou_predictions = bisam.select_masks(\n",
    "    low_res_masks, \n",
    "    iou_predictions, \n",
    "    multimask_output=False\n",
    ")\n",
    "preds = bisam.upscale_masks(\n",
    "    preds,\n",
    "    IMG_SIZE\n",
    ")\n",
    "preds = preds > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90d76c06-db14-499a-ae21-91a7f5660920",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mx\u001b[49m\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[0;31mNameError\u001b[0m: name 'x' is not defined"
     ]
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff59f3c1-d055-4658-98ac-3d50093ffc2b",
   "metadata": {},
   "source": [
    "### Custom impl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8aa9d05-0d23-40bc-adb3-0aa49067c38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hydra\n",
    "from src.commons.constants import PROJECT_PATH\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "import pandas as pd\n",
    "import os\n",
    "from copy import deepcopy\n",
    "import torch.nn.functional as F\n",
    "from torchmetrics import Metric\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils import data\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "from src.commons.utils_io import load_sam\n",
    "from src.commons.utils import to_numpy, SegAnyChangeVersion, show_img, show_pair_img, show_prediction_sample, resize\n",
    "from src.models.commons.mask_process import extract_object_from_batch, binarize_mask\n",
    "from src.commons.constants import IMG_SIZE\n",
    "from src.data.process import generate_grid_prompt\n",
    "from src.commons.utils import create_sample_grid_with_prompt, get_mask_with_prompt, fig2arr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d12ac6-b9b8-49a4-850d-15d6c3108d96",
   "metadata": {},
   "source": [
    "### Load dloader manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "d518aa01-8748-4083-96a5-5c6e3b5a61d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/MDizier/data/dl/levir-cd/train/label/train_1.png\n",
      "/home/MDizier/data/dl/levir-cd/train/label/train_2.png\n"
     ]
    }
   ],
   "source": [
    "from src.data.loader import BiTemporalDataset\n",
    "from src.data.process import DefaultTransform\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "params = {\n",
    "    \"prompt_type\": \"sample\",\n",
    "    \"n_prompt\": 1,\n",
    "    \"n_shape\":3,\n",
    "    \"loc\": \"center\",\n",
    "    \"batch_size\": 2,\n",
    "}\n",
    "ds = BiTemporalDataset(\n",
    "            name=\"levir-cd\",\n",
    "            dtype=\"train\",\n",
    "            transform=DefaultTransform(),\n",
    "            params=OmegaConf.create(params),\n",
    "        )\n",
    "\n",
    "dloader = data.DataLoader(\n",
    "            ds,\n",
    "            batch_size=params.get('batch_size'),\n",
    "            shuffle=False,\n",
    "            num_workers=0,\n",
    "        )\n",
    "\n",
    "batch = next(iter(dloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "3cc35b11-221b-4d9d-8ec4-448fec5ff761",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "445"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "efe24b08-d9e2-4ffb-896b-9d441ebb8b99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "444"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(445 // 2 )*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc4b152e-af6d-41b5-b5c6-ea40cf579ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#module = hydra.utils.instantiate(cfg.model.instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a6277bf0-3d8d-4ed4-a272-d1d67535bbb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = \"/var/data/usr/mdizier/stylo_magique/checkpoints/sam/sam_vit_b_01ec64.pth\"\n",
    "# module.model.load_state_dict(torch.load(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d2d2cf2-a46c-4c16-99f5-12fd4a9c7b13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-30 14:14:00,322 - INFO ::  build vit_b BiSam\n"
     ]
    }
   ],
   "source": [
    "from src.models.segment_any_change.model import BiSam\n",
    "\n",
    "bisam = load_sam(\n",
    "    model_type=\"vit_b\", model_cls=BiSam, version= \"dev\", device=\"cpu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4c8ca9d7-8a99-418f-9a14-12de9168cbdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "patcher = bisam.image_encoder.patch_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3dc213f8-b1fa-4d66-9b9d-2ee54f48e87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_patches = patcher(batch[\"img_A\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2d6e3256-9597-4320-8cd3-13e9caf8195c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([768, 3, 16, 16])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(patcher.proj.named_parameters()))[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3825b2d2-43d7-4fc6-a995-beeabbf1e3fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 1024, 1024])\n",
      "torch.Size([2, 64, 64, 768])\n"
     ]
    }
   ],
   "source": [
    "print(batch[\"img_A\"].shape)\n",
    "print(img_patches.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "84ae8575-5ffc-4706-a9a6-503f29c39239",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import Type\n",
    "from models.segment_anything.modeling.common import MLPBlock\n",
    "from models.segment_anything.modeling.transformer import Attention\n",
    "from torch import Tensor, nn\n",
    "import torch\n",
    "\n",
    "\n",
    "class CrossAttentionBlock(nn.Module):\n",
    "    def __init__(\n",
    "            self, \n",
    "            embedding_dim: int, \n",
    "            num_heads: int, \n",
    "            mlp_dim: int = 2048,\n",
    "            activation: Type[nn.Module] = nn.ReLU,\n",
    "\n",
    "        ) -> None:\n",
    "        super().__init__()\n",
    "        self.cross_attn = Attention(embedding_dim, num_heads)\n",
    "        self.norm1 = nn.LayerNorm(embedding_dim)\n",
    "        self.norm2 = nn.LayerNorm(embedding_dim)\n",
    "        self.norm3 = nn.LayerNorm(embedding_dim)\n",
    "        self.mlp = MLPBlock(embedding_dim, mlp_dim, activation)\n",
    "\n",
    "    def forward(self, queries, keys):\n",
    "        \"\"\"Queries attend to keys\"\"\"\n",
    "        q = self.norm1(queries)\n",
    "        k = self.norm2(keys)\n",
    "        attn_out = self.cross_attn(q=q, k=k, v=k)\n",
    "        q = q + attn_out\n",
    "        q = self.norm2(q)\n",
    "        out = self.mlp(q) + q\n",
    "        return out\n",
    "\n",
    "\n",
    "# class Attention(nn.Module):\n",
    "#     \"\"\"\n",
    "#     An attention layer that allows for downscaling the size of the embedding\n",
    "#     after projection to queries, keys, and values.\n",
    "#     \"\"\"\n",
    "\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         embedding_dim: int,\n",
    "#         num_heads: int,\n",
    "#         downsample_rate: int = 1,\n",
    "#     ) -> None:\n",
    "#         super().__init__()\n",
    "#         self.embedding_dim = embedding_dim\n",
    "#         self.internal_dim = embedding_dim // downsample_rate\n",
    "#         self.num_heads = num_heads\n",
    "#         assert self.internal_dim % num_heads == 0, \"num_heads must divide embedding_dim.\"\n",
    "\n",
    "#         self.q_proj = nn.Linear(embedding_dim, self.internal_dim)\n",
    "#         self.k_proj = nn.Linear(embedding_dim, self.internal_dim)\n",
    "#         self.v_proj = nn.Linear(embedding_dim, self.internal_dim)\n",
    "#         self.out_proj = nn.Linear(self.internal_dim, embedding_dim)\n",
    "\n",
    "#     def _separate_heads(self, x: Tensor, num_heads: int) -> Tensor:\n",
    "#         b, n, c = x.shape\n",
    "#         x = x.reshape(b, n, num_heads, c // num_heads)\n",
    "#         return x.transpose(1, 2)  # B x N_heads x N_tokens x C_per_head\n",
    "\n",
    "#     def _recombine_heads(self, x: Tensor) -> Tensor:\n",
    "#         b, n_heads, n_tokens, c_per_head = x.shape\n",
    "#         x = x.transpose(1, 2)\n",
    "#         return x.reshape(b, n_tokens, n_heads * c_per_head)  # B x N_tokens x C\n",
    "\n",
    "#     def forward(self, q: Tensor, k: Tensor, v: Tensor) -> Tensor:\n",
    "#         # Input projections\n",
    "#         q = self.q_proj(q)\n",
    "#         k = self.k_proj(k)\n",
    "#         v = self.v_proj(v)\n",
    "\n",
    "#         # Separate into heads\n",
    "#         print(\"q\", q.shape)\n",
    "#         q = self._separate_heads(q, self.num_heads)\n",
    "#         print(\"q\", q.shape)\n",
    "\n",
    "#         k = self._separate_heads(k, self.num_heads)\n",
    "#         v = self._separate_heads(v, self.num_heads)\n",
    "\n",
    "#         # Attention\n",
    "#         _, _, _, c_per_head = q.shape\n",
    "#         attn = q @ k.permute(0, 1, 3, 2)  # B x N_heads x N_tokens x N_tokens\n",
    "#         attn = attn / math.sqrt(c_per_head)\n",
    "#         attn = torch.softmax(attn, dim=-1)\n",
    "#         print(\"attn shape\", attn.shape)\n",
    "#         # Get output\n",
    "#         out = attn @ v\n",
    "#         print(out.shape)\n",
    "#         out = self._recombine_heads(out)\n",
    "#         out = self.out_proj(out)\n",
    "\n",
    "#         return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "fdea08ba-adb8-4a12-9d73-beefaf40633a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = bisam.image_encoder(batch[\"img_A\"]).permute(0, 2, 3, 1)\n",
    "x2 = bisam.image_encoder(batch[\"img_B\"]).permute(0, 2, 3, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "512d7fb7-6d41-46ff-9aae-83a26d40f97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "blk = CrossAttentionBlock(embedding_dim=256, num_heads=4, mlp_dim=2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "3098be7a-04ab-4df6-a40d-5a2c425dcd5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 64, 64, 256])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "ee048221-2ce4-454c-889d-1b58388ec29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "b, h, w, c = x1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "0b466f75-8b24-4375-85c6-9499135b9274",
   "metadata": {},
   "outputs": [],
   "source": [
    "# flat spatial dimensions\n",
    "out = blk(queries=x1.view(2, -1, 256), keys=x2.view(2, -1, 256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "bf2e4054-7b7c-4a9f-9a98-e1f56955ca84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4096"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "64*64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e2de5fc9-3809-49ed-8ffa-7d42178edcb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4096, 256])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "9e8af1eb-ba47-47dd-bc59-ca3a7572cd4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = out.view(b, h, w, c).permute(0, 3, 1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "167e3021-1b4e-4813-8223-58a09bf0af8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 256, 64, 64])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "b3457623-1883-42f3-85ea-cc66ff24299f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28.0"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "672 / 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730e17c9-e932-4fdd-8920-298e793c744e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mp-3.10",
   "language": "python",
   "name": "mp_3.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
