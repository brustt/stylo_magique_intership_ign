{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a077cba-3aa0-4f3c-aa4e-0ff76f7351d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1a7a05f-8615-42e5-ae8c-174df768b619",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome in JZAY\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "import pandas as pd\n",
    "import os\n",
    "from copy import deepcopy\n",
    "import torch.nn.functional as F\n",
    "from torchmetrics import Metric\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils import data\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "from src.commons.utils import to_numpy, SegAnyChangeVersion, show_img, show_pair_img, show_prediction_sample, resize\n",
    "from src.models.commons.mask_process import extract_object_from_batch, binarize_mask\n",
    "from src.commons.constants import IMG_SIZE\n",
    "from src.data.process import generate_grid_prompt\n",
    "from src.commons.utils import create_sample_grid_with_prompt, get_mask_with_prompt, fig2arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7ce822f-1383-4f6d-855d-48a41b79e977",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.commons.utils_io import load_sam\n",
    "from src.models.commons.bisam import BiSam2, SamModeInference\n",
    "from src.models.commons.model import BiSam\n",
    "\n",
    "from src.commons.utils import batch_to_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6befc0-89ad-477f-96d9-fa9b6e3d3def",
   "metadata": {},
   "source": [
    "### Load dloader manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7b39254-3994-4f95-b1fb-4fd354a3146e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data.loader import BiTemporalDataset\n",
    "from src.data.process import DefaultTransform\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "params = {\n",
    "    \"prompt_type\": \"sample\",\n",
    "    \"n_prompt\": 1,\n",
    "    \"n_shape\":3,\n",
    "    \"loc\": \"center\",\n",
    "    \"batch_size\": 2,\n",
    "}\n",
    "ds = BiTemporalDataset(\n",
    "            name=\"levir-cd\",\n",
    "            dtype=\"test\",\n",
    "            transform=DefaultTransform(),\n",
    "            params=OmegaConf.create(params),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d3c3788-fc07-4c37-a430-17a4d0d23c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dloader = data.DataLoader(\n",
    "            ds,\n",
    "            batch_size=params.get('batch_size'),\n",
    "            shuffle=False,\n",
    "            num_workers=0,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8eff82f-7fe4-437d-b129-211754affa32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/rustt/Documents/IGN/data/levir-cd/test/label/test_1.png\n",
      "/home/rustt/Documents/IGN/data/levir-cd/test/label/test_2.png\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(dloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd4bf86-bbf8-4017-956a-b45b6a9049c0",
   "metadata": {},
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7b1c4eb-212a-4aee-8c66-2a7ae1109a7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-23 21:21:21,942 - INFO ::  build vit_b BiSam\n"
     ]
    }
   ],
   "source": [
    "bisam = load_sam(\n",
    "    model_type=\"vit_b\", model_cls=BiSam, version= \"dev\", device=\"cpu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b19bca-6d80-4545-a92c-9b1a1154a856",
   "metadata": {},
   "source": [
    "### Load config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36dff64d-cb8f-44e0-971c-650b914c92f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.commons.utils_io import load_config\n",
    "import hydra\n",
    "from hydra.core.global_hydra import GlobalHydra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "969326b1-9fd4-402e-b274-925336df0d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "GlobalHydra.instance().clear()\n",
    "list_args=[\"experiment=mp_naive\", \"sam_type=small\", \"data=levir-cd\", \"data.params.n_shape=3\", \"data.params.num_worker=0\"]\n",
    "cfg = load_config(list_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "08c98ffa-42c8-4ada-92e1-4d2eee4f2f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "module = hydra.utils.instantiate(cfg.model.instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "784233ff-f9e1-4fe7-bcb0-9ca9f69dcc5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_images = torch.cat(\n",
    "    [batch[\"img_A\"], batch[\"img_B\"]]\n",
    ")\n",
    "input_images = bisam.preprocess(input_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d1ad78e8-6c59-4b53-baf4-08c8aaff23c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 49.7 s, sys: 17.5 s, total: 1min 7s\n",
      "Wall time: 8.49 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "emb_B = bisam.image_encoder(batch[\"img_B\"][0].unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e3ed91f1-2e7f-43ac-80e4-fb2896576618",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_A = torch.rand((256, 64, 64))\n",
    "img_B = torch.rand((256, 64, 64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b2617cf2-c4d5-49d7-98b8-de5601051558",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_embeddings = torch.cat([img_A, img_B])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2001144d-a8f4-4ddc-ae3b-2b23fdeb0472",
   "metadata": {},
   "outputs": [],
   "source": [
    "# one mask for prompt\n",
    "point_coords = batch[\"point_coords\"]\n",
    "point_labels = batch[\"point_labels\"]\n",
    "\n",
    "sparse_embeddings, dense_embeddings = bisam.prompt_encoder(\n",
    "    points=(\n",
    "        point_coords[:,None, :],\n",
    "        point_labels[:, None,...],\n",
    "    ), \n",
    "    boxes=None,\n",
    "    masks=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6ee28dbd-3679-43e4-9565-096557bf3fed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sparse_embeddings: torch.Size([2, 1, 4, 256])\n",
      "dense_embeddings: torch.Size([2, 1, 256, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "print(f\"sparse_embeddings: {sparse_embeddings.shape}\")\n",
    "print(f\"dense_embeddings: {dense_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cc441a95-fd80-4359-9117-e8e653e890d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "layer = nn.Conv2d(image_embeddings.shape[0], image_embeddings.shape[0] // 2, kernel_size=3, padding=1)\n",
    "out = layer(image_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "18c219e8-2abd-4388-96a3-6d6d38068370",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([256, 512, 3, 3])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8abe8c7b-b10e-4504-99e5-d477cb050be1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512, 64, 64])\n",
      "torch.Size([256, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "print(image_embeddings.shape)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01814f9-e045-4f78-adf4-1706a1f0e5d1",
   "metadata": {},
   "source": [
    "### Try to extent SAM modules to bitemporal\n",
    "* prompt embedding\n",
    "* Maks decoder\n",
    "\n",
    "Essayons de concatener les embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d2de4338-701f-4340-b11e-a5ce059160bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512, 64, 64])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f7e4b6d5-4fcf-49f7-bac2-3f223ea17a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_embeddings = image_embeddings.unsqueeze(0) # extent over batch dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c0440112-824c-4270-9c3b-dcb323e51bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.segment_anything.modeling.prompt_encoder_dev import PromptEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b8a6717c-69fd-4b89-be89-a24bebf5e673",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_embed_dim = image_embeddings.shape[1]\n",
    "image_size = 1024\n",
    "vit_patch_size = 16\n",
    "image_embedding_size = image_size // vit_patch_size\n",
    "\n",
    "\n",
    "prompt_encoder_extent = PromptEncoder(\n",
    "    embed_dim=prompt_embed_dim,\n",
    "    image_embedding_size=(image_embedding_size, image_embedding_size),\n",
    "    input_image_size=(image_size, image_size),\n",
    "    mask_in_chans=16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ec3ddb7b-f05d-4b1f-a5e7-414f8e0e5c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_embeddings, dense_embeddings = prompt_encoder_extent(\n",
    "    points=(\n",
    "        point_coords[:,None, :],\n",
    "        point_labels[:, None,...],\n",
    "    ), \n",
    "    boxes=None,\n",
    "    masks=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "874b4ed5-10ab-4f00-8a65-08cc8987b850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sparse_embeddings: torch.Size([2, 1, 4, 512])\n",
      "dense_embeddings: torch.Size([2, 1, 512, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "print(f\"sparse_embeddings: {sparse_embeddings.shape}\")\n",
    "print(f\"dense_embeddings: {dense_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212286b4-24b5-429b-beff-21edbf16c74e",
   "metadata": {},
   "source": [
    "### Cross attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c47fe7c3-9a96-469c-afc8-41788182bc08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class LocalCrossAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, window_size):\n",
    "        super(LocalCrossAttention, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.window_size = window_size\n",
    "        self.multihead_attn = nn.MultiheadAttention(embed_dim=embed_dim, num_heads=num_heads, batch_first=True)\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(embed_dim, embed_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        B, C, H, W = x1.size()\n",
    "        window_size = self.window_size\n",
    "        assert H % window_size == 0 and W % window_size == 0, \"Height and Width must be divisible by the window size.\"\n",
    "\n",
    "        # Function to divide into windows\n",
    "        def window_partition(x):\n",
    "            \"\"\"\n",
    "            return [B * num_windows, C, window_size, window_size].\n",
    "            \"\"\"\n",
    "            B, C, H, W = x.shape\n",
    "            x = x.view(B, C, H // window_size, window_size, W // window_size, window_size)\n",
    "            windows = x.permute(0, 2, 4, 1, 3, 5).contiguous()\n",
    "            windows = windows.view(-1, C, window_size, window_size)\n",
    "            return windows\n",
    "\n",
    "        # Function to merge windows back to feature map\n",
    "        def window_reverse(windows, H, W):\n",
    "            B = int(windows.shape[0] / (H * W / window_size / window_size))\n",
    "            x = windows.view(B, H // window_size, W // window_size, -1, window_size, window_size)\n",
    "            x = x.permute(0, 3, 1, 4, 2, 5).contiguous()\n",
    "            x = x.view(B, -1, H, W)\n",
    "            return x\n",
    "\n",
    "        x1_windows = window_partition(x1)  # shape: (num_windows*B, C, window_size, window_size)\n",
    "        x2_windows = window_partition(x2)  # shape: (num_windows*B, C, window_size, window_size)\n",
    "\n",
    "        # Flatten spatial dimensions\n",
    "        x1_windows = x1_windows.view(-1, C, window_size * window_size).permute(0, 2, 1)  # (num_windows*B, window_size*window_size, C)\n",
    "        x2_windows = x2_windows.view(-1, C, window_size * window_size).permute(0, 2, 1)  # (num_windows*B, window_size*window_size, C)\n",
    "\n",
    "        # Apply cross attention within windows : multihead_attn(query, key, value)\n",
    "        attn_output, _ = self.multihead_attn(x1_windows, x2_windows, x2_windows)  # (num_windows*B, window_size*window_size, C)\n",
    "        x1_windows = x1_windows + attn_output\n",
    "        x1_windows = self.norm1(x1_windows)\n",
    "\n",
    "        # Feed Forward\n",
    "        ff_output = self.ff(x1_windows)  # (num_windows*B, window_size*window_size, C)\n",
    "        x1_windows = x1_windows + ff_output\n",
    "        x1_windows = self.norm2(x1_windows)\n",
    "\n",
    "        # Reshape back to windowed spatial dimensions\n",
    "        x1_windows = x1_windows.permute(0, 2, 1).view(-1, C, window_size, window_size)  # (num_windows*B, C, window_size, window_size)\n",
    "\n",
    "        # Merge windows back to original dimensions\n",
    "        x1 = window_reverse(x1_windows, H, W)  # (B, C, H, W)\n",
    "\n",
    "        return x1\n",
    "\n",
    "\n",
    "sam_decoder = None  # replace with actual decoder\n",
    "embed_dim = 256\n",
    "num_heads = 8\n",
    "window_size = 8  # Example window size, adjust as needed\n",
    "\n",
    "# Instantiate the local cross attention module\n",
    "local_cross_attention_module = LocalCrossAttention(embed_dim, num_heads, window_size)\n",
    "\n",
    "B = 2\n",
    "# Example input tensors, replace with actual image data\n",
    "emb1 = torch.rand(B, 256, 64, 64)\n",
    "emb2 = torch.rand(B, 256, 64, 64)\n",
    "\n",
    "output = local_cross_attention_module(emb1, emb2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "07913dbd-907b-4b73-b538-75e79894f652",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 256, 64, 64])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ed62a1df-2f45-485d-ae41-09e82f1d7143",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to divide into windows\n",
    "def window_partition(x):\n",
    "    \"\"\"\n",
    "    return [B * num_windows, C, window_size, window_size].\n",
    "    \"\"\"\n",
    "    B, C, H, W = x.shape\n",
    "    x = x.view(B, C, H // window_size, window_size, W // window_size, window_size)\n",
    "    windows = x.permute(0, 2, 4, 1, 3, 5).contiguous()\n",
    "    windows = windows.view(-1, C, window_size, window_size)\n",
    "    return windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b1fbf8ee-6cbd-47bb-8a85-8e0c1ce3da0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_tensor = []\n",
    "for _ in range(10):\n",
    "    stack_tensor.append(\n",
    "        torch.zeros(1, 1, 64, 64) + _\n",
    "    )\n",
    "stack_tensor = torch.cat(stack_tensor, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "7a0f502f-43ce-4de4-a951-d8586b18b475",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10, 64, 64])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stack_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e5be09e1-0be5-42ca-9612-3ef62aa90790",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 8\n",
    "win_tensor = window_partition(stack_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "1bd2eccb-f46e-4d67-a0be-a57abfe6cc30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 10, 8, 8])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "win_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "51fd93ee-b8f6-4a72-94ed-989ccec994c3",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (32) must match the size of tensor b (64) at non-singleton dimension 4",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m low_res_masks, iou_predictions \u001b[38;5;241m=\u001b[39m  \u001b[43mbisam\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmask_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_masks_batch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimage_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage_embeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# (B, 256, 64, 64)\u001b[39;49;00m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimage_pe\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbisam\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprompt_encoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_dense_pe\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# (1, 256, 64, 64)\u001b[39;49;00m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43msparse_prompt_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msparse_embeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# (B, N, 2, 256)\u001b[39;49;00m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdense_prompt_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdense_embeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# (B, N, 256, 64, 64)\u001b[39;49;00m\n\u001b[1;32m      6\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m preds, iou_predictions \u001b[38;5;241m=\u001b[39m bisam\u001b[38;5;241m.\u001b[39mselect_masks(\n\u001b[1;32m      9\u001b[0m     low_res_masks, \n\u001b[1;32m     10\u001b[0m     iou_predictions, \n\u001b[1;32m     11\u001b[0m     multimask_output\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     12\u001b[0m )\n\u001b[1;32m     13\u001b[0m preds \u001b[38;5;241m=\u001b[39m bisam\u001b[38;5;241m.\u001b[39mupscale_masks(\n\u001b[1;32m     14\u001b[0m     preds,\n\u001b[1;32m     15\u001b[0m     IMG_SIZE\n\u001b[1;32m     16\u001b[0m )\n",
      "File \u001b[0;32m~/Documents/IGN/stage_stylo_magique_2024/src/models/segment_anything/modeling/mask_decoder_dev.py:160\u001b[0m, in \u001b[0;36mMaskDecoder.predict_masks_batch\u001b[0;34m(self, image_embeddings, image_pe, sparse_prompt_embeddings, dense_prompt_embeddings)\u001b[0m\n\u001b[1;32m    156\u001b[0m src \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrepeat_interleave(image_embeddings[:, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m], tokens\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    158\u001b[0m \u001b[38;5;66;03m# print(f\"img_embedding (src) expanded to tokens dim : {src.shape}\")\u001b[39;00m\n\u001b[0;32m--> 160\u001b[0m src \u001b[38;5;241m=\u001b[39m \u001b[43msrc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdense_prompt_embeddings\u001b[49m\n\u001b[1;32m    161\u001b[0m pos_src \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrepeat_interleave(image_pe, tokens\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    162\u001b[0m pos_src \u001b[38;5;241m=\u001b[39m pos_src\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mexpand(src\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (32) must match the size of tensor b (64) at non-singleton dimension 4"
     ]
    }
   ],
   "source": [
    "low_res_masks, iou_predictions =  bisam.mask_decoder.predict_masks_batch(\n",
    "    image_embeddings=image_embeddings,  # (B, 256, 64, 64)\n",
    "    image_pe=bisam.prompt_encoder.get_dense_pe(),  # (1, 256, 64, 64)\n",
    "    sparse_prompt_embeddings=sparse_embeddings,  # (B, N, 2, 256)\n",
    "    dense_prompt_embeddings=dense_embeddings,  # (B, N, 256, 64, 64)\n",
    ")\n",
    "\n",
    "preds, iou_predictions = bisam.select_masks(\n",
    "    low_res_masks, \n",
    "    iou_predictions, \n",
    "    multimask_output=False\n",
    ")\n",
    "preds = bisam.upscale_masks(\n",
    "    preds,\n",
    "    IMG_SIZE\n",
    ")\n",
    "preds = preds > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ae8575-5ffc-4706-a9a6-503f29c39239",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
