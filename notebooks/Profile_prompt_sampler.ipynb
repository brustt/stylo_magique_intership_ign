{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fdf8facc-ab7a-44bf-a1ef-74e1eda1dbeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hydra\n",
    "import torch\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "from typing import Any, Dict, List, Optional, Tuple, Union\n",
    "from deprecated import deprecated\n",
    "from omegaconf import DictConfig\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils import data\n",
    "\n",
    "from src.data.process import DefaultTransform\n",
    "from src.data.loader import BiTemporalDataset, load_levircd_sample\n",
    "from src.models.commons.mask_process import binarize_mask, extract_object_from_batch\n",
    "from src.models.segment_anything.utils.transforms import ResizeLongestSide\n",
    "from src.models.segment_anything.utils.amg import build_point_grid\n",
    "from src.commons.utils import *\n",
    "from src.commons.utils_io import load_img\n",
    "from src.commons.constants import PROJECT_PATH, DEVICE, IMG_SIZE\n",
    "\n",
    "from hydra.core.global_hydra import GlobalHydra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f868bcf8-2e80-490e-bc4d-adeaf0e700d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.profiler import profile, record_function, ProfilerActivity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0cce21f-a2d3-4ddd-a3bd-339fd5507037",
   "metadata": {},
   "source": [
    "### Batch iteration with prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5e868f-bf51-4dd7-a4c7-719f3a47d50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"prompt_type\": \"sample\",\n",
    "    \"n_prompt\": 1,\n",
    "    \"n_shape\": 3,\n",
    "    \"loc\": \"center\",\n",
    "    \"batch_size\": 2,\n",
    "}\n",
    "ds = BiTemporalDataset(\n",
    "        name=\"levir-cd\",\n",
    "        dtype=\"train\",\n",
    "        transform=DefaultTransform(),\n",
    "        params=OmegaConf.create(params),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d847828-4acf-4c8a-b138-ff617ef5395c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dloader = data.DataLoader(\n",
    "            ds,\n",
    "            batch_size=params.get('batch_size'),\n",
    "            shuffle=False,\n",
    "            num_workers=0,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2709ae80-a721-4261-b687-9b61895a42e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3d601b-470b-4d86-88c5-130b6f180e77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc60b80-c67c-4b72-9149-906766fcde8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7806cf19-0c84-41e7-9b20-c01414713e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_levircd_sample(None, data_type=\"test\", seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01440069-aa1c-4178-9f02-04c72c5abada",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds_name = \"levir-cd\"\n",
    "# device = \"cpu\"\n",
    "# batch_size = 1\n",
    "# ds_type = \"test\"\n",
    "\n",
    "# fname=None\n",
    "# fidx=100\n",
    "\n",
    "# exp_name = \"adapter_concat_bce_lrx10-4_shuffle\"\n",
    "\n",
    "# if not any([fname, fidx]):\n",
    "#     raise ValueError(\"Please provide at least a sample name or an sample index\")\n",
    "# # priority to fname\n",
    "# if fname is not None:\n",
    "#     fidx = ds.get_from_name(fname, return_sample=False)\n",
    "# # TODO: allow indices sequence\n",
    "# if isinstance(fidx, int): fidx = [fidx]\n",
    "\n",
    "\n",
    "# cfg, data_module, module = load_exp(exp_name)\n",
    "# ds = get_ds(ds_type, data_module)\n",
    "# sub_dl = data.DataLoader(data.Subset(ds, fidx), batch_size=1, shuffle=False)\n",
    "# batch = next(iter(sub_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933e0160-4e17-4e24-929c-341b9e35b798",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7242358c-0636-4e94-8a2c-852dfd799694",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e8d993-9da1-4493-b409-d02481ffcdd1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8337bb42-27e7-4276-8215-97bac0447fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PointSampler:\n",
    "    \"\"\"Prompt sampler - restricted to points\n",
    "    \n",
    "    Each generated points is under coordinates format (X,Y) in pixels.\n",
    "    \"\"\"\n",
    "    MIN_AREA = 25\n",
    "    \n",
    "    def __init__(self):\n",
    "        self._register_sample_method = {\n",
    "            \"random\": self.draw_random_point,\n",
    "            \"center\": self.draw_center_point,\n",
    "        }\n",
    "\n",
    "    def sample_candidates_shapes(self, shapes: torch.Tensor, n_shape: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        # assign equals weights\n",
    "        probs = torch.ones(shapes.shape[0]) / shapes.shape[0]\n",
    "        # Sample the shapes\n",
    "        id_candidates_shapes = torch.multinomial(\n",
    "            probs,\n",
    "            n_shape,\n",
    "            # we sample with replacement to keeping same tensor dimensions over batch if not enough shapes\n",
    "            replacement=False if (shapes.shape[0] >= n_shape) else True,\n",
    "        )\n",
    "        # get the coord of the pixels shapes (M x 3) - M number of not zeros pixels\n",
    "        coords_candidates = torch.nonzero(shapes[id_candidates_shapes]).to(torch.float)\n",
    "        return coords_candidates, id_candidates_shapes\n",
    "    \n",
    "\n",
    "    def sample(self, mask: torch.Tensor,  n_point_per_shape: int, loc: str, n_shape: int):\n",
    "        \"\"\"\n",
    "        Sample m points over n random shape\n",
    "        Return new label if a subset of shapes (n_shape) is selected\n",
    "        \"\"\"\n",
    "\n",
    "        if loc not in list(self._register_sample_method):\n",
    "            raise ValueError(\n",
    "                f\"loc method not valid. Valid values for loc : {list(self._register_sample_method)}\"\n",
    "            )\n",
    "        if not n_shape:\n",
    "            raise ValueError(\"please provide n_shape to sample. One point per shape\")\n",
    "            \n",
    "        if mask.ndim < 3:\n",
    "            mask = mask.unsqueeze(0)\n",
    "        # track id shapes if we a subset of shapes\n",
    "        id_selected_shapes = None\n",
    "        \n",
    "        # extract shapes from mask - squeeze batch dimension\n",
    "        shapes = extract_object_from_batch(mask).squeeze(0)\n",
    "        # print(\"shape\", shapes.shape)\n",
    "        # filter on areas\n",
    "        areas = torch.sum(shapes, dim=(1, 2))\n",
    "        indices = torch.where(areas > self.MIN_AREA)[0]\n",
    "        shapes = shapes[indices,:,:]\n",
    "\n",
    "        # check if there is some shapes extracted - check sum for no-shapes return\n",
    "        # check > 1 first for speed in case of shapes - return no shapes :  (1 x) 1 x H x W\n",
    "        if shapes.shape[0] > 1 or torch.sum(shapes):\n",
    "            # extract all shapes (max of batch) if there are not enough shapes\n",
    "            # n_shape = min(n_shape,  shapes.shape[0]) # not good for diff batch size\n",
    "            coords_candidates, id_selected_shapes = self.sample_candidates_shapes(shapes, n_shape)\n",
    "            # first column of coords_candidates == index of shape\n",
    "            # iterate over the shapes\n",
    "            sample_coords = torch.cat(\n",
    "                [\n",
    "                    # mask coordinates based on shape index\n",
    "                    # select only coordinates dims for _register_sample_method : [:, 1:] => (N, 2)\n",
    "                    self._register_sample_method[loc](\n",
    "                        coords_candidates[coords_candidates[:, 0] == s][:, 1:], n_point_per_shape\n",
    "                    )\n",
    "                    for s in torch.unique(coords_candidates[:, 0])\n",
    "                ]\n",
    "            )\n",
    "            # simulate point type (foreground / background) - foreground default\n",
    "            labels_points = torch.ones(len(sample_coords))\n",
    "        else:\n",
    "            # empty return = sample random points\n",
    "            #sample_coords = torch.zeros((n_shape*n_point_per_shape, 2), dtype=torch.float32) - 1000\n",
    "            sample_coords = torch.as_tensor(np.random.randint(0, mask.shape[-1], size=(n_shape, 2)))\n",
    "            # label - 1\n",
    "            labels_points = torch.zeros(len(sample_coords)) - 1\n",
    "\n",
    "        return sample_coords, labels_points\n",
    "\n",
    "    def draw_random_point(self, shape, n_point):\n",
    "        \"\"\"draw one random point from shape\"\"\"\n",
    "        idx = torch.multinomial(\n",
    "            torch.ones(shape.shape[0], dtype=torch.float), num_samples=n_point\n",
    "        )\n",
    "        # invert pixels coords to (x, y) format\n",
    "        return torch.flip(shape[idx], dims=(1,))\n",
    "\n",
    "    def draw_center_point(self, shape, n_point):\n",
    "        \"\"\"\n",
    "        Sample approximation center. Proxy for hard concave object where \"natural center\" (simple average) doesn't belong to the shape.\n",
    "        shape : (M, 2) : (shape's pixels, px coordinates dim)\n",
    "        \"\"\"\n",
    "        # proxy for hard concave object\n",
    "        visible_center = torch.mean(shape, dim=0).to(int)\n",
    "        # euclidean distance\n",
    "        dist_center = torch.cdist(visible_center.unsqueeze(0).to(torch.fldeviceoat), shape, p=2.).view(-1)\n",
    "        idx = torch.nonzero(dist_center).view(-1)\n",
    "        dist_center, shape = dist_center[idx], shape[idx,...]\n",
    "        # sample point from inverse distance weighting => in favor of \"closest center\" point - take first 50 pts arbitrary\n",
    "        # values, indices  = torch.topk(dist_center, min(50, dist_center.shape[0]), largest=False, sorted=True)\n",
    "        # values, indices = values.view(-1), indices.view(-1)\n",
    "        idx = torch.multinomial(\n",
    "            1/dist_center, num_samples=n_point\n",
    "        )\n",
    "        # flip to convert to (x, y) format\n",
    "        return torch.flip(shape[idx], dims=(1,))\n",
    "\n",
    "    def profiled_sample(self, mask: torch.Tensor, n_point_per_shape: int, loc: str, n_shape: int):\n",
    "        with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],\n",
    "                     record_shapes=True,\n",
    "                     profile_memory=True) as prof:\n",
    "            with record_function(\"sample\"):\n",
    "                result = self.sample(mask, n_point_per_shape, loc, n_shape)\n",
    "        \n",
    "        print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10))\n",
    "        #print(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=10))\n",
    "        prof.export_chrome_trace(\"trace_chrome\" + str(prof.step_num) + \".json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5f6df8-9dfa-451f-9464-e6bb88d41908",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "523d3b32-e530-4a9c-827d-814021112e9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "89cca87b-55a0-4a25-a78a-8e2dbc86d3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage\n",
    "idx=12\n",
    "path_label,path_A, path_B = df.iloc[idx]\n",
    "label = load_img(path_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f7958416-7a08-43ae-a35d-67e83938e884",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_img(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b5914554-b0ba-4131-a46c-c856e9ec28ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W kineto_shim.cpp:333] Adding profiling metadata requires using torch.profiler with Kineto support (USE_KINETO=1)\n",
      "[W kineto_shim.cpp:333] Adding profiling metadata requires using torch.profiler with Kineto support (USE_KINETO=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                             Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg       CPU Mem  Self CPU Mem      CUDA Mem  Self CUDA Mem    # of Calls  \n",
      "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                           sample        43.71%      84.845ms       100.00%     194.125ms     194.125ms       4.180ms         2.15%     194.137ms     194.137ms          12 b     -13.82 Kb         512 b      -6.33 Gb             1  \n",
      "                 aten::max_pool2d         3.56%       6.909ms         7.05%      13.692ms      27.384us       2.518ms         1.30%     104.631ms     209.262us           0 b           0 b       3.06 Gb      -2.80 Gb           500  \n",
      "    aten::max_pool2d_with_indices         3.49%       6.783ms         3.49%       6.783ms      13.566us     102.113ms        52.60%     102.113ms     204.226us           0 b           0 b       5.86 Gb       5.86 Gb           500  \n",
      "                      aten::index         0.30%     588.000us        20.28%      39.378ms       4.375ms      39.256ms        20.22%      40.164ms       4.463ms           0 b           0 b     672.05 Mb     672.02 Mb             9  \n",
      "                        aten::mul         3.13%       6.072ms         3.13%       6.072ms      12.144us      29.811ms        15.36%      29.811ms      59.622us           0 b           0 b       1.95 Gb       1.95 Gb           500  \n",
      "                    aten::one_hot         0.02%      47.000us         0.10%     188.000us     188.000us      10.000us         0.01%       7.066ms       7.066ms           0 b           0 b     648.00 Mb           0 b             1  \n",
      "                      aten::fill_         0.02%      39.000us         0.02%      39.000us       2.167us       5.566ms         2.87%       5.566ms     309.222us           0 b           0 b           0 b           0 b            18  \n",
      "                      aten::zeros         0.02%      33.000us         0.04%      77.000us      77.000us      24.000us         0.01%       5.537ms       5.537ms           0 b           0 b     648.00 Mb           0 b             1  \n",
      "                      aten::zero_         0.01%      19.000us         0.02%      39.000us      39.000us      15.000us         0.01%       5.504ms       5.504ms           0 b           0 b           0 b           0 b             1  \n",
      "                        aten::sum         0.12%     224.000us         0.12%     230.000us      46.000us       2.905ms         1.50%       2.942ms     588.400us          16 b          16 b       1.00 Kb       1.00 Kb             5  \n",
      "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 194.125ms\n",
      "Self CUDA time total: 194.137ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "label = torch.tensor(label, dtype=torch.float32)\n",
    "n_point_per_shape = 1\n",
    "loc = \"random\"\n",
    "n_shape = 3\n",
    "sampler = PointSampler()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    label = label.cuda()\n",
    "\n",
    "sampler.profiled_sample(label, n_point_per_shape, loc, n_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9ff6a172-d6a3-4ae9-af47-3ea1af5f9086",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sampler' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43msampler\u001b[49m\u001b[38;5;241m.\u001b[39msample(label, n_point_per_shape, loc, n_shape)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sampler' is not defined"
     ]
    }
   ],
   "source": [
    "result = sampler.sample(label, n_point_per_shape, loc, n_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6fd2dd8-7a9e-444f-9520-0cb25216595e",
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ac6fb86f-06fb-4537-a60a-92b09c5a5aaa",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'device'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mlabel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'device'"
     ]
    }
   ],
   "source": [
    "label.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb55e522-5cb2-4384-a108-084658d556d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4273e0c6-74e1-4dc2-bda3-5d1abcbef556",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608e8e2b-0060-4e38-9499-4a6f7d1560e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
