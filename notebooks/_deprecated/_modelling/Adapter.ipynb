{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b4f34bf-3b59-4ed3-ade1-6852fa94a7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "class Adapter_ViT(nn.Module):\n",
    "    \"\"\"Applies mlp adapter to a vision transformer.\n",
    "\n",
    "    Args:\n",
    "        vit_model: a vision transformer model, see base_vit.py\n",
    "        num_layers: number of hidden layers\n",
    "        num_classes: how many classes the model output, default to the vit model\n",
    "\n",
    "    Examples::\n",
    "        >>> model = timm.create_model(\"vit_base_patch16_224.orig_in21k_ft_in1k\", pretrained=True)\n",
    "        >>> adapter_model = Adapter_ViT(model, r=4)\n",
    "        >>> preds = adapter_model(img)\n",
    "        >>> print(preds.shape)\n",
    "        torch.Size([1, 1000])\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                vit_model,\n",
    "                num_classes: int = 0):\n",
    "        super(Adapter_ViT, self).__init__()\n",
    "        \n",
    "        assert num_classes > 0\n",
    "        \n",
    "        for param in vit_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        self.dim = vit_model.blocks[0].attn.qkv.in_features\n",
    "        self.adapter = nn.Sequential()\n",
    "        for t_layer_i in range(len(vit_model.blocks)//2):\n",
    "            self.adapter.add_module(\"layer_\" + str(t_layer_i), nn.Linear(self.dim, self.dim))\n",
    "            self.adapter.add_module(\"relu_\" + str(t_layer_i), nn.ReLU())\n",
    "        self.adapter.add_module(\"fc\", nn.Linear(self.dim, num_classes))\n",
    "        \n",
    "        self.backbone = vit_model\n",
    "        self.backbone.head = self.adapter\n",
    "        \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return self.backbone(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02b1e316-a5c9-4333-ae92-53e3273a713b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-29 15:12:50,612 - INFO ::  Weights loaded for : ['image_encoder']\n"
     ]
    }
   ],
   "source": [
    "from src.commons.utils_io import load_config\n",
    "import hydra\n",
    "from hydra.core.global_hydra import GlobalHydra\n",
    "\n",
    "GlobalHydra.instance().clear()\n",
    "list_args=[\"experiment=mp_naive\", \"sam_type=small\", \"data=levir-cd\", \"data.params.n_shape=3\", \"data.params.num_worker=0\"]\n",
    "cfg = load_config(list_args)\n",
    "\n",
    "module = hydra.utils.instantiate(cfg.model.instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82b8def-a7fa-4b86-a13a-a7762052e227",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = timm.create_model(\"vit_base_patch16_224.orig_in21k_ft_in1k\", pretrained=True)\n",
    "adapter_model = Adapter_ViT(model,num_classes=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a5a0d77-8f44-4e22-a336-5654831c15ad",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'detach'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m t1 \u001b[38;5;241m=\u001b[39m  \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mParameter\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m768\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/mp/lib/python3.10/site-packages/torch/nn/parameter.py:39\u001b[0m, in \u001b[0;36mParameter.__new__\u001b[0;34m(cls, data, requires_grad)\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mTensor\u001b[38;5;241m.\u001b[39m_make_subclass(\u001b[38;5;28mcls\u001b[39m, data, requires_grad)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Path for custom tensors: set a flag on the instance to indicate parameter-ness.\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m t \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m()\u001b[38;5;241m.\u001b[39mrequires_grad_(requires_grad)\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(t) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(data):\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating a Parameter from an instance of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(data)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     42\u001b[0m                        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequires that detach() returns an instance of the same type, but return \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     43\u001b[0m                        \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(t)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m was found instead. To use the type as a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     44\u001b[0m                        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParameter, please correct the detach() semantics defined by \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     45\u001b[0m                        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mits __torch_dispatch__() implementation.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'int' object has no attribute 'detach'"
     ]
    }
   ],
   "source": [
    "t1 =  nn.Parameter((768))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870627d1-909e-4916-b541-b6ba9c727295",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
