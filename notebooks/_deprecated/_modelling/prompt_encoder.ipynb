{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "89e41aa9-4f1c-4f0a-b2ce-ac6ed2bba2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) Meta Platforms, Inc. and affiliates.\n",
    "# All rights reserved.\n",
    "\n",
    "# This source code is licensed under the license found in the\n",
    "# LICENSE file in the root directory of this source tree.\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from typing import Any, Optional, Tuple, Type\n",
    "\n",
    "from segment_any_change.sa_dev.modeling.common import LayerNorm2d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "c5490d8a-63c2-4aff-ba35-83a76ded487c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PromptEncoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embed_dim: int,\n",
    "        image_embedding_size: Tuple[int, int],\n",
    "        input_image_size: Tuple[int, int],\n",
    "        mask_in_chans: int,\n",
    "        activation: Type[nn.Module] = nn.GELU,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Encodes prompts for input to SAM's mask decoder.\n",
    "\n",
    "        Arguments:\n",
    "          embed_dim (int): The prompts' embedding dimension\n",
    "          image_embedding_size (tuple(int, int)): The spatial size of the\n",
    "            image embedding, as (H, W).\n",
    "          input_image_size (int): The padded size of the image as input\n",
    "            to the image encoder, as (H, W).\n",
    "          mask_in_chans (int): The number of hidden channels used for\n",
    "            encoding input masks.\n",
    "          activation (nn.Module): The activation to use when encoding\n",
    "            input masks.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.input_image_size = input_image_size\n",
    "        self.image_embedding_size = image_embedding_size\n",
    "        self.pe_layer = PositionEmbeddingRandom(embed_dim // 2)\n",
    "\n",
    "        self.num_point_embeddings: int = 4  # pos/neg point + 2 box corners\n",
    "        point_embeddings = [\n",
    "            nn.Embedding(1, embed_dim) for i in range(self.num_point_embeddings)\n",
    "        ]\n",
    "        self.point_embeddings = nn.ModuleList(point_embeddings)\n",
    "        self.not_a_point_embed = nn.Embedding(1, embed_dim)\n",
    "\n",
    "        self.mask_input_size = (\n",
    "            4 * image_embedding_size[0],\n",
    "            4 * image_embedding_size[1],\n",
    "        )\n",
    "        self.mask_downscaling = nn.Sequential(\n",
    "            nn.Conv2d(1, mask_in_chans // 4, kernel_size=2, stride=2),\n",
    "            LayerNorm2d(mask_in_chans // 4),\n",
    "            activation(),\n",
    "            nn.Conv2d(mask_in_chans // 4, mask_in_chans, kernel_size=2, stride=2),\n",
    "            LayerNorm2d(mask_in_chans),\n",
    "            activation(),\n",
    "            nn.Conv2d(mask_in_chans, embed_dim, kernel_size=1),\n",
    "        )\n",
    "        self.no_mask_embed = nn.Embedding(1, embed_dim)\n",
    "\n",
    "    def get_dense_pe(self) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Returns the positional encoding used to encode point prompts,\n",
    "        applied to a dense set of points the shape of the image encoding.\n",
    "\n",
    "        Returns:\n",
    "          torch.Tensor: Positional encoding with shape\n",
    "            1x(embed_dim)x(embedding_h)x(embedding_w)\n",
    "        \"\"\"\n",
    "        return self.pe_layer(self.image_embedding_size).unsqueeze(0)\n",
    "\n",
    "    def _embed_points(\n",
    "        self,\n",
    "        points: torch.Tensor,\n",
    "        labels: torch.Tensor,\n",
    "        pad: bool,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Embeds point prompts.\"\"\"\n",
    "        points = points + 0.5  # Shift to center of pixel\n",
    "        if pad:# if boxes is None\n",
    "            print(f\"point shape raw {points.shape}\")\n",
    "            padding_point = torch.zeros((*points.shape[:2], 1, 2), device=points.device)\n",
    "            print(f\"padding_point shape raw {padding_point.shape}\")\n",
    "\n",
    "            padding_label = -torch.ones((*labels.shape[:2], 1), device=labels.device)\n",
    "            points = torch.cat([points, padding_point], dim=2)\n",
    "            print(f\"concat points and padding shape {points.shape}\")\n",
    "\n",
    "            labels = torch.cat([labels, padding_label], dim=2)\n",
    "        point_embedding = self.pe_layer.forward_with_coords(\n",
    "            points, self.input_image_size\n",
    "        )\n",
    "        print(\"emb points\")\n",
    "        print(point_embedding.shape)\n",
    "        point_embedding[labels == -1] = 0.0\n",
    "        point_embedding[labels == -1] += self.not_a_point_embed.weight\n",
    "        point_embedding[labels == 0] += self.point_embeddings[0].weight\n",
    "        point_embedding[labels == 1] += self.point_embeddings[1].weight\n",
    "        return point_embedding\n",
    "\n",
    "    def _embed_boxes(self, boxes: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Embeds box prompts.\"\"\"\n",
    "        boxes = boxes + 0.5  # Shift to center of pixel\n",
    "        coords = boxes.reshape(-1, 2, 2)\n",
    "        corner_embedding = self.pe_layer.forward_with_coords(\n",
    "            coords, self.input_image_size\n",
    "        )\n",
    "        corner_embedding[:, 0, :] += self.point_embeddings[2].weight\n",
    "        corner_embedding[:, 1, :] += self.point_embeddings[3].weight\n",
    "        return corner_embedding\n",
    "\n",
    "    def _embed_masks(self, masks: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Embeds mask inputs.\"\"\"\n",
    "        mask_embedding = self.mask_downscaling(masks)\n",
    "        return mask_embedding\n",
    "\n",
    "    def _get_batch_size(\n",
    "        self,\n",
    "        points: Optional[Tuple[torch.Tensor, torch.Tensor]],\n",
    "        boxes: Optional[torch.Tensor],\n",
    "        masks: Optional[torch.Tensor],\n",
    "    ) -> int:\n",
    "        \"\"\"\n",
    "        Gets the batch size of the output given the batch size of the input prompts.\n",
    "        \"\"\"\n",
    "        if points is not None:\n",
    "            return points[0].shape[1]\n",
    "        elif boxes is not None:\n",
    "            return boxes.shape[0]\n",
    "        elif masks is not None:\n",
    "            return masks.shape[0]\n",
    "        else:\n",
    "            return 1\n",
    "\n",
    "    def _get_device(self) -> torch.device:\n",
    "        return self.point_embeddings[0].weight.device\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        points: Optional[Tuple[torch.Tensor, torch.Tensor]],\n",
    "        boxes: Optional[torch.Tensor],\n",
    "        masks: Optional[torch.Tensor],\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Embeds different types of prompts, returning both sparse and dense\n",
    "        embeddings.\n",
    "\n",
    "        Arguments:\n",
    "          points (tuple(torch.Tensor, torch.Tensor) or none): point coordinates\n",
    "            and labels to embed.\n",
    "          boxes (torch.Tensor or none): boxes to embed\n",
    "          masks (torch.Tensor or none): masks to embed\n",
    "\n",
    "        Returns:\n",
    "          torch.Tensor: sparse embeddings for the points and boxes, with shape\n",
    "            BxNx(embed_dim), where N is determined by the number of input points\n",
    "            and boxes.\n",
    "          torch.Tensor: dense embeddings for the masks, in the shape\n",
    "            Bx(embed_dim)x(embed_H)x(embed_W)\n",
    "        \"\"\"\n",
    "        B = 4\n",
    "        bs = self._get_batch_size(points, boxes, masks)\n",
    "        print(f\"bs : {bs}\")\n",
    "        sparse_embeddings = torch.empty(\n",
    "            (B, bs, 0, self.embed_dim), device=self._get_device()\n",
    "        )\n",
    "        \n",
    "        print(sparse_embeddings.shape)\n",
    "        \n",
    "        if points is not None:\n",
    "            coords, labels = points\n",
    "            point_embeddings = self._embed_points(coords, labels, pad=(boxes is None))\n",
    "            print(point_embeddings.shape)\n",
    "            print(sparse_embeddings.shape)\n",
    "            sparse_embeddings = torch.cat([sparse_embeddings, point_embeddings], dim=2)\n",
    "        \n",
    "        print(sparse_embeddings.shape)\n",
    "        \n",
    "        if boxes is not None:\n",
    "            box_embeddings = self._embed_boxes(boxes)\n",
    "            sparse_embeddings = torch.cat([sparse_embeddings, box_embeddings], dim=1)\n",
    "        \n",
    "        print(sparse_embeddings.shape)\n",
    "        \n",
    "        if masks is not None:\n",
    "            dense_embeddings = self._embed_masks(masks)\n",
    "        else:\n",
    "            dense_embeddings = self.no_mask_embed.weight.reshape(1, -1, 1, 1).expand(\n",
    "                bs, -1, self.image_embedding_size[0], self.image_embedding_size[1]\n",
    "            )\n",
    "\n",
    "        return sparse_embeddings, dense_embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "e6151214-ba29-4591-8264-777668ed00d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionEmbeddingRandom(nn.Module):\n",
    "    \"\"\"\n",
    "    Positional encoding using random spatial frequencies.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_pos_feats: int = 64, scale: Optional[float] = None) -> None:\n",
    "        super().__init__()\n",
    "        if scale is None or scale <= 0.0:\n",
    "            scale = 1.0\n",
    "        self.register_buffer(\n",
    "            \"positional_encoding_gaussian_matrix\",\n",
    "            scale * torch.randn((2, num_pos_feats)),\n",
    "        )\n",
    "\n",
    "    def _pe_encoding(self, coords: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Positionally encode points that are normalized to [0,1].\"\"\"\n",
    "        # assuming coords are in [0, 1]^2 square and have d_1 x ... x d_n x 2 shape\n",
    "        coords = 2 * coords - 1\n",
    "        coords = coords @ self.positional_encoding_gaussian_matrix\n",
    "        coords = 2 * np.pi * coords\n",
    "        # outputs d_1 x ... x d_n x C shape\n",
    "        return torch.cat([torch.sin(coords), torch.cos(coords)], dim=-1)\n",
    "\n",
    "    def forward(self, size: Tuple[int, int]) -> torch.Tensor:\n",
    "        \"\"\"Generate positional encoding for a grid of the specified size.\"\"\"\n",
    "        h, w = size\n",
    "        device: Any = self.positional_encoding_gaussian_matrix.device\n",
    "        grid = torch.ones((h, w), device=device, dtype=torch.float32)\n",
    "        y_embed = grid.cumsum(dim=0) - 0.5\n",
    "        x_embed = grid.cumsum(dim=1) - 0.5\n",
    "        y_embed = y_embed / h\n",
    "        x_embed = x_embed / w\n",
    "\n",
    "        pe = self._pe_encoding(torch.stack([x_embed, y_embed], dim=-1))\n",
    "        return pe.permute(2, 0, 1)  # C x H x W\n",
    "\n",
    "    def forward_with_coords(\n",
    "        self, coords_input: torch.Tensor, image_size: Tuple[int, int]\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Positionally encode points that are not normalized to [0,1].\"\"\"\n",
    "        coords = coords_input.clone()\n",
    "        coords[:, :, :, 0] = coords[:, :, :, 0] / image_size[1]\n",
    "        coords[:, :, :, 1] = coords[:, :, :, 1] / image_size[0]\n",
    "        return self._pe_encoding(coords.to(torch.float))  # B x N x C\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e943926-3179-4e11-8473-180e2f848fd6",
   "metadata": {},
   "source": [
    "#### Numpy prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "fcbc5960-256e-4867-8f29-ca985f13c43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from magic_pen.data.process import prepare_prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "3fc6153d-a9ff-46bb-9537-d36124c3bfd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_point = np.array([[370., 640.], [500, 700]], dtype=np.float32)\n",
    "input_label = np.array([0., 1.], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "2ff2459a-0e86-499c-a997-1008fb00ab8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float32')"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_point.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "59da0f1a-6ef3-44f3-b103-5e28e93f2dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "coords_torch, labels_torch, box_torch, mask_input_torch = prepare_prompts(input_point, input_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "a6dfcf5e-4f38-4f83-8000-24d69032304b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2])"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_torch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "cd3151ae-e2d0-4c75-884e-4af8b1c24fb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 2])"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coords_torch.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c020620a-56b2-43ba-8f66-98b746fe61dc",
   "metadata": {},
   "source": [
    "#### Torch prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "db31c54f-b67a-4313-8c7d-7edc15192fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from magic_pen.data.process import generate_grid_prompt\n",
    "from magic_pen.config import *\n",
    "batch_size = 4\n",
    "n_points = 16\n",
    "\n",
    "coords_torch = torch.as_tensor(\n",
    "    np.tile(generate_grid_prompt(n_points), (batch_size, 1, 1)), dtype=torch.float, device=DEVICE\n",
    ")\n",
    "labels_torch = torch.as_tensor(\n",
    "    np.ones((batch_size, n_points*n_points)), dtype=torch.int, device=DEVICE\n",
    ")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "15b88e60-59ad-4abc-8de0-60c7e2e9f265",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(256, 2)"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_grid_prompt(n_points).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "24ae7eee-42f0-4e20-8772-3c2121fd1d1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 256, 2]), torch.Size([4, 256]))"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coords_torch.shape, labels_torch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "d5ba6942-073b-4c53-a1d8-dcefa8926736",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 256])"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_torch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "3135ac9f-de8b-47b2-81f8-05725f729b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt\n",
    "# input_point = torch.tensor([[200, 900], [150, 150], [100, 450], [600, 300], [370, 640], [800, 800]]).cuda()\n",
    "# input_label = torch.tensor([1, 1, 1, 1, 1, 1]).cuda()\n",
    "\n",
    "# coords_torch = torch.tensor([[370, 640]]).cpu()[None,...]\n",
    "# labels_torch = torch.tensor([0, 1]).cpu()[None,...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "7ab2d119-fce8-4db8-85ac-1e41bbe8c6ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bs : 256\n",
      "torch.Size([4, 256, 0, 256])\n",
      "point shape raw torch.Size([4, 256, 1, 2])\n",
      "padding_point shape raw torch.Size([4, 256, 1, 2])\n",
      "concat points and padding shape torch.Size([4, 256, 2, 2])\n",
      "emb points\n",
      "torch.Size([4, 256, 2, 256])\n",
      "torch.Size([4, 256, 2, 256])\n",
      "torch.Size([4, 256, 0, 256])\n",
      "torch.Size([4, 256, 2, 256])\n",
      "torch.Size([4, 256, 2, 256])\n"
     ]
    }
   ],
   "source": [
    "points = (coords_torch[:,:, None, :].cpu(), labels_torch[..., None].cpu())\n",
    "\n",
    "prompt_embed_dim = 256\n",
    "image_size = 1024\n",
    "vit_patch_size = 16\n",
    "image_embedding_size = image_size // vit_patch_size\n",
    "\n",
    "prompt_encoder = PromptEncoder(\n",
    "    embed_dim=prompt_embed_dim,\n",
    "    image_embedding_size=(image_embedding_size, image_embedding_size),\n",
    "    input_image_size=(image_size, image_size),\n",
    "    mask_in_chans=16,\n",
    ")\n",
    "\n",
    "sparse_embeddings, dense_embeddings = prompt_encoder(\n",
    "    points=points,\n",
    "    boxes=None,\n",
    "    masks=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "adbee0fa-ce54-4f2b-9b50-2eaad6364ff7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 257, 256])"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparse_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "7ed9b86d-e249-4e11-a1dc-a9aa7a491c1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 256, 64, 64])"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dense_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9126c289-0b5f-495f-bc99-6261fb2bf416",
   "metadata": {},
   "outputs": [],
   "source": [
    "if point_coords is not None:\n",
    "    points = (point_coords, point_labels)\n",
    "else:\n",
    "    points = None\n",
    "\n",
    "# Embed prompts\n",
    "sparse_embeddings, dense_embeddings = self.model.prompt_encoder(\n",
    "    points=points,\n",
    "    boxes=boxes,\n",
    "    masks=mask_input,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a772a2-170b-4575-9711-9851c3b6f4f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3b1200-f968-4e45-b671-bfd8f3de4df7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6a311f-7a26-4247-a0b1-74fd9e0a07a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
