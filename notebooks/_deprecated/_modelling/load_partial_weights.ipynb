{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b2e6f97-0bce-47b6-814c-d3b053393d67",
   "metadata": {},
   "source": [
    "### Load partial weights\n",
    "\n",
    "* cas d'usage : charger weights image encoder et prompt encoder et entrainer decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6a8e700-01aa-4c28-8d1d-8283e87fbea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "import pandas as pd\n",
    "import os\n",
    "from copy import deepcopy\n",
    "import torch.nn.functional as F\n",
    "from torchmetrics import Metric\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils import data\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "from src.commons.utils import to_numpy, SegAnyChangeVersion, show_img, show_pair_img, show_prediction_sample, resize\n",
    "from src.models.commons.mask_process import extract_object_from_batch, binarize_mask\n",
    "from src.commons.constants import IMG_SIZE\n",
    "from src.data.process import generate_grid_prompt\n",
    "from src.commons.utils import create_sample_grid_with_prompt, get_mask_with_prompt, fig2arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3c3142a-4449-4388-962e-52331131076a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.commons.utils_io import load_sam\n",
    "from src.models.commons.bisam import BiSam2, SamModeInference\n",
    "from src.models.segment_any_change.model import BiSam\n",
    "\n",
    "from src.commons.utils import batch_to_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e382abc-7cb9-4c9a-b6ad-1c0a266fe661",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_prompts_on_mask(mask: torch.Tensor, batch, batch_idx: int):\n",
    "    if mask.shape[-1] != IMG_SIZE[0]:\n",
    "        mask = resize(mask, IMG_SIZE)\n",
    "    coord_points = batch[\"point_coords\"][batch_idx]\n",
    "    mask_pt = get_mask_with_prompt(binarize_mask(mask[batch_idx], th=0) , coord_points)\n",
    "    show_img(mask_pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f791cee2-7a13-403a-9e6b-14975f5932a9",
   "metadata": {},
   "source": [
    "### Load dloader manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "969a563a-d1ec-4390-bb8e-741bc9ed7858",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data.loader import BiTemporalDataset\n",
    "from src.data.process import DefaultTransform\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "params = {\n",
    "    \"prompt_type\": \"sample\",\n",
    "    \"n_prompt\": 1,\n",
    "    \"n_shape\":3,\n",
    "    \"loc\": \"center\",\n",
    "    \"batch_size\": 2,\n",
    "}\n",
    "ds = BiTemporalDataset(\n",
    "            name=\"levir-cd\",\n",
    "            dtype=\"train\",\n",
    "            transform=DefaultTransform(),\n",
    "            params=OmegaConf.create(params),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6bfe9599-416f-4c08-a312-2cf465924bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dloader = data.DataLoader(\n",
    "            ds,\n",
    "            batch_size=params.get('batch_size'),\n",
    "            shuffle=False,\n",
    "            num_workers=0,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b5bf053b-5b06-4efe-9ed0-ef9306aad829",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/MDizier/data/dl/levir-cd/test/label/test_1.png\n",
      "/home/MDizier/data/dl/levir-cd/test/label/test_2.png\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(dloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36aa9d67-bb22-4bc3-a9be-532a9f788399",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 445/445 [19:37<00:00,  2.65s/it]\n"
     ]
    }
   ],
   "source": [
    "no_label_track = []\n",
    "min_area = 10\n",
    "\n",
    "for sample in tqdm(ds):\n",
    "    if torch.sum(sample[\"label\"]) < min_area:\n",
    "        no_label_track.append(sample[\"label_path\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "879ad518-ea10-4e0d-a03a-941cbbcb88fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(no_label_track)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702bf80b-677f-4341-a95d-5e6ef6e73d55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c558038d-d474-48a7-a1cc-ae642cab63f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/MDizier/data/dl/levir-cd/train/label/train_183.png',\n",
       " '/home/MDizier/data/dl/levir-cd/train/label/train_192.png',\n",
       " '/home/MDizier/data/dl/levir-cd/train/label/train_195.png',\n",
       " '/home/MDizier/data/dl/levir-cd/train/label/train_196.png',\n",
       " '/home/MDizier/data/dl/levir-cd/train/label/train_197.png',\n",
       " '/home/MDizier/data/dl/levir-cd/train/label/train_198.png',\n",
       " '/home/MDizier/data/dl/levir-cd/train/label/train_204.png',\n",
       " '/home/MDizier/data/dl/levir-cd/train/label/train_205.png',\n",
       " '/home/MDizier/data/dl/levir-cd/train/label/train_206.png',\n",
       " '/home/MDizier/data/dl/levir-cd/train/label/train_212.png',\n",
       " '/home/MDizier/data/dl/levir-cd/train/label/train_213.png',\n",
       " '/home/MDizier/data/dl/levir-cd/train/label/train_214.png',\n",
       " '/home/MDizier/data/dl/levir-cd/train/label/train_217.png',\n",
       " '/home/MDizier/data/dl/levir-cd/train/label/train_218.png',\n",
       " '/home/MDizier/data/dl/levir-cd/train/label/train_219.png',\n",
       " '/home/MDizier/data/dl/levir-cd/train/label/train_221.png',\n",
       " '/home/MDizier/data/dl/levir-cd/train/label/train_251.png',\n",
       " '/home/MDizier/data/dl/levir-cd/train/label/train_260.png',\n",
       " '/home/MDizier/data/dl/levir-cd/train/label/train_264.png',\n",
       " '/home/MDizier/data/dl/levir-cd/train/label/train_265.png',\n",
       " '/home/MDizier/data/dl/levir-cd/train/label/train_269.png',\n",
       " '/home/MDizier/data/dl/levir-cd/train/label/train_280.png',\n",
       " '/home/MDizier/data/dl/levir-cd/train/label/train_294.png',\n",
       " '/home/MDizier/data/dl/levir-cd/train/label/train_302.png',\n",
       " '/home/MDizier/data/dl/levir-cd/train/label/train_303.png',\n",
       " '/home/MDizier/data/dl/levir-cd/train/label/train_306.png',\n",
       " '/home/MDizier/data/dl/levir-cd/train/label/train_309.png',\n",
       " '/home/MDizier/data/dl/levir-cd/train/label/train_310.png',\n",
       " '/home/MDizier/data/dl/levir-cd/train/label/train_317.png',\n",
       " '/home/MDizier/data/dl/levir-cd/train/label/train_319.png',\n",
       " '/home/MDizier/data/dl/levir-cd/train/label/train_337.png',\n",
       " '/home/MDizier/data/dl/levir-cd/train/label/train_339.png',\n",
       " '/home/MDizier/data/dl/levir-cd/train/label/train_343.png',\n",
       " '/home/MDizier/data/dl/levir-cd/train/label/train_376.png',\n",
       " '/home/MDizier/data/dl/levir-cd/train/label/train_384.png',\n",
       " '/home/MDizier/data/dl/levir-cd/train/label/train_387.png',\n",
       " '/home/MDizier/data/dl/levir-cd/train/label/train_388.png',\n",
       " '/home/MDizier/data/dl/levir-cd/train/label/train_403.png',\n",
       " '/home/MDizier/data/dl/levir-cd/train/label/train_408.png',\n",
       " '/home/MDizier/data/dl/levir-cd/train/label/train_419.png',\n",
       " '/home/MDizier/data/dl/levir-cd/train/label/train_423.png',\n",
       " '/home/MDizier/data/dl/levir-cd/train/label/train_424.png',\n",
       " '/home/MDizier/data/dl/levir-cd/train/label/train_428.png',\n",
       " '/home/MDizier/data/dl/levir-cd/train/label/train_432.png',\n",
       " '/home/MDizier/data/dl/levir-cd/train/label/train_433.png',\n",
       " '/home/MDizier/data/dl/levir-cd/train/label/train_434.png',\n",
       " '/home/MDizier/data/dl/levir-cd/train/label/train_435.png']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_label_track"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18f8943-ae24-45c3-86b2-2138bdb76daf",
   "metadata": {},
   "source": [
    "### change model : many prompt to one mask :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e54e2369-7dd0-4f6f-98ef-51c434088a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.commons.utils_io import load_config\n",
    "import hydra\n",
    "from hydra.core.global_hydra import GlobalHydra\n",
    "import pprint as pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c1f37f61-bedd-4f4a-9f9e-1689660006cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "GlobalHydra.instance().clear()\n",
    "list_args=[\"experiment=probing_diff\", \"sam_type=small\", \"data=levir-cd\", \"data.params.n_shape=3\", \"data.params.num_worker=0\"]\n",
    "cfg = load_config(list_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ad9c2370-6173-4abc-890f-6d93064f0145",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-31 09:22:41,540 - INFO ::  Weights loaded for : ['image_encoder']\n"
     ]
    }
   ],
   "source": [
    "module = hydra.utils.instantiate(cfg.model.instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d3555a87-1760-4376-a16b-ac02eae19fb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'callbacks': {'model_checkpoint': {'_target_': 'lightning.pytorch.callbacks.ModelCheckpoint', 'dirpath': '${paths.output_dir}/checkpoints', 'filename': 'epoch_{epoch:03d}', 'monitor': 'val/loss', 'verbose': False, 'save_last': True, 'save_top_k': 1, 'mode': 'min', 'auto_insert_metric_name': False, 'save_weights_only': False, 'every_n_train_steps': None, 'train_time_interval': None, 'every_n_epochs': None, 'save_on_train_epoch_end': None}, 'rich_progress_bar': {'_target_': 'lightning.pytorch.callbacks.RichProgressBar'}},\n",
      " 'ckpt_path': None,\n",
      " 'data': {'name': 'levir-cd', '_target_': 'src.data.datamodule.CDDataModule', 'params': {'prompt_type': 'sample', 'n_prompt': 1, 'loc': 'center', 'batch_size': 2, 'n_shape': 3, 'num_worker': 0, 'pin_memory': False}},\n",
      " 'extras': {'ignore_warnings': False, 'enforce_tags': True, 'print_config': True},\n",
      " 'logger': {'tensorboard': {'_target_': 'lightning.pytorch.loggers.tensorboard.TensorBoardLogger', 'save_dir': '${paths.output_dir}/tensorboard/', 'name': None, 'default_hp_metric': False}},\n",
      " 'model': {'network': {'image_encoder': {'_target_': 'src.models.segment_anything.modeling.image_encoder_dev.ImageEncoderViT', 'depth': 12, 'embed_dim': 768, 'img_size': 1024, 'mlp_ratio': 4, 'norm_layer': None, 'num_heads': 12, 'patch_size': 16, 'qkv_bias': True, 'use_rel_pos': True, 'global_attn_indexes': [2, 5, 8, 11], 'window_size': 14, 'out_chans': 256}, 'prompt_encoder': {'_target_': 'src.models.segment_anything.modeling.prompt_encoder_dev.PromptEncoder', 'embed_dim': 256, 'image_embedding_size': [64, 64], 'input_image_size': [1024, 1024], 'mask_in_chans': 16}, 'mask_decoder': {'transformer': {'_target_': 'src.models.segment_anything.modeling.transformer_dev.TwoWayTransformer', 'depth': 2, 'embedding_dim': 256, 'mlp_dim': 2048, 'num_heads': 8}, '_target_': 'src.models.segment_anything.modeling.mask_decoder_dev.MaskDecoder', 'num_multimask_outputs': 3, 'transformer_dim': 256, 'iou_head_depth': 3, 'iou_head_hidden_dim': 256}, '_target_': 'src.models.magic_pen.bisam_diff.BiSamDiff'}, 'instance': {'_target_': 'src.models.magic_pen.task.MagicPenModule', 'network': '${model.network}', 'params': {'sam_ckpt_path': '${sam_ckpt_path}', 'use_weights': ['image_encoder']}}},\n",
      " 'paths': {'root_dir': '${oc.env:PROJECT_PATH}', 'data_dir': '${oc.env:DATA_PATH}', 'log_dir': '${oc.env:LOGS_PATH}', 'output_dir': '${hydra:runtime.output_dir}', 'work_dir': '${hydra:runtime.cwd}', 'sam_ckpt_dir': '${oc.env:CHECKPOINTS_PATH}/sam'},\n",
      " 'sam_ckpt_path': '/var/data/usr/mdizier/stylo_magique/checkpoints/sam/sam_vit_b_01ec64.pth',\n",
      " 'sam_enc_arch': 'vit-b',\n",
      " 'sam_name': 'small',\n",
      " 'seed': 66,\n",
      " 'tags': ['dev'],\n",
      " 'task_name': 'train_naive',\n",
      " 'test': True,\n",
      " 'train': True,\n",
      " 'trainer': {'_target_': 'lightning.pytorch.trainer.Trainer', 'default_root_dir': '${paths.output_dir}', 'min_epochs': 1, 'max_epochs': 100, 'accelerator': 'cpu', 'devices': 1, 'check_val_every_n_epoch': 1, 'deterministic': False}}\n"
     ]
    }
   ],
   "source": [
    "pp.pprint(dict(cfg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "82376d16-4b22-4744-9612-320595b1043d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = module.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "68665089-9d77-4ff2-826b-e8157b287b7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total model parameters : 93.74M\n"
     ]
    }
   ],
   "source": [
    "print(f\"total model parameters : {sum(p.numel() for p in model.parameters())/1e6:.2f}M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "95acc125-f1fc-4994-bb14-1beab5d072da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image encoder number parameters : 89.67M\n"
     ]
    }
   ],
   "source": [
    "print(f\"image encoder number parameters : {sum(p.numel() for p in model.image_encoder.parameters())/1e6:.2f}M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c276b841-66f3-4c87-b0d6-de72a7b231c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt_encoder number parameters : 0.01M\n"
     ]
    }
   ],
   "source": [
    "print(f\"prompt_encoder number parameters : {sum(p.numel() for p in model.prompt_encoder.parameters())/1e6:.2f}M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ec9408fd-edcc-4cc4-8f53-1f30fa2da95c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mask_decoder number parameters : 4.06M\n"
     ]
    }
   ],
   "source": [
    "print(f\"mask_decoder number parameters : {sum(p.numel() for p in model.mask_decoder.parameters())/1e6:.2f}M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "199097bd-2cec-49ec-9203-30cacad30ac9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BiSamDiff(\n",
       "  (image_encoder): ImageEncoderViT(\n",
       "    (patch_embed): PatchEmbed(\n",
       "      (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "    )\n",
       "    (blocks): ModuleList(\n",
       "      (0-11): 12 x Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (neck): Sequential(\n",
       "      (0): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): LayerNorm2d()\n",
       "      (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (3): LayerNorm2d()\n",
       "    )\n",
       "  )\n",
       "  (prompt_encoder): PromptEncoder(\n",
       "    (pe_layer): PositionEmbeddingRandom()\n",
       "    (point_embeddings): ModuleList(\n",
       "      (0-3): 4 x Embedding(1, 256)\n",
       "    )\n",
       "    (not_a_point_embed): Embedding(1, 256)\n",
       "    (mask_downscaling): Sequential(\n",
       "      (0): Conv2d(1, 4, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (1): LayerNorm2d()\n",
       "      (2): GELU(approximate='none')\n",
       "      (3): Conv2d(4, 16, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (4): LayerNorm2d()\n",
       "      (5): GELU(approximate='none')\n",
       "      (6): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (no_mask_embed): Embedding(1, 256)\n",
       "  )\n",
       "  (mask_decoder): MaskDecoder(\n",
       "    (transformer): TwoWayTransformer(\n",
       "      (layers): ModuleList(\n",
       "        (0-1): 2 x TwoWayAttentionBlock(\n",
       "          (self_attn): Attention(\n",
       "            (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (cross_attn_token_to_image): Attention(\n",
       "            (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
       "          )\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): MLPBlock(\n",
       "            (lin1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "            (lin2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "            (act): ReLU()\n",
       "          )\n",
       "          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (cross_attn_image_to_token): Attention(\n",
       "            (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (final_attn_token_to_image): Attention(\n",
       "        (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
       "      )\n",
       "      (norm_final_attn): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (iou_token): Embedding(1, 256)\n",
       "    (mask_tokens): Embedding(4, 256)\n",
       "    (output_upscaling): Sequential(\n",
       "      (0): ConvTranspose2d(256, 64, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (1): LayerNorm2d()\n",
       "      (2): GELU(approximate='none')\n",
       "      (3): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (4): GELU(approximate='none')\n",
       "    )\n",
       "    (output_hypernetworks_mlps): ModuleList(\n",
       "      (0-3): 4 x MLP(\n",
       "        (layers): ModuleList(\n",
       "          (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
       "          (2): Linear(in_features=256, out_features=32, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (iou_prediction_head): MLP(\n",
       "      (layers): ModuleList(\n",
       "        (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
       "        (2): Linear(in_features=256, out_features=4, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4ba85332-1d48-4c7d-9567-a4aa4afa7a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.commons.utils_io import load_sam\n",
    "from src.models.magic_pen.bisam_concat import BiSamConcat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c5d706d8-4f00-43e6-96ef-926e7f5d6c5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-26 16:04:41,574 - INFO ::  build vit_b BiSamConcat\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for BiSamConcat:\n\tsize mismatch for image_encoder.neck.0.weight: copying a param with shape torch.Size([256, 768, 1, 1]) from checkpoint, the shape in current model is torch.Size([512, 768, 1, 1]).\n\tsize mismatch for image_encoder.neck.1.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for image_encoder.neck.1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for image_encoder.neck.2.weight: copying a param with shape torch.Size([256, 256, 3, 3]) from checkpoint, the shape in current model is torch.Size([512, 512, 3, 3]).\n\tsize mismatch for image_encoder.neck.3.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for image_encoder.neck.3.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for prompt_encoder.pe_layer.positional_encoding_gaussian_matrix: copying a param with shape torch.Size([2, 128]) from checkpoint, the shape in current model is torch.Size([2, 256]).\n\tsize mismatch for prompt_encoder.point_embeddings.0.weight: copying a param with shape torch.Size([1, 256]) from checkpoint, the shape in current model is torch.Size([1, 512]).\n\tsize mismatch for prompt_encoder.point_embeddings.1.weight: copying a param with shape torch.Size([1, 256]) from checkpoint, the shape in current model is torch.Size([1, 512]).\n\tsize mismatch for prompt_encoder.point_embeddings.2.weight: copying a param with shape torch.Size([1, 256]) from checkpoint, the shape in current model is torch.Size([1, 512]).\n\tsize mismatch for prompt_encoder.point_embeddings.3.weight: copying a param with shape torch.Size([1, 256]) from checkpoint, the shape in current model is torch.Size([1, 512]).\n\tsize mismatch for prompt_encoder.not_a_point_embed.weight: copying a param with shape torch.Size([1, 256]) from checkpoint, the shape in current model is torch.Size([1, 512]).\n\tsize mismatch for prompt_encoder.mask_downscaling.6.weight: copying a param with shape torch.Size([256, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([512, 16, 1, 1]).\n\tsize mismatch for prompt_encoder.mask_downscaling.6.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for prompt_encoder.no_mask_embed.weight: copying a param with shape torch.Size([1, 256]) from checkpoint, the shape in current model is torch.Size([1, 512]).\n\tsize mismatch for mask_decoder.transformer.layers.0.self_attn.q_proj.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for mask_decoder.transformer.layers.0.self_attn.q_proj.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for mask_decoder.transformer.layers.0.self_attn.k_proj.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for mask_decoder.transformer.layers.0.self_attn.k_proj.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for mask_decoder.transformer.layers.0.self_attn.v_proj.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for mask_decoder.transformer.layers.0.self_attn.v_proj.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for mask_decoder.transformer.layers.0.self_attn.out_proj.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for mask_decoder.transformer.layers.0.self_attn.out_proj.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for mask_decoder.transformer.layers.0.norm1.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for mask_decoder.transformer.layers.0.norm1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for mask_decoder.transformer.layers.0.cross_attn_token_to_image.q_proj.weight: copying a param with shape torch.Size([128, 256]) from checkpoint, the shape in current model is torch.Size([256, 512]).\n\tsize mismatch for mask_decoder.transformer.layers.0.cross_attn_token_to_image.q_proj.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for mask_decoder.transformer.layers.0.cross_attn_token_to_image.k_proj.weight: copying a param with shape torch.Size([128, 256]) from checkpoint, the shape in current model is torch.Size([256, 512]).\n\tsize mismatch for mask_decoder.transformer.layers.0.cross_attn_token_to_image.k_proj.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for mask_decoder.transformer.layers.0.cross_attn_token_to_image.v_proj.weight: copying a param with shape torch.Size([128, 256]) from checkpoint, the shape in current model is torch.Size([256, 512]).\n\tsize mismatch for mask_decoder.transformer.layers.0.cross_attn_token_to_image.v_proj.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for mask_decoder.transformer.layers.0.cross_attn_token_to_image.out_proj.weight: copying a param with shape torch.Size([256, 128]) from checkpoint, the shape in current model is torch.Size([512, 256]).\n\tsize mismatch for mask_decoder.transformer.layers.0.cross_attn_token_to_image.out_proj.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for mask_decoder.transformer.layers.0.norm2.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for mask_decoder.transformer.layers.0.norm2.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for mask_decoder.transformer.layers.0.mlp.lin1.weight: copying a param with shape torch.Size([2048, 256]) from checkpoint, the shape in current model is torch.Size([2048, 512]).\n\tsize mismatch for mask_decoder.transformer.layers.0.mlp.lin2.weight: copying a param with shape torch.Size([256, 2048]) from checkpoint, the shape in current model is torch.Size([512, 2048]).\n\tsize mismatch for mask_decoder.transformer.layers.0.mlp.lin2.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for mask_decoder.transformer.layers.0.norm3.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for mask_decoder.transformer.layers.0.norm3.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for mask_decoder.transformer.layers.0.norm4.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for mask_decoder.transformer.layers.0.norm4.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for mask_decoder.transformer.layers.0.cross_attn_image_to_token.q_proj.weight: copying a param with shape torch.Size([128, 256]) from checkpoint, the shape in current model is torch.Size([256, 512]).\n\tsize mismatch for mask_decoder.transformer.layers.0.cross_attn_image_to_token.q_proj.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for mask_decoder.transformer.layers.0.cross_attn_image_to_token.k_proj.weight: copying a param with shape torch.Size([128, 256]) from checkpoint, the shape in current model is torch.Size([256, 512]).\n\tsize mismatch for mask_decoder.transformer.layers.0.cross_attn_image_to_token.k_proj.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for mask_decoder.transformer.layers.0.cross_attn_image_to_token.v_proj.weight: copying a param with shape torch.Size([128, 256]) from checkpoint, the shape in current model is torch.Size([256, 512]).\n\tsize mismatch for mask_decoder.transformer.layers.0.cross_attn_image_to_token.v_proj.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for mask_decoder.transformer.layers.0.cross_attn_image_to_token.out_proj.weight: copying a param with shape torch.Size([256, 128]) from checkpoint, the shape in current model is torch.Size([512, 256]).\n\tsize mismatch for mask_decoder.transformer.layers.0.cross_attn_image_to_token.out_proj.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for mask_decoder.transformer.layers.1.self_attn.q_proj.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for mask_decoder.transformer.layers.1.self_attn.q_proj.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for mask_decoder.transformer.layers.1.self_attn.k_proj.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for mask_decoder.transformer.layers.1.self_attn.k_proj.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for mask_decoder.transformer.layers.1.self_attn.v_proj.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for mask_decoder.transformer.layers.1.self_attn.v_proj.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for mask_decoder.transformer.layers.1.self_attn.out_proj.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for mask_decoder.transformer.layers.1.self_attn.out_proj.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for mask_decoder.transformer.layers.1.norm1.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for mask_decoder.transformer.layers.1.norm1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for mask_decoder.transformer.layers.1.cross_attn_token_to_image.q_proj.weight: copying a param with shape torch.Size([128, 256]) from checkpoint, the shape in current model is torch.Size([256, 512]).\n\tsize mismatch for mask_decoder.transformer.layers.1.cross_attn_token_to_image.q_proj.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for mask_decoder.transformer.layers.1.cross_attn_token_to_image.k_proj.weight: copying a param with shape torch.Size([128, 256]) from checkpoint, the shape in current model is torch.Size([256, 512]).\n\tsize mismatch for mask_decoder.transformer.layers.1.cross_attn_token_to_image.k_proj.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for mask_decoder.transformer.layers.1.cross_attn_token_to_image.v_proj.weight: copying a param with shape torch.Size([128, 256]) from checkpoint, the shape in current model is torch.Size([256, 512]).\n\tsize mismatch for mask_decoder.transformer.layers.1.cross_attn_token_to_image.v_proj.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for mask_decoder.transformer.layers.1.cross_attn_token_to_image.out_proj.weight: copying a param with shape torch.Size([256, 128]) from checkpoint, the shape in current model is torch.Size([512, 256]).\n\tsize mismatch for mask_decoder.transformer.layers.1.cross_attn_token_to_image.out_proj.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for mask_decoder.transformer.layers.1.norm2.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for mask_decoder.transformer.layers.1.norm2.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for mask_decoder.transformer.layers.1.mlp.lin1.weight: copying a param with shape torch.Size([2048, 256]) from checkpoint, the shape in current model is torch.Size([2048, 512]).\n\tsize mismatch for mask_decoder.transformer.layers.1.mlp.lin2.weight: copying a param with shape torch.Size([256, 2048]) from checkpoint, the shape in current model is torch.Size([512, 2048]).\n\tsize mismatch for mask_decoder.transformer.layers.1.mlp.lin2.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for mask_decoder.transformer.layers.1.norm3.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for mask_decoder.transformer.layers.1.norm3.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for mask_decoder.transformer.layers.1.norm4.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for mask_decoder.transformer.layers.1.norm4.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for mask_decoder.transformer.layers.1.cross_attn_image_to_token.q_proj.weight: copying a param with shape torch.Size([128, 256]) from checkpoint, the shape in current model is torch.Size([256, 512]).\n\tsize mismatch for mask_decoder.transformer.layers.1.cross_attn_image_to_token.q_proj.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for mask_decoder.transformer.layers.1.cross_attn_image_to_token.k_proj.weight: copying a param with shape torch.Size([128, 256]) from checkpoint, the shape in current model is torch.Size([256, 512]).\n\tsize mismatch for mask_decoder.transformer.layers.1.cross_attn_image_to_token.k_proj.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for mask_decoder.transformer.layers.1.cross_attn_image_to_token.v_proj.weight: copying a param with shape torch.Size([128, 256]) from checkpoint, the shape in current model is torch.Size([256, 512]).\n\tsize mismatch for mask_decoder.transformer.layers.1.cross_attn_image_to_token.v_proj.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for mask_decoder.transformer.layers.1.cross_attn_image_to_token.out_proj.weight: copying a param with shape torch.Size([256, 128]) from checkpoint, the shape in current model is torch.Size([512, 256]).\n\tsize mismatch for mask_decoder.transformer.layers.1.cross_attn_image_to_token.out_proj.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for mask_decoder.transformer.final_attn_token_to_image.q_proj.weight: copying a param with shape torch.Size([128, 256]) from checkpoint, the shape in current model is torch.Size([256, 512]).\n\tsize mismatch for mask_decoder.transformer.final_attn_token_to_image.q_proj.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for mask_decoder.transformer.final_attn_token_to_image.k_proj.weight: copying a param with shape torch.Size([128, 256]) from checkpoint, the shape in current model is torch.Size([256, 512]).\n\tsize mismatch for mask_decoder.transformer.final_attn_token_to_image.k_proj.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for mask_decoder.transformer.final_attn_token_to_image.v_proj.weight: copying a param with shape torch.Size([128, 256]) from checkpoint, the shape in current model is torch.Size([256, 512]).\n\tsize mismatch for mask_decoder.transformer.final_attn_token_to_image.v_proj.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for mask_decoder.transformer.final_attn_token_to_image.out_proj.weight: copying a param with shape torch.Size([256, 128]) from checkpoint, the shape in current model is torch.Size([512, 256]).\n\tsize mismatch for mask_decoder.transformer.final_attn_token_to_image.out_proj.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for mask_decoder.transformer.norm_final_attn.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for mask_decoder.transformer.norm_final_attn.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for mask_decoder.iou_token.weight: copying a param with shape torch.Size([1, 256]) from checkpoint, the shape in current model is torch.Size([1, 512]).\n\tsize mismatch for mask_decoder.mask_tokens.weight: copying a param with shape torch.Size([4, 256]) from checkpoint, the shape in current model is torch.Size([4, 512]).\n\tsize mismatch for mask_decoder.output_upscaling.0.weight: copying a param with shape torch.Size([256, 64, 2, 2]) from checkpoint, the shape in current model is torch.Size([512, 128, 2, 2]).\n\tsize mismatch for mask_decoder.output_upscaling.0.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for mask_decoder.output_upscaling.1.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for mask_decoder.output_upscaling.1.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for mask_decoder.output_upscaling.3.weight: copying a param with shape torch.Size([64, 32, 2, 2]) from checkpoint, the shape in current model is torch.Size([128, 64, 2, 2]).\n\tsize mismatch for mask_decoder.output_upscaling.3.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for mask_decoder.output_hypernetworks_mlps.0.layers.0.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for mask_decoder.output_hypernetworks_mlps.0.layers.0.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for mask_decoder.output_hypernetworks_mlps.0.layers.1.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for mask_decoder.output_hypernetworks_mlps.0.layers.1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for mask_decoder.output_hypernetworks_mlps.0.layers.2.weight: copying a param with shape torch.Size([32, 256]) from checkpoint, the shape in current model is torch.Size([64, 512]).\n\tsize mismatch for mask_decoder.output_hypernetworks_mlps.0.layers.2.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for mask_decoder.output_hypernetworks_mlps.1.layers.0.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for mask_decoder.output_hypernetworks_mlps.1.layers.0.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for mask_decoder.output_hypernetworks_mlps.1.layers.1.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for mask_decoder.output_hypernetworks_mlps.1.layers.1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for mask_decoder.output_hypernetworks_mlps.1.layers.2.weight: copying a param with shape torch.Size([32, 256]) from checkpoint, the shape in current model is torch.Size([64, 512]).\n\tsize mismatch for mask_decoder.output_hypernetworks_mlps.1.layers.2.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for mask_decoder.output_hypernetworks_mlps.2.layers.0.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for mask_decoder.output_hypernetworks_mlps.2.layers.0.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for mask_decoder.output_hypernetworks_mlps.2.layers.1.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for mask_decoder.output_hypernetworks_mlps.2.layers.1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for mask_decoder.output_hypernetworks_mlps.2.layers.2.weight: copying a param with shape torch.Size([32, 256]) from checkpoint, the shape in current model is torch.Size([64, 512]).\n\tsize mismatch for mask_decoder.output_hypernetworks_mlps.2.layers.2.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for mask_decoder.output_hypernetworks_mlps.3.layers.0.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for mask_decoder.output_hypernetworks_mlps.3.layers.0.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for mask_decoder.output_hypernetworks_mlps.3.layers.1.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for mask_decoder.output_hypernetworks_mlps.3.layers.1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for mask_decoder.output_hypernetworks_mlps.3.layers.2.weight: copying a param with shape torch.Size([32, 256]) from checkpoint, the shape in current model is torch.Size([64, 512]).\n\tsize mismatch for mask_decoder.output_hypernetworks_mlps.3.layers.2.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for mask_decoder.iou_prediction_head.layers.0.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([256, 512]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m bisam_concat \u001b[38;5;241m=\u001b[39m \u001b[43mload_sam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvit_b\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBiSamConcat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_strict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/IGN/stage_stylo_magique_2024/src/commons/utils_io.py:60\u001b[0m, in \u001b[0;36mload_sam\u001b[0;34m(model_type, model_cls, version, device, is_strict, init_ckpt, embed_dim)\u001b[0m\n\u001b[1;32m     56\u001b[0m     sam \u001b[38;5;241m=\u001b[39m sam_model_registry_v2[model_type](\n\u001b[1;32m     57\u001b[0m         checkpoint\u001b[38;5;241m=\u001b[39mckpt, model\u001b[38;5;241m=\u001b[39mmodel_cls, is_strict\u001b[38;5;241m=\u001b[39mis_strict, embed_dim\u001b[38;5;241m=\u001b[39membed_dim\n\u001b[1;32m     58\u001b[0m     )\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mcase\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdev\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 60\u001b[0m     sam \u001b[38;5;241m=\u001b[39m \u001b[43msam_model_registry\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_cls\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_strict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_strict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[43membed_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membed_dim\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mcase\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     64\u001b[0m     sam \u001b[38;5;241m=\u001b[39m sam_model_registry_v0[model_type](\n\u001b[1;32m     65\u001b[0m         checkpoint\u001b[38;5;241m=\u001b[39mckpt, is_strict\u001b[38;5;241m=\u001b[39mis_strict\n\u001b[1;32m     66\u001b[0m     )\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "File \u001b[0;32m~/Documents/IGN/stage_stylo_magique_2024/src/models/segment_anything/build_sam_dev.py:64\u001b[0m, in \u001b[0;36mbuild_sam_vit_b\u001b[0;34m(checkpoint, model, is_strict, embed_dim)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbuild_sam_vit_b\u001b[39m(checkpoint\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, model\u001b[38;5;241m=\u001b[39mSam, is_strict:\u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, embed_dim: \u001b[38;5;28mint\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m):\n\u001b[1;32m     62\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbuild vit_b \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 64\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_build_sam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_embed_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m768\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_depth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m12\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_num_heads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m12\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_global_attn_indexes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m11\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheckpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_strict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_strict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompt_embed_dim\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43membed_dim\u001b[49m\n\u001b[1;32m     73\u001b[0m \n\u001b[1;32m     74\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/IGN/stage_stylo_magique_2024/src/models/segment_anything/build_sam_dev.py:143\u001b[0m, in \u001b[0;36m_build_sam\u001b[0;34m(encoder_embed_dim, encoder_depth, encoder_num_heads, encoder_global_attn_indexes, model, checkpoint, is_strict, prompt_embed_dim)\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(checkpoint, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    142\u001b[0m         state_dict \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[0;32m--> 143\u001b[0m     \u001b[43msam\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_strict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sam\n",
      "File \u001b[0;32m~/miniforge3/envs/magic_pen/lib/python3.11/site-packages/torch/nn/modules/module.py:2041\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   2036\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2037\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2038\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(k) \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   2040\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2041\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2042\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   2043\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for BiSamConcat:\n\tsize mismatch for image_encoder.neck.0.weight: copying a param with shape torch.Size([256, 768, 1, 1]) from checkpoint, the shape in current model is torch.Size([512, 768, 1, 1]).\n\tsize mismatch for image_encoder.neck.1.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for image_encoder.neck.1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for image_encoder.neck.2.weight: copying a param with shape torch.Size([256, 256, 3, 3]) from checkpoint, the shape in current model is torch.Size([512, 512, 3, 3]).\n\tsize mismatch for image_encoder.neck.3.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for image_encoder.neck.3.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for prompt_encoder.pe_layer.positional_encoding_gaussian_matrix: copying a param with shape torch.Size([2, 128]) from checkpoint, the shape in current model is torch.Size([2, 256]).\n\tsize mismatch for prompt_encoder.point_embeddings.0.weight: copying a param with shape torch.Size([1, 256]) from checkpoint, the shape in current model is torch.Size([1, 512]).\n\tsize mismatch for prompt_encoder.point_embeddings.1.weight: copying a param with shape torch.Size([1, 256]) from checkpoint, the shape in current model is torch.Size([1, 512]).\n\tsize mismatch for prompt_encoder.point_embeddings.2.weight: copying a param with shape torch.Size([1, 256]) from checkpoint, the shape in current model is torch.Size([1, 512]).\n\tsize mismatch for prompt_encoder.point_embeddings.3.weight: copying a param with shape torch.Size([1, 256]) from checkpoint, the shape in current model is torch.Size([1, 512]).\n\tsize mismatch for prompt_encoder.not_a_point_embed.weight: copying a param with shape torch.Size([1, 256]) from checkpoint, the shape in current model is torch.Size([1, 512]).\n\tsize mismatch for prompt_encoder.mask_downscaling.6.weight: copying a param with shape torch.Size([256, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([512, 16, 1, 1]).\n\tsize mismatch for prompt_encoder.mask_downscaling.6.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for prompt_encoder.no_mask_embed.weight: copying a param with shape torch.Size([1, 256]) from checkpoint, the shape in current model is torch.Size([1, 512]).\n\tsize mismatch for mask_decoder.transformer.layers.0.self_attn.q_proj.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for mask_decoder.transformer.layers.0.self_attn.q_proj.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for mask_decoder.transformer.layers.0.self_attn.k_proj.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for mask_decoder.transformer.layers.0.self_attn.k_proj.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for mask_decoder.transformer.layers.0.self_attn.v_proj.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for mask_decoder.transformer.layers.0.self_attn.v_proj.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for mask_decoder.transformer.layers.0.self_attn.out_proj.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for mask_decoder.transformer.layers.0.self_attn.out_proj.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for mask_decoder.transformer.layers.0.norm1.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for mask_decoder.transformer.layers.0.norm1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for mask_decoder.transformer.layers.0.cross_attn_token_to_image.q_proj.weight: copying a param with shape torch.Size([128, 256]) from checkpoint, the shape in current model is torch.Size([256, 512]).\n\tsize mismatch for mask_decoder.transformer.layers.0.cross_attn_token_to_image.q_proj.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for mask_decoder.transformer.layers.0.cross_attn_token_to_image.k_proj.weight: copying a param with shape torch.Size([128, 256]) from checkpoint, the shape in current model is torch.Size([256, 512]).\n\tsize mismatch for mask_decoder.transformer.layers.0.cross_attn_token_to_image.k_proj.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for mask_decoder.transformer.layers.0.cross_attn_token_to_image.v_proj.weight: copying a param with shape torch.Size([128, 256]) from checkpoint, the shape in current model is torch.Size([256, 512]).\n\tsize mismatch for mask_decoder.transformer.layers.0.cross_attn_token_to_image.v_proj.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for mask_decoder.transformer.layers.0.cross_attn_token_to_image.out_proj.weight: copying a param with shape torch.Size([256, 128]) from checkpoint, the shape in current model is torch.Size([512, 256]).\n\tsize mismatch for mask_decoder.transformer.layers.0.cross_attn_token_to_image.out_proj.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for mask_decoder.transformer.layers.0.norm2.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for mask_decoder.transformer.layers.0.norm2.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for mask_decoder.transformer.layers.0.mlp.lin1.weight: copying a param with shape torch.Size([2048, 256]) from checkpoint, the shape in current model is torch.Size([2048, 512]).\n\tsize mismatch for mask_decoder.transformer.layers.0.mlp.lin2.weight: copying a param with shape torch.Size([256, 2048]) from checkpoint, the shape in current model is torch.Size([512, 2048]).\n\tsize mismatch for mask_decoder.transformer.layers.0.mlp.lin2.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for mask_decoder.transformer.layers.0.norm3.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for mask_decoder.transformer.layers.0.norm3.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for mask_decoder.transformer.layers.0.norm4.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for mask_decoder.transformer.layers.0.norm4.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for mask_decoder.transformer.layers.0.cross_attn_image_to_token.q_proj.weight: copying a param with shape torch.Size([128, 256]) from checkpoint, the shape in current model is torch.Size([256, 512]).\n\tsize mismatch for mask_decoder.transformer.layers.0.cross_attn_image_to_token.q_proj.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for mask_decoder.transformer.layers.0.cross_attn_image_to_token.k_proj.weight: copying a param with shape torch.Size([128, 256]) from checkpoint, the shape in current model is torch.Size([256, 512]).\n\tsize mismatch for mask_decoder.transformer.layers.0.cross_attn_image_to_token.k_proj.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for mask_decoder.transformer.layers.0.cross_attn_image_to_token.v_proj.weight: copying a param with shape torch.Size([128, 256]) from checkpoint, the shape in current model is torch.Size([256, 512]).\n\tsize mismatch for mask_decoder.transformer.layers.0.cross_attn_image_to_token.v_proj.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for mask_decoder.transformer.layers.0.cross_attn_image_to_token.out_proj.weight: copying a param with shape torch.Size([256, 128]) from checkpoint, the shape in current model is torch.Size([512, 256]).\n\tsize mismatch for mask_decoder.transformer.layers.0.cross_attn_image_to_token.out_proj.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for mask_decoder.transformer.layers.1.self_attn.q_proj.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for mask_decoder.transformer.layers.1.self_attn.q_proj.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for mask_decoder.transformer.layers.1.self_attn.k_proj.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for mask_decoder.transformer.layers.1.self_attn.k_proj.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for mask_decoder.transformer.layers.1.self_attn.v_proj.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for mask_decoder.transformer.layers.1.self_attn.v_proj.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for mask_decoder.transformer.layers.1.self_attn.out_proj.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for mask_decoder.transformer.layers.1.self_attn.out_proj.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for mask_decoder.transformer.layers.1.norm1.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for mask_decoder.transformer.layers.1.norm1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for mask_decoder.transformer.layers.1.cross_attn_token_to_image.q_proj.weight: copying a param with shape torch.Size([128, 256]) from checkpoint, the shape in current model is torch.Size([256, 512]).\n\tsize mismatch for mask_decoder.transformer.layers.1.cross_attn_token_to_image.q_proj.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for mask_decoder.transformer.layers.1.cross_attn_token_to_image.k_proj.weight: copying a param with shape torch.Size([128, 256]) from checkpoint, the shape in current model is torch.Size([256, 512]).\n\tsize mismatch for mask_decoder.transformer.layers.1.cross_attn_token_to_image.k_proj.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for mask_decoder.transformer.layers.1.cross_attn_token_to_image.v_proj.weight: copying a param with shape torch.Size([128, 256]) from checkpoint, the shape in current model is torch.Size([256, 512]).\n\tsize mismatch for mask_decoder.transformer.layers.1.cross_attn_token_to_image.v_proj.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for mask_decoder.transformer.layers.1.cross_attn_token_to_image.out_proj.weight: copying a param with shape torch.Size([256, 128]) from checkpoint, the shape in current model is torch.Size([512, 256]).\n\tsize mismatch for mask_decoder.transformer.layers.1.cross_attn_token_to_image.out_proj.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for mask_decoder.transformer.layers.1.norm2.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for mask_decoder.transformer.layers.1.norm2.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for mask_decoder.transformer.layers.1.mlp.lin1.weight: copying a param with shape torch.Size([2048, 256]) from checkpoint, the shape in current model is torch.Size([2048, 512]).\n\tsize mismatch for mask_decoder.transformer.layers.1.mlp.lin2.weight: copying a param with shape torch.Size([256, 2048]) from checkpoint, the shape in current model is torch.Size([512, 2048]).\n\tsize mismatch for mask_decoder.transformer.layers.1.mlp.lin2.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for mask_decoder.transformer.layers.1.norm3.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for mask_decoder.transformer.layers.1.norm3.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for mask_decoder.transformer.layers.1.norm4.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for mask_decoder.transformer.layers.1.norm4.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for mask_decoder.transformer.layers.1.cross_attn_image_to_token.q_proj.weight: copying a param with shape torch.Size([128, 256]) from checkpoint, the shape in current model is torch.Size([256, 512]).\n\tsize mismatch for mask_decoder.transformer.layers.1.cross_attn_image_to_token.q_proj.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for mask_decoder.transformer.layers.1.cross_attn_image_to_token.k_proj.weight: copying a param with shape torch.Size([128, 256]) from checkpoint, the shape in current model is torch.Size([256, 512]).\n\tsize mismatch for mask_decoder.transformer.layers.1.cross_attn_image_to_token.k_proj.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for mask_decoder.transformer.layers.1.cross_attn_image_to_token.v_proj.weight: copying a param with shape torch.Size([128, 256]) from checkpoint, the shape in current model is torch.Size([256, 512]).\n\tsize mismatch for mask_decoder.transformer.layers.1.cross_attn_image_to_token.v_proj.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for mask_decoder.transformer.layers.1.cross_attn_image_to_token.out_proj.weight: copying a param with shape torch.Size([256, 128]) from checkpoint, the shape in current model is torch.Size([512, 256]).\n\tsize mismatch for mask_decoder.transformer.layers.1.cross_attn_image_to_token.out_proj.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for mask_decoder.transformer.final_attn_token_to_image.q_proj.weight: copying a param with shape torch.Size([128, 256]) from checkpoint, the shape in current model is torch.Size([256, 512]).\n\tsize mismatch for mask_decoder.transformer.final_attn_token_to_image.q_proj.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for mask_decoder.transformer.final_attn_token_to_image.k_proj.weight: copying a param with shape torch.Size([128, 256]) from checkpoint, the shape in current model is torch.Size([256, 512]).\n\tsize mismatch for mask_decoder.transformer.final_attn_token_to_image.k_proj.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for mask_decoder.transformer.final_attn_token_to_image.v_proj.weight: copying a param with shape torch.Size([128, 256]) from checkpoint, the shape in current model is torch.Size([256, 512]).\n\tsize mismatch for mask_decoder.transformer.final_attn_token_to_image.v_proj.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for mask_decoder.transformer.final_attn_token_to_image.out_proj.weight: copying a param with shape torch.Size([256, 128]) from checkpoint, the shape in current model is torch.Size([512, 256]).\n\tsize mismatch for mask_decoder.transformer.final_attn_token_to_image.out_proj.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for mask_decoder.transformer.norm_final_attn.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for mask_decoder.transformer.norm_final_attn.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for mask_decoder.iou_token.weight: copying a param with shape torch.Size([1, 256]) from checkpoint, the shape in current model is torch.Size([1, 512]).\n\tsize mismatch for mask_decoder.mask_tokens.weight: copying a param with shape torch.Size([4, 256]) from checkpoint, the shape in current model is torch.Size([4, 512]).\n\tsize mismatch for mask_decoder.output_upscaling.0.weight: copying a param with shape torch.Size([256, 64, 2, 2]) from checkpoint, the shape in current model is torch.Size([512, 128, 2, 2]).\n\tsize mismatch for mask_decoder.output_upscaling.0.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for mask_decoder.output_upscaling.1.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for mask_decoder.output_upscaling.1.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for mask_decoder.output_upscaling.3.weight: copying a param with shape torch.Size([64, 32, 2, 2]) from checkpoint, the shape in current model is torch.Size([128, 64, 2, 2]).\n\tsize mismatch for mask_decoder.output_upscaling.3.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for mask_decoder.output_hypernetworks_mlps.0.layers.0.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for mask_decoder.output_hypernetworks_mlps.0.layers.0.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for mask_decoder.output_hypernetworks_mlps.0.layers.1.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for mask_decoder.output_hypernetworks_mlps.0.layers.1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for mask_decoder.output_hypernetworks_mlps.0.layers.2.weight: copying a param with shape torch.Size([32, 256]) from checkpoint, the shape in current model is torch.Size([64, 512]).\n\tsize mismatch for mask_decoder.output_hypernetworks_mlps.0.layers.2.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for mask_decoder.output_hypernetworks_mlps.1.layers.0.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for mask_decoder.output_hypernetworks_mlps.1.layers.0.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for mask_decoder.output_hypernetworks_mlps.1.layers.1.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for mask_decoder.output_hypernetworks_mlps.1.layers.1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for mask_decoder.output_hypernetworks_mlps.1.layers.2.weight: copying a param with shape torch.Size([32, 256]) from checkpoint, the shape in current model is torch.Size([64, 512]).\n\tsize mismatch for mask_decoder.output_hypernetworks_mlps.1.layers.2.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for mask_decoder.output_hypernetworks_mlps.2.layers.0.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for mask_decoder.output_hypernetworks_mlps.2.layers.0.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for mask_decoder.output_hypernetworks_mlps.2.layers.1.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for mask_decoder.output_hypernetworks_mlps.2.layers.1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for mask_decoder.output_hypernetworks_mlps.2.layers.2.weight: copying a param with shape torch.Size([32, 256]) from checkpoint, the shape in current model is torch.Size([64, 512]).\n\tsize mismatch for mask_decoder.output_hypernetworks_mlps.2.layers.2.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for mask_decoder.output_hypernetworks_mlps.3.layers.0.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for mask_decoder.output_hypernetworks_mlps.3.layers.0.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for mask_decoder.output_hypernetworks_mlps.3.layers.1.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for mask_decoder.output_hypernetworks_mlps.3.layers.1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for mask_decoder.output_hypernetworks_mlps.3.layers.2.weight: copying a param with shape torch.Size([32, 256]) from checkpoint, the shape in current model is torch.Size([64, 512]).\n\tsize mismatch for mask_decoder.output_hypernetworks_mlps.3.layers.2.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for mask_decoder.iou_prediction_head.layers.0.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([256, 512])."
     ]
    }
   ],
   "source": [
    "bisam_concat = load_sam(model_type=\"vit_b\", model_cls=BiSamConcat, is_strict=False, embed_dim=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e415660-4445-42fc-a221-b4a2acc8f444",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict = self.model.state_dict()\n",
    "pretrained_weights = {k: v for k, v in pretrained_weights.items() if not k.startswith(\"mask_decoder\")}\n",
    "model_dict.update(pretrained_weights)\n",
    "# let default values for decoder init \n",
    "self.model.load_state_dict(model_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b30ac66-eb77-4933-aaf0-ce1abe5de0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = torch.load(cfg.sam_ckpt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a8095a71-07e2-42c2-a748-72f0a6f7fa80",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['image_encoder.neck.0.weight', 'image_encoder.neck.1.weight', 'image_encoder.neck.1.bias', 'image_encoder.neck.2.weight', 'image_encoder.neck.3.weight', 'image_encoder.neck.3.bias', 'image_encoder.patch_embed.proj.weight', 'image_encoder.patch_embed.proj.bias', 'image_encoder.blocks.0.norm1.weight', 'image_encoder.blocks.0.norm1.bias', 'image_encoder.blocks.0.attn.rel_pos_h', 'image_encoder.blocks.0.attn.rel_pos_w', 'image_encoder.blocks.0.attn.qkv.weight', 'image_encoder.blocks.0.attn.qkv.bias', 'image_encoder.blocks.0.attn.proj.weight', 'image_encoder.blocks.0.attn.proj.bias', 'image_encoder.blocks.0.norm2.weight', 'image_encoder.blocks.0.norm2.bias', 'image_encoder.blocks.0.mlp.lin1.weight', 'image_encoder.blocks.0.mlp.lin1.bias', 'image_encoder.blocks.0.mlp.lin2.weight', 'image_encoder.blocks.0.mlp.lin2.bias', 'image_encoder.blocks.1.norm1.weight', 'image_encoder.blocks.1.norm1.bias', 'image_encoder.blocks.1.attn.rel_pos_h', 'image_encoder.blocks.1.attn.rel_pos_w', 'image_encoder.blocks.1.attn.qkv.weight', 'image_encoder.blocks.1.attn.qkv.bias', 'image_encoder.blocks.1.attn.proj.weight', 'image_encoder.blocks.1.attn.proj.bias', 'image_encoder.blocks.1.norm2.weight', 'image_encoder.blocks.1.norm2.bias', 'image_encoder.blocks.1.mlp.lin1.weight', 'image_encoder.blocks.1.mlp.lin1.bias', 'image_encoder.blocks.1.mlp.lin2.weight', 'image_encoder.blocks.1.mlp.lin2.bias', 'image_encoder.blocks.2.norm1.weight', 'image_encoder.blocks.2.norm1.bias', 'image_encoder.blocks.2.attn.rel_pos_h', 'image_encoder.blocks.2.attn.rel_pos_w', 'image_encoder.blocks.2.attn.qkv.weight', 'image_encoder.blocks.2.attn.qkv.bias', 'image_encoder.blocks.2.attn.proj.weight', 'image_encoder.blocks.2.attn.proj.bias', 'image_encoder.blocks.2.norm2.weight', 'image_encoder.blocks.2.norm2.bias', 'image_encoder.blocks.2.mlp.lin1.weight', 'image_encoder.blocks.2.mlp.lin1.bias', 'image_encoder.blocks.2.mlp.lin2.weight', 'image_encoder.blocks.2.mlp.lin2.bias', 'image_encoder.blocks.3.norm1.weight', 'image_encoder.blocks.3.norm1.bias', 'image_encoder.blocks.3.attn.rel_pos_h', 'image_encoder.blocks.3.attn.rel_pos_w', 'image_encoder.blocks.3.attn.qkv.weight', 'image_encoder.blocks.3.attn.qkv.bias', 'image_encoder.blocks.3.attn.proj.weight', 'image_encoder.blocks.3.attn.proj.bias', 'image_encoder.blocks.3.norm2.weight', 'image_encoder.blocks.3.norm2.bias', 'image_encoder.blocks.3.mlp.lin1.weight', 'image_encoder.blocks.3.mlp.lin1.bias', 'image_encoder.blocks.3.mlp.lin2.weight', 'image_encoder.blocks.3.mlp.lin2.bias', 'image_encoder.blocks.4.norm1.weight', 'image_encoder.blocks.4.norm1.bias', 'image_encoder.blocks.4.attn.rel_pos_h', 'image_encoder.blocks.4.attn.rel_pos_w', 'image_encoder.blocks.4.attn.qkv.weight', 'image_encoder.blocks.4.attn.qkv.bias', 'image_encoder.blocks.4.attn.proj.weight', 'image_encoder.blocks.4.attn.proj.bias', 'image_encoder.blocks.4.norm2.weight', 'image_encoder.blocks.4.norm2.bias', 'image_encoder.blocks.4.mlp.lin1.weight', 'image_encoder.blocks.4.mlp.lin1.bias', 'image_encoder.blocks.4.mlp.lin2.weight', 'image_encoder.blocks.4.mlp.lin2.bias', 'image_encoder.blocks.5.norm1.weight', 'image_encoder.blocks.5.norm1.bias', 'image_encoder.blocks.5.attn.rel_pos_h', 'image_encoder.blocks.5.attn.rel_pos_w', 'image_encoder.blocks.5.attn.qkv.weight', 'image_encoder.blocks.5.attn.qkv.bias', 'image_encoder.blocks.5.attn.proj.weight', 'image_encoder.blocks.5.attn.proj.bias', 'image_encoder.blocks.5.norm2.weight', 'image_encoder.blocks.5.norm2.bias', 'image_encoder.blocks.5.mlp.lin1.weight', 'image_encoder.blocks.5.mlp.lin1.bias', 'image_encoder.blocks.5.mlp.lin2.weight', 'image_encoder.blocks.5.mlp.lin2.bias', 'image_encoder.blocks.6.norm1.weight', 'image_encoder.blocks.6.norm1.bias', 'image_encoder.blocks.6.attn.rel_pos_h', 'image_encoder.blocks.6.attn.rel_pos_w', 'image_encoder.blocks.6.attn.qkv.weight', 'image_encoder.blocks.6.attn.qkv.bias', 'image_encoder.blocks.6.attn.proj.weight', 'image_encoder.blocks.6.attn.proj.bias', 'image_encoder.blocks.6.norm2.weight', 'image_encoder.blocks.6.norm2.bias', 'image_encoder.blocks.6.mlp.lin1.weight', 'image_encoder.blocks.6.mlp.lin1.bias', 'image_encoder.blocks.6.mlp.lin2.weight', 'image_encoder.blocks.6.mlp.lin2.bias', 'image_encoder.blocks.7.norm1.weight', 'image_encoder.blocks.7.norm1.bias', 'image_encoder.blocks.7.attn.rel_pos_h', 'image_encoder.blocks.7.attn.rel_pos_w', 'image_encoder.blocks.7.attn.qkv.weight', 'image_encoder.blocks.7.attn.qkv.bias', 'image_encoder.blocks.7.attn.proj.weight', 'image_encoder.blocks.7.attn.proj.bias', 'image_encoder.blocks.7.norm2.weight', 'image_encoder.blocks.7.norm2.bias', 'image_encoder.blocks.7.mlp.lin1.weight', 'image_encoder.blocks.7.mlp.lin1.bias', 'image_encoder.blocks.7.mlp.lin2.weight', 'image_encoder.blocks.7.mlp.lin2.bias', 'image_encoder.blocks.8.norm1.weight', 'image_encoder.blocks.8.norm1.bias', 'image_encoder.blocks.8.attn.rel_pos_h', 'image_encoder.blocks.8.attn.rel_pos_w', 'image_encoder.blocks.8.attn.qkv.weight', 'image_encoder.blocks.8.attn.qkv.bias', 'image_encoder.blocks.8.attn.proj.weight', 'image_encoder.blocks.8.attn.proj.bias', 'image_encoder.blocks.8.norm2.weight', 'image_encoder.blocks.8.norm2.bias', 'image_encoder.blocks.8.mlp.lin1.weight', 'image_encoder.blocks.8.mlp.lin1.bias', 'image_encoder.blocks.8.mlp.lin2.weight', 'image_encoder.blocks.8.mlp.lin2.bias', 'image_encoder.blocks.9.norm1.weight', 'image_encoder.blocks.9.norm1.bias', 'image_encoder.blocks.9.attn.rel_pos_h', 'image_encoder.blocks.9.attn.rel_pos_w', 'image_encoder.blocks.9.attn.qkv.weight', 'image_encoder.blocks.9.attn.qkv.bias', 'image_encoder.blocks.9.attn.proj.weight', 'image_encoder.blocks.9.attn.proj.bias', 'image_encoder.blocks.9.norm2.weight', 'image_encoder.blocks.9.norm2.bias', 'image_encoder.blocks.9.mlp.lin1.weight', 'image_encoder.blocks.9.mlp.lin1.bias', 'image_encoder.blocks.9.mlp.lin2.weight', 'image_encoder.blocks.9.mlp.lin2.bias', 'image_encoder.blocks.10.norm1.weight', 'image_encoder.blocks.10.norm1.bias', 'image_encoder.blocks.10.attn.rel_pos_h', 'image_encoder.blocks.10.attn.rel_pos_w', 'image_encoder.blocks.10.attn.qkv.weight', 'image_encoder.blocks.10.attn.qkv.bias', 'image_encoder.blocks.10.attn.proj.weight', 'image_encoder.blocks.10.attn.proj.bias', 'image_encoder.blocks.10.norm2.weight', 'image_encoder.blocks.10.norm2.bias', 'image_encoder.blocks.10.mlp.lin1.weight', 'image_encoder.blocks.10.mlp.lin1.bias', 'image_encoder.blocks.10.mlp.lin2.weight', 'image_encoder.blocks.10.mlp.lin2.bias', 'image_encoder.blocks.11.norm1.weight', 'image_encoder.blocks.11.norm1.bias', 'image_encoder.blocks.11.attn.rel_pos_h', 'image_encoder.blocks.11.attn.rel_pos_w', 'image_encoder.blocks.11.attn.qkv.weight', 'image_encoder.blocks.11.attn.qkv.bias', 'image_encoder.blocks.11.attn.proj.weight', 'image_encoder.blocks.11.attn.proj.bias', 'image_encoder.blocks.11.norm2.weight', 'image_encoder.blocks.11.norm2.bias', 'image_encoder.blocks.11.mlp.lin1.weight', 'image_encoder.blocks.11.mlp.lin1.bias', 'image_encoder.blocks.11.mlp.lin2.weight', 'image_encoder.blocks.11.mlp.lin2.bias', 'prompt_encoder.pe_layer.positional_encoding_gaussian_matrix', 'mask_decoder.transformer.layers.0.self_attn.q_proj.weight', 'mask_decoder.transformer.layers.0.self_attn.q_proj.bias', 'mask_decoder.transformer.layers.0.self_attn.k_proj.weight', 'mask_decoder.transformer.layers.0.self_attn.k_proj.bias', 'mask_decoder.transformer.layers.0.self_attn.v_proj.weight', 'mask_decoder.transformer.layers.0.self_attn.v_proj.bias', 'mask_decoder.transformer.layers.0.self_attn.out_proj.weight', 'mask_decoder.transformer.layers.0.self_attn.out_proj.bias', 'mask_decoder.transformer.layers.0.norm1.weight', 'mask_decoder.transformer.layers.0.norm1.bias', 'mask_decoder.transformer.layers.0.cross_attn_token_to_image.q_proj.weight', 'mask_decoder.transformer.layers.0.cross_attn_token_to_image.q_proj.bias', 'mask_decoder.transformer.layers.0.cross_attn_token_to_image.k_proj.weight', 'mask_decoder.transformer.layers.0.cross_attn_token_to_image.k_proj.bias', 'mask_decoder.transformer.layers.0.cross_attn_token_to_image.v_proj.weight', 'mask_decoder.transformer.layers.0.cross_attn_token_to_image.v_proj.bias', 'mask_decoder.transformer.layers.0.cross_attn_token_to_image.out_proj.weight', 'mask_decoder.transformer.layers.0.cross_attn_token_to_image.out_proj.bias', 'mask_decoder.transformer.layers.0.norm2.weight', 'mask_decoder.transformer.layers.0.norm2.bias', 'mask_decoder.transformer.layers.0.mlp.lin1.weight', 'mask_decoder.transformer.layers.0.mlp.lin1.bias', 'mask_decoder.transformer.layers.0.mlp.lin2.weight', 'mask_decoder.transformer.layers.0.mlp.lin2.bias', 'mask_decoder.transformer.layers.0.norm3.weight', 'mask_decoder.transformer.layers.0.norm3.bias', 'mask_decoder.transformer.layers.0.norm4.weight', 'mask_decoder.transformer.layers.0.norm4.bias', 'mask_decoder.transformer.layers.0.cross_attn_image_to_token.q_proj.weight', 'mask_decoder.transformer.layers.0.cross_attn_image_to_token.q_proj.bias', 'mask_decoder.transformer.layers.0.cross_attn_image_to_token.k_proj.weight', 'mask_decoder.transformer.layers.0.cross_attn_image_to_token.k_proj.bias', 'mask_decoder.transformer.layers.0.cross_attn_image_to_token.v_proj.weight', 'mask_decoder.transformer.layers.0.cross_attn_image_to_token.v_proj.bias', 'mask_decoder.transformer.layers.0.cross_attn_image_to_token.out_proj.weight', 'mask_decoder.transformer.layers.0.cross_attn_image_to_token.out_proj.bias', 'mask_decoder.transformer.layers.1.self_attn.q_proj.weight', 'mask_decoder.transformer.layers.1.self_attn.q_proj.bias', 'mask_decoder.transformer.layers.1.self_attn.k_proj.weight', 'mask_decoder.transformer.layers.1.self_attn.k_proj.bias', 'mask_decoder.transformer.layers.1.self_attn.v_proj.weight', 'mask_decoder.transformer.layers.1.self_attn.v_proj.bias', 'mask_decoder.transformer.layers.1.self_attn.out_proj.weight', 'mask_decoder.transformer.layers.1.self_attn.out_proj.bias', 'mask_decoder.transformer.layers.1.norm1.weight', 'mask_decoder.transformer.layers.1.norm1.bias', 'mask_decoder.transformer.layers.1.cross_attn_token_to_image.q_proj.weight', 'mask_decoder.transformer.layers.1.cross_attn_token_to_image.q_proj.bias', 'mask_decoder.transformer.layers.1.cross_attn_token_to_image.k_proj.weight', 'mask_decoder.transformer.layers.1.cross_attn_token_to_image.k_proj.bias', 'mask_decoder.transformer.layers.1.cross_attn_token_to_image.v_proj.weight', 'mask_decoder.transformer.layers.1.cross_attn_token_to_image.v_proj.bias', 'mask_decoder.transformer.layers.1.cross_attn_token_to_image.out_proj.weight', 'mask_decoder.transformer.layers.1.cross_attn_token_to_image.out_proj.bias', 'mask_decoder.transformer.layers.1.norm2.weight', 'mask_decoder.transformer.layers.1.norm2.bias', 'mask_decoder.transformer.layers.1.mlp.lin1.weight', 'mask_decoder.transformer.layers.1.mlp.lin1.bias', 'mask_decoder.transformer.layers.1.mlp.lin2.weight', 'mask_decoder.transformer.layers.1.mlp.lin2.bias', 'mask_decoder.transformer.layers.1.norm3.weight', 'mask_decoder.transformer.layers.1.norm3.bias', 'mask_decoder.transformer.layers.1.norm4.weight', 'mask_decoder.transformer.layers.1.norm4.bias', 'mask_decoder.transformer.layers.1.cross_attn_image_to_token.q_proj.weight', 'mask_decoder.transformer.layers.1.cross_attn_image_to_token.q_proj.bias', 'mask_decoder.transformer.layers.1.cross_attn_image_to_token.k_proj.weight', 'mask_decoder.transformer.layers.1.cross_attn_image_to_token.k_proj.bias', 'mask_decoder.transformer.layers.1.cross_attn_image_to_token.v_proj.weight', 'mask_decoder.transformer.layers.1.cross_attn_image_to_token.v_proj.bias', 'mask_decoder.transformer.layers.1.cross_attn_image_to_token.out_proj.weight', 'mask_decoder.transformer.layers.1.cross_attn_image_to_token.out_proj.bias', 'mask_decoder.transformer.final_attn_token_to_image.q_proj.weight', 'mask_decoder.transformer.final_attn_token_to_image.q_proj.bias', 'mask_decoder.transformer.final_attn_token_to_image.k_proj.weight', 'mask_decoder.transformer.final_attn_token_to_image.k_proj.bias', 'mask_decoder.transformer.final_attn_token_to_image.v_proj.weight', 'mask_decoder.transformer.final_attn_token_to_image.v_proj.bias', 'mask_decoder.transformer.final_attn_token_to_image.out_proj.weight', 'mask_decoder.transformer.final_attn_token_to_image.out_proj.bias', 'mask_decoder.transformer.norm_final_attn.weight', 'mask_decoder.transformer.norm_final_attn.bias', 'prompt_encoder.point_embeddings.0.weight', 'prompt_encoder.point_embeddings.1.weight', 'prompt_encoder.point_embeddings.2.weight', 'prompt_encoder.point_embeddings.3.weight', 'prompt_encoder.not_a_point_embed.weight', 'mask_decoder.output_upscaling.0.weight', 'mask_decoder.output_upscaling.0.bias', 'mask_decoder.output_upscaling.1.weight', 'mask_decoder.output_upscaling.1.bias', 'mask_decoder.output_upscaling.3.weight', 'mask_decoder.output_upscaling.3.bias', 'mask_decoder.output_hypernetworks_mlps.0.layers.0.weight', 'mask_decoder.output_hypernetworks_mlps.0.layers.0.bias', 'mask_decoder.output_hypernetworks_mlps.0.layers.1.weight', 'mask_decoder.output_hypernetworks_mlps.0.layers.1.bias', 'mask_decoder.output_hypernetworks_mlps.0.layers.2.weight', 'mask_decoder.output_hypernetworks_mlps.0.layers.2.bias', 'mask_decoder.output_hypernetworks_mlps.1.layers.0.weight', 'mask_decoder.output_hypernetworks_mlps.1.layers.0.bias', 'mask_decoder.output_hypernetworks_mlps.1.layers.1.weight', 'mask_decoder.output_hypernetworks_mlps.1.layers.1.bias', 'mask_decoder.output_hypernetworks_mlps.1.layers.2.weight', 'mask_decoder.output_hypernetworks_mlps.1.layers.2.bias', 'mask_decoder.output_hypernetworks_mlps.2.layers.0.weight', 'mask_decoder.output_hypernetworks_mlps.2.layers.0.bias', 'mask_decoder.output_hypernetworks_mlps.2.layers.1.weight', 'mask_decoder.output_hypernetworks_mlps.2.layers.1.bias', 'mask_decoder.output_hypernetworks_mlps.2.layers.2.weight', 'mask_decoder.output_hypernetworks_mlps.2.layers.2.bias', 'mask_decoder.output_hypernetworks_mlps.3.layers.0.weight', 'mask_decoder.output_hypernetworks_mlps.3.layers.0.bias', 'mask_decoder.output_hypernetworks_mlps.3.layers.1.weight', 'mask_decoder.output_hypernetworks_mlps.3.layers.1.bias', 'mask_decoder.output_hypernetworks_mlps.3.layers.2.weight', 'mask_decoder.output_hypernetworks_mlps.3.layers.2.bias', 'prompt_encoder.mask_downscaling.0.weight', 'prompt_encoder.mask_downscaling.0.bias', 'prompt_encoder.mask_downscaling.1.weight', 'prompt_encoder.mask_downscaling.1.bias', 'prompt_encoder.mask_downscaling.3.weight', 'prompt_encoder.mask_downscaling.3.bias', 'prompt_encoder.mask_downscaling.4.weight', 'prompt_encoder.mask_downscaling.4.bias', 'prompt_encoder.mask_downscaling.6.weight', 'prompt_encoder.mask_downscaling.6.bias', 'prompt_encoder.no_mask_embed.weight', 'mask_decoder.iou_prediction_head.layers.0.weight', 'mask_decoder.iou_prediction_head.layers.0.bias', 'mask_decoder.iou_prediction_head.layers.1.weight', 'mask_decoder.iou_prediction_head.layers.1.bias', 'mask_decoder.iou_prediction_head.layers.2.weight', 'mask_decoder.iou_prediction_head.layers.2.bias', 'mask_decoder.iou_token.weight', 'mask_decoder.mask_tokens.weight', 'image_encoder.pos_embed'])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0efece-2e1a-4540-a3ba-cac2535b326f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
